"""
æ–‡ä»¶ç®¡ç†æœåŠ¡
è´Ÿè´£æ–‡ä»¶çš„ä¸Šä¼ ã€å¤„ç†ã€å­˜å‚¨å’Œç®¡ç†åŠŸèƒ½
"""

import os
import hashlib
import logging
import yaml
from typing import Optional, Dict, Any, List
from datetime import datetime, timezone, timedelta
from werkzeug.datastructures import FileStorage
from werkzeug.utils import secure_filename
import re
import urllib.parse
import json
import threading

from utils.MySQLManager import MySQLManager


class FileService:
    """æ–‡ä»¶ç®¡ç†æœåŠ¡ç±»"""
    
    def __init__(self, config_path: str = 'config/config.yaml'):
        """
        åˆå§‹åŒ–æ–‡ä»¶æœåŠ¡
        
        Args:
            config_path: é…ç½®æ–‡ä»¶è·¯å¾„
        """
        self.config_path = config_path
        self.logger = logging.getLogger(__name__)
        
        # åŠ è½½é…ç½®
        self._load_config()
        
        # åˆå§‹åŒ–æ•°æ®åº“ç®¡ç†å™¨
        self.mysql_manager = MySQLManager()
        
        # åˆ›å»ºå¿…è¦çš„ç›®å½•
        self._create_directories()

    def _safe_filename(self, filename: str) -> str:
        """
        ç”Ÿæˆå®‰å…¨çš„æ–‡ä»¶åï¼Œæ”¯æŒä¸­æ–‡å­—ç¬¦
        
        Args:
            filename: åŸå§‹æ–‡ä»¶å
            
        Returns:
            str: å®‰å…¨çš„æ–‡ä»¶å
        """
        if not filename:
            return "unknown"
        
        # ç§»é™¤è·¯å¾„åˆ†éš”ç¬¦å’Œå…¶ä»–å±é™©å­—ç¬¦ï¼Œä½†ä¿ç•™ä¸­æ–‡å­—ç¬¦
        dangerous_chars = r'[<>:"/\\|?*\x00-\x1f]'
        safe_name = re.sub(dangerous_chars, '_', filename)
        
        # ç§»é™¤å¼€å¤´å’Œç»“å°¾çš„ç©ºæ ¼ã€ç‚¹å·
        safe_name = safe_name.strip(' .')
        
        # å¦‚æœæ–‡ä»¶åä¸ºç©ºæˆ–åªæœ‰æ‰©å±•åï¼Œä½¿ç”¨é»˜è®¤åç§°
        if not safe_name or safe_name.startswith('.'):
            safe_name = f"file_{safe_name}" if safe_name.startswith('.') else "unknown_file"
        
        # é™åˆ¶æ–‡ä»¶åé•¿åº¦ï¼ˆä¿ç•™æ‰©å±•åï¼‰
        if len(safe_name) > 200:
            name_part, ext_part = os.path.splitext(safe_name)
            max_name_len = 200 - len(ext_part)
            safe_name = name_part[:max_name_len] + ext_part
        
        return safe_name
    
    def _load_config(self) -> None:
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        try:
            with open(self.config_path, 'r', encoding='utf-8') as file:
                config = yaml.safe_load(file)
                self.file_config = config['file']
                
                # ğŸ”§ ä¿®å¤ï¼šå°†ç›¸å¯¹è·¯å¾„è½¬æ¢ä¸ºç»å¯¹è·¯å¾„
                project_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
                
                # è½¬æ¢ä¸Šä¼ ç›®å½•è·¯å¾„
                upload_folder = self.file_config['upload_folder']
                if not os.path.isabs(upload_folder):
                    self.file_config['upload_folder'] = os.path.abspath(os.path.join(project_root, upload_folder))
                
                # è½¬æ¢ä¸´æ—¶ç›®å½•è·¯å¾„
                temp_folder = self.file_config['temp_folder']
                if not os.path.isabs(temp_folder):
                    self.file_config['temp_folder'] = os.path.abspath(os.path.join(project_root, temp_folder))
                
                self.logger.info(f"æ–‡ä»¶æœåŠ¡é…ç½®åŠ è½½æˆåŠŸ")
                self.logger.info(f"ä¸Šä¼ ç›®å½•: {self.file_config['upload_folder']}")
                self.logger.info(f"ä¸´æ—¶ç›®å½•: {self.file_config['temp_folder']}")
                
        except Exception as e:
            self.logger.error(f"åŠ è½½æ–‡ä»¶æœåŠ¡é…ç½®å¤±è´¥: {str(e)}")
            raise
    
    def _create_directories(self) -> None:
        """åˆ›å»ºå¿…è¦çš„ç›®å½•"""
        directories = [
            self.file_config['upload_folder'],
            self.file_config['temp_folder']
        ]
        
        # æ·»åŠ æ–‡ä»¶ç±»å‹å­ç›®å½•
        upload_folder = self.file_config['upload_folder']
        file_type_dirs = ['pdf', 'doc', 'docx', 'xlsx', 'xls', 'pptx', 'ppt', 'txt', 'md', 'images']
        for file_type in file_type_dirs:
            directories.append(os.path.join(upload_folder, file_type))
        
        for directory in directories:
            if not os.path.exists(directory):
                os.makedirs(directory, exist_ok=True)
                self.logger.info(f"åˆ›å»ºç›®å½•: {directory}")
    
    def _get_file_hash(self, file_path: str) -> str:
        """
        è®¡ç®—æ–‡ä»¶å“ˆå¸Œå€¼
        
        Args:
            file_path: æ–‡ä»¶è·¯å¾„
            
        Returns:
            str: æ–‡ä»¶çš„SHA256å“ˆå¸Œå€¼
        """
        hash_sha256 = hashlib.sha256()
        try:
            with open(file_path, "rb") as f:
                for chunk in iter(lambda: f.read(4096), b""):
                    hash_sha256.update(chunk)
            return hash_sha256.hexdigest()
        except Exception as e:
            self.logger.error(f"è®¡ç®—æ–‡ä»¶å“ˆå¸Œå¤±è´¥: {str(e)}")
            return ""
    
    def _is_allowed_file(self, filename: str) -> bool:
        """
        æ£€æŸ¥æ–‡ä»¶ç±»å‹æ˜¯å¦å…è®¸
        
        Args:
            filename: æ–‡ä»¶å
            
        Returns:
            bool: æ˜¯å¦å…è®¸ä¸Šä¼ 
        """
        if '.' not in filename:
            return False
        
        file_ext = filename.rsplit('.', 1)[1].lower()
        return file_ext in self.file_config['allowed_extensions']
    
    def upload_file(self, file: FileStorage, metadata: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """
        ä¸Šä¼ æ–‡ä»¶
        
        Args:
            file: ä¸Šä¼ çš„æ–‡ä»¶å¯¹è±¡
            metadata: æ–‡ä»¶å…ƒæ•°æ®
            
        Returns:
            Dict[str, Any]: ä¸Šä¼ ç»“æœ
        """
        try:
            # æ·»åŠ è¯¦ç»†çš„è°ƒè¯•ä¿¡æ¯
            self.logger.info(f"=== æ–‡ä»¶ä¸Šä¼ å¼€å§‹ ===")
            self.logger.info(f"æ¥æ”¶åˆ°çš„æ–‡ä»¶å¯¹è±¡: {type(file)}")
            self.logger.info(f"file.filename: '{file.filename}' (ç±»å‹: {type(file.filename)})")
            self.logger.info(f"file.filenameåŸå§‹å­—èŠ‚: {repr(file.filename.encode('utf-8')) if file.filename else 'None'}")
            
            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            if not file or file.filename == '':
                return {
                    'success': False,
                    'message': 'æœªé€‰æ‹©æ–‡ä»¶',
                    'file_id': None
                }
            
            # æ£€æŸ¥æ–‡ä»¶ç±»å‹
            if not self._is_allowed_file(file.filename):
                return {
                    'success': False,
                    'message': f'ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {file.filename}',
                    'file_id': None
                }
            
            # æ£€æŸ¥æ–‡ä»¶å¤§å°
            file.seek(0, os.SEEK_END)
            file_size = file.tell()
            file.seek(0)
            
            if file_size > self.file_config['max_file_size']:
                return {
                    'success': False,
                    'message': f'æ–‡ä»¶å¤§å°è¶…è¿‡é™åˆ¶ ({self.file_config["max_file_size"]} bytes)',
                    'file_id': None
                }
            
            # ä¿å­˜åŸå§‹æ–‡ä»¶åï¼ˆç”¨äºæ˜¾ç¤ºï¼‰
            original_filename = file.filename
            
            # ç›´æ¥ä»åŸå§‹æ–‡ä»¶åè·å–æ‰©å±•å
            if '.' in original_filename:
                file_ext = original_filename.rsplit('.', 1)[1].lower()
            else:
                file_ext = ''
            
            # è®°å½•è°ƒè¯•ä¿¡æ¯
            self.logger.info(f"ä¸Šä¼ æ–‡ä»¶è°ƒè¯•ä¿¡æ¯ - åŸå§‹æ–‡ä»¶å: {original_filename}, æ‰©å±•å: {file_ext}")
            
            # ç”Ÿæˆå”¯ä¸€çš„ç‰©ç†æ–‡ä»¶å
            timestamp = datetime.now(timezone(timedelta(hours=8))).strftime('%Y%m%d_%H%M%S')
            # å¯¹äºç£ç›˜å­˜å‚¨ï¼Œä½¿ç”¨æ›´å®‰å…¨çš„æ–‡ä»¶åï¼ˆè‹±æ–‡+æ•°å­—ï¼‰
            import hashlib
            name_hash = hashlib.md5(original_filename.encode('utf-8')).hexdigest()[:8]
            unique_filename = f"{timestamp}_{name_hash}.{file_ext}" if file_ext else f"{timestamp}_{name_hash}"
            
            # æ ¹æ®æ–‡ä»¶ç±»å‹é€‰æ‹©å­ç›®å½•
            if file_ext in ['jpg', 'jpeg', 'png', 'gif', 'bmp']:
                sub_dir = 'images'
            else:
                sub_dir = file_ext
            
            # ä¿å­˜æ–‡ä»¶åˆ°ç›¸åº”çš„å­ç›®å½•
            file_dir = os.path.join(self.file_config['upload_folder'], sub_dir)
            file_path = os.path.join(file_dir, unique_filename)
            file.save(file_path)
            
            # è®¡ç®—æ–‡ä»¶å“ˆå¸Œ
            content_hash = self._get_file_hash(file_path)
            
            # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨ç›¸åŒå†…å®¹çš„æ–‡ä»¶
            existing_file = self._check_duplicate_file(content_hash)
            if existing_file:
                # åˆ é™¤åˆšä¿å­˜çš„é‡å¤æ–‡ä»¶
                os.remove(file_path)
                return {
                    'success': True,
                    'message': 'æ–‡ä»¶å·²å­˜åœ¨',
                    'file_id': existing_file['id'],
                    'duplicate': True
                }
            
            # ä¿å­˜æ–‡ä»¶ä¿¡æ¯åˆ°æ•°æ®åº“ï¼ˆfilenameå­—æ®µä¿å­˜åŸå§‹æ–‡ä»¶åç”¨äºæ˜¾ç¤ºï¼‰
            file_data = {
                'filename': original_filename,  # ä¿å­˜åŸå§‹æ–‡ä»¶åï¼ˆåŒ…å«ä¸­æ–‡ï¼‰
                'file_path': file_path,
                'file_type': file_ext,
                'file_size': file_size,
                'upload_time': datetime.now(timezone(timedelta(hours=8))),
                'process_status': 'pending',
                'content_hash': content_hash,
                'metadata': json.dumps(metadata or {}, ensure_ascii=False)
            }
            
            # è°ƒè¯•ï¼šæ•°æ®åº“å­˜å‚¨å‰çš„æ•°æ®
            self.logger.info(f"å‡†å¤‡å­˜å‚¨åˆ°æ•°æ®åº“çš„æ•°æ®: filename='{file_data['filename']}', file_path='{file_data['file_path']}'")
            
            success = self.mysql_manager.insert_data('documents', file_data)
            
            if success:
                # è·å–æ’å…¥çš„æ–‡ä»¶ID
                file_info = self._get_file_by_hash(content_hash)
                file_id = file_info['id'] if file_info else None
                
                self.logger.info(f"æ–‡ä»¶ä¸Šä¼ æˆåŠŸ: {original_filename}, ID: {file_id}")
                
                # å¼‚æ­¥å¯åŠ¨å¤„ç†æµç¨‹
                if file_ext == 'pdf':  # åªå¯¹PDFæ–‡ä»¶è¿›è¡Œåç»­å¤„ç†
                    threading.Thread(target=self._async_process_file, args=(file_id, file_path)).start()
                
                return {
                    'success': True,
                    'message': 'æ–‡ä»¶ä¸Šä¼ æˆåŠŸ',
                    'file_id': file_id,
                    'duplicate': False
                }
            else:
                # åˆ é™¤å·²ä¿å­˜çš„æ–‡ä»¶
                os.remove(file_path)
                return {
                    'success': False,
                    'message': 'ä¿å­˜æ–‡ä»¶ä¿¡æ¯å¤±è´¥',
                    'file_id': None
                }
                
        except Exception as e:
            self.logger.error(f"æ–‡ä»¶ä¸Šä¼ å¤±è´¥: {str(e)}")
            return {
                'success': False,
                'message': f'æ–‡ä»¶ä¸Šä¼ å¤±è´¥: {str(e)}',
                'file_id': None
            }
    
    def _check_duplicate_file(self, content_hash: str) -> Optional[Dict[str, Any]]:
        """
        æ£€æŸ¥é‡å¤æ–‡ä»¶
        
        Args:
            content_hash: æ–‡ä»¶å†…å®¹å“ˆå¸Œ
            
        Returns:
            Optional[Dict[str, Any]]: é‡å¤æ–‡ä»¶ä¿¡æ¯ï¼Œä¸å­˜åœ¨è¿”å›None
        """
        try:
            query = "SELECT * FROM documents WHERE content_hash = :hash LIMIT 1"
            result = self.mysql_manager.execute_query(query, {'hash': content_hash})
            return result[0] if result else None
        except Exception as e:
            self.logger.error(f"æ£€æŸ¥é‡å¤æ–‡ä»¶å¤±è´¥: {str(e)}")
            return None
    
    def _get_file_by_hash(self, content_hash: str) -> Optional[Dict[str, Any]]:
        """
        æ ¹æ®å“ˆå¸Œè·å–æ–‡ä»¶ä¿¡æ¯
        
        Args:
            content_hash: æ–‡ä»¶å†…å®¹å“ˆå¸Œ
            
        Returns:
            Optional[Dict[str, Any]]: æ–‡ä»¶ä¿¡æ¯
        """
        try:
            query = "SELECT * FROM documents WHERE content_hash = :hash LIMIT 1"
            result = self.mysql_manager.execute_query(query, {'hash': content_hash})
            return result[0] if result else None
        except Exception as e:
            self.logger.error(f"è·å–æ–‡ä»¶ä¿¡æ¯å¤±è´¥: {str(e)}")
            return None
    
    def get_file_info(self, file_id: int) -> Optional[Dict[str, Any]]:
        """
        è·å–æ–‡ä»¶ä¿¡æ¯
        
        Args:
            file_id: æ–‡ä»¶ID
            
        Returns:
            Optional[Dict[str, Any]]: æ–‡ä»¶ä¿¡æ¯
        """
        try:
            query = "SELECT * FROM documents WHERE id = :file_id"
            result = self.mysql_manager.execute_query(query, {'file_id': file_id})
            
            if result:
                file_info = result[0]
                # è§£æå…ƒæ•°æ®
                if file_info.get('metadata'):
                    file_info['metadata'] = json.loads(file_info['metadata'])
                return file_info
            return None
            
        except Exception as e:
            self.logger.error(f"è·å–æ–‡ä»¶ä¿¡æ¯å¤±è´¥: {str(e)}")
            return None
    
    def get_file_list(self, page: int = 1, page_size: int = 20, 
                     file_type: Optional[str] = None, 
                     process_status: Optional[str] = None,
                     filename: Optional[str] = None) -> Dict[str, Any]:
        """
        è·å–æ–‡ä»¶åˆ—è¡¨
        
        Args:
            page: é¡µç 
            page_size: æ¯é¡µæ•°é‡
            file_type: æ–‡ä»¶ç±»å‹è¿‡æ»¤
            process_status: å¤„ç†çŠ¶æ€è¿‡æ»¤
            filename: æ–‡ä»¶åæ¨¡ç³Šæœç´¢ï¼ˆæ”¯æŒæ–‡ä»¶åã€å…ƒæ•°æ®ç­‰å­—æ®µï¼‰
            
        Returns:
            Dict[str, Any]: æ–‡ä»¶åˆ—è¡¨å’Œåˆ†é¡µä¿¡æ¯
        """
        try:
            # æ„å»ºæŸ¥è¯¢æ¡ä»¶
            where_conditions = []
            params = {}
            
            if file_type:
                where_conditions.append("file_type = :file_type")
                params['file_type'] = file_type
            
            if process_status:
                where_conditions.append("process_status = :process_status")
                params['process_status'] = process_status
            
            # æ·»åŠ æ–‡ä»¶åæ¨¡ç³Šæœç´¢ï¼ˆæ”¯æŒå¤šå­—æ®µæœç´¢ï¼‰
            if filename:
                # æ”¯æŒæ–‡ä»¶åã€å…ƒæ•°æ®ç­‰å­—æ®µçš„æ¨¡ç³Šæœç´¢
                search_conditions = [
                    "filename LIKE :filename_search",
                    "metadata LIKE :filename_search"
                ]
                where_conditions.append(f"({' OR '.join(search_conditions)})")
                params['filename_search'] = f"%{filename}%"
            
            where_clause = " AND ".join(where_conditions) if where_conditions else "1=1"
            
            # è®¡ç®—æ€»æ•°
            count_query = f"SELECT COUNT(*) as total FROM documents WHERE {where_clause}"
            count_result = self.mysql_manager.execute_query(count_query, params)
            total = count_result[0]['total'] if count_result else 0
            
            # è®¡ç®—åç§»é‡
            offset = (page - 1) * page_size
            params['limit'] = page_size
            params['offset'] = offset
            
            # æŸ¥è¯¢æ–‡ä»¶åˆ—è¡¨
            list_query = f"""
            SELECT id, filename, file_type, file_size, upload_time, process_status, process_time
            FROM documents 
            WHERE {where_clause}
            ORDER BY upload_time DESC
            LIMIT :limit OFFSET :offset
            """
            
            files = self.mysql_manager.execute_query(list_query, params)
            
            return {
                'files': files,
                'total': total,
                'page': page,
                'page_size': page_size,
                'total_pages': (total + page_size - 1) // page_size
            }
            
        except Exception as e:
            self.logger.error(f"è·å–æ–‡ä»¶åˆ—è¡¨å¤±è´¥: {str(e)}")
            return {
                'files': [],
                'total': 0,
                'page': page,
                'page_size': page_size,
                'total_pages': 0
            }
    
    def update_file_status(self, file_id: int, status: str, process_time: Optional[datetime] = None, send_websocket: bool = True) -> bool:
        """
        æ›´æ–°æ–‡ä»¶å¤„ç†çŠ¶æ€
        
        Args:
            file_id: æ–‡ä»¶ID
            status: æ–°çŠ¶æ€
            process_time: å¤„ç†æ—¶é—´
            
        Returns:
            bool: æ›´æ–°æˆåŠŸè¿”å›True
        """
        try:
            update_data = {'process_status': status}
            if process_time:
                update_data['process_time'] = process_time
            else:
                update_data['process_time'] = datetime.now(timezone(timedelta(hours=8)))
            
            success = self.mysql_manager.update_data(
                'documents',
                update_data,
                'id = :file_id',
                {'file_id': file_id}
            )
            
            if success:
                self.logger.info(f"æ–‡ä»¶çŠ¶æ€æ›´æ–°æˆåŠŸï¼ŒID: {file_id}, çŠ¶æ€: {status}")
                
                # å‘é€WebSocketè¿›åº¦æ›´æ–°
                if send_websocket:
                    self._send_progress_update(file_id, status, process_time)
            
            return success
            
        except Exception as e:
            self.logger.error(f"æ›´æ–°æ–‡ä»¶çŠ¶æ€å¤±è´¥: {str(e)}")
            return False
    
    def _send_progress_update(self, file_id: int, status: str, process_time: Optional[datetime] = None):
        """
        å‘é€WebSocketè¿›åº¦æ›´æ–°
        
        Args:
            file_id: æ–‡ä»¶ID
            status: å¤„ç†çŠ¶æ€
            process_time: å¤„ç†æ—¶é—´
        """
        try:
            from app.utils.websocket import send_file_progress
            
            # è®¡ç®—è¿›åº¦æ•°æ®
            progress_data = self._calculate_progress(status)
            
            # æ·»åŠ æ—¶é—´æˆ³
            if process_time:
                progress_data['timestamp'] = process_time.isoformat()
            else:
                progress_data['timestamp'] = datetime.now(timezone(timedelta(hours=8))).isoformat()
            
            # å‘é€WebSocketæ¶ˆæ¯
            send_file_progress(file_id, progress_data)
            
        except Exception as e:
            self.logger.error(f"å‘é€WebSocketè¿›åº¦æ›´æ–°å¤±è´¥: {str(e)}")
    
    def _calculate_progress(self, status: str) -> Dict[str, Any]:
        """
        æ ¹æ®çŠ¶æ€è®¡ç®—è¿›åº¦
        
        Args:
            status: å¤„ç†çŠ¶æ€
            
        Returns:
            Dict[str, Any]: è¿›åº¦ä¿¡æ¯
        """
        progress_map = {
            'pending': {'progress': 10, 'stage': 'uploaded', 'stage_name': 'æ–‡ä»¶å·²ä¸Šä¼ '},
            'extracting': {'progress': 25, 'stage': 'extracting', 'stage_name': 'å†…å®¹æå–ä¸­'},
            'extracted': {'progress': 40, 'stage': 'extracted', 'stage_name': 'å†…å®¹æå–å®Œæˆ'},
            'vectorizing': {'progress': 50, 'stage': 'vectorizing', 'stage_name': 'å‘é‡åŒ–å¤„ç†ä¸­'},
            'vectorized': {'progress': 60, 'stage': 'vectorized', 'stage_name': 'å‘é‡åŒ–å®Œæˆ'},
            'bm25_processing': {'progress': 65, 'stage': 'bm25_processing', 'stage_name': 'BM25å€’æ’å¤„ç†ä¸­'},
            'bm25_completed': {'progress': 70, 'stage': 'bm25_completed', 'stage_name': 'BM25å€’æ’å®Œæˆ'},
            'graph_processing': {'progress': 80, 'stage': 'graph_processing', 'stage_name': 'çŸ¥è¯†å›¾è°±æ„å»ºä¸­'},
            'graph_completed': {'progress': 85, 'stage': 'graph_completed', 'stage_name': 'çŸ¥è¯†å›¾è°±æ„å»ºå®Œæˆ'},
            'mysql_processing': {'progress': 95, 'stage': 'mysql_processing', 'stage_name': 'MySQLä¿å­˜ä¸­'},
            'completed': {'progress': 100, 'stage': 'completed', 'stage_name': 'å¤„ç†å®Œæˆ'},
            'extract_failed': {'progress': 40, 'stage': 'extract_failed', 'stage_name': 'å†…å®¹æå–å¤±è´¥'},
            'vectorize_failed': {'progress': 60, 'stage': 'vectorize_failed', 'stage_name': 'å‘é‡åŒ–å¤±è´¥'},
            'bm25_failed': {'progress': 70, 'stage': 'bm25_failed', 'stage_name': 'BM25å€’æ’å¤±è´¥'},
            'graph_failed': {'progress': 85, 'stage': 'graph_failed', 'stage_name': 'çŸ¥è¯†å›¾è°±æ„å»ºå¤±è´¥'},
            'mysql_failed': {'progress': 95, 'stage': 'mysql_failed', 'stage_name': 'MySQLä¿å­˜å¤±è´¥'},
            'process_failed': {'progress': 0, 'stage': 'process_failed', 'stage_name': 'å¤„ç†å¤±è´¥'}
        }
        
        return progress_map.get(status, {'progress': 0, 'stage': 'unknown', 'stage_name': 'æœªçŸ¥çŠ¶æ€'})
    
    def _delete_processed_files(self, file_id: int, original_filename: str) -> Dict[str, bool]:
        """
        åˆ é™¤å¤„ç†è¿‡ç¨‹ä¸­ç”Ÿæˆçš„æ–‡ä»¶
        
        Args:
            file_id: æ–‡ä»¶ID
            original_filename: åŸå§‹æ–‡ä»¶å
            
        Returns:
            Dict[str, bool]: å„ç±»æ–‡ä»¶åˆ é™¤ç»“æœ
        """
        results = {
            'json_files': False,
            'bm25_files': False,
            'figure_files': False
        }
        
        try:
            upload_folder = self.file_config['upload_folder']
            
            # 1. åˆ é™¤JSONæ–‡ä»¶
            json_dir = os.path.join(upload_folder, 'json')
            if os.path.exists(json_dir):
                # å¯èƒ½çš„JSONæ–‡ä»¶åæ¨¡å¼
                base_name = os.path.splitext(original_filename)[0]
                json_patterns = [
                    f"{base_name}_doc_{file_id}.json",
                    f"*_doc_{file_id}.json"
                ]
                
                json_deleted = 0
                for pattern in json_patterns:
                    if '*' in pattern:
                        import glob
                        matching_files = glob.glob(os.path.join(json_dir, pattern))
                        for file_path in matching_files:
                            try:
                                os.remove(file_path)
                                json_deleted += 1
                                self.logger.info(f"åˆ é™¤JSONæ–‡ä»¶: {file_path}")
                            except Exception as e:
                                self.logger.warning(f"åˆ é™¤JSONæ–‡ä»¶å¤±è´¥: {file_path}, é”™è¯¯: {e}")
                    else:
                        file_path = os.path.join(json_dir, pattern)
                        if os.path.exists(file_path):
                            try:
                                os.remove(file_path)
                                json_deleted += 1
                                self.logger.info(f"åˆ é™¤JSONæ–‡ä»¶: {file_path}")
                            except Exception as e:
                                self.logger.warning(f"åˆ é™¤JSONæ–‡ä»¶å¤±è´¥: {file_path}, é”™è¯¯: {e}")
                
                results['json_files'] = json_deleted > 0
            
            # 2. åˆ é™¤BM25ç´¢å¼•æ–‡ä»¶
            bm25_dir = os.path.join(upload_folder, 'bm25')
            if os.path.exists(bm25_dir):
                import glob
                # BM25æ–‡ä»¶åæ¨¡å¼: bm25_*_{file_id}.json
                bm25_patterns = [
                    f"bm25_*_{file_id}.json",
                    f"bm25_combined_{file_id}.json",
                    f"bm25_sections_{file_id}.json", 
                    f"bm25_fragments_{file_id}.json"
                ]
                
                bm25_deleted = 0
                for pattern in bm25_patterns:
                    matching_files = glob.glob(os.path.join(bm25_dir, pattern))
                    for file_path in matching_files:
                        try:
                            os.remove(file_path)
                            bm25_deleted += 1
                            self.logger.info(f"åˆ é™¤BM25æ–‡ä»¶: {file_path}")
                        except Exception as e:
                            self.logger.warning(f"åˆ é™¤BM25æ–‡ä»¶å¤±è´¥: {file_path}, é”™è¯¯: {e}")
                
                results['bm25_files'] = bm25_deleted > 0
            
            # 3. åˆ é™¤æå–çš„å›¾ç‰‡æ–‡ä»¶
            # ä¿®å¤ï¼šfiguresç›®å½•åœ¨é¡¹ç›®æ ¹ç›®å½•ï¼Œä¸åœ¨uploadç›®å½•ä¸‹
            project_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
            figures_dir = os.path.join(project_root, 'figures')
            
            figure_deleted = 0
            
            # æ–¹æ³•1ï¼šé€šè¿‡MySQL figuresè¡¨ç²¾ç¡®æŸ¥æ‰¾å›¾ç‰‡è·¯å¾„
            try:
                # æŸ¥è¯¢è¯¥æ–‡æ¡£ç›¸å…³çš„æ‰€æœ‰å›¾ç‰‡è·¯å¾„
                query = """
                SELECT DISTINCT f.image_path 
                FROM figures f 
                JOIN sections s ON f.section_id = s.section_id 
                WHERE s.document_id = :doc_id AND f.image_path IS NOT NULL AND f.image_path != ''
                """
                
                figure_records = self.mysql_manager.fetch_all(query, {'doc_id': file_id})
                
                for record in figure_records:
                    image_path = record.get('image_path', '')
                    if image_path:
                        # æ„å»ºå®Œæ•´è·¯å¾„
                        if image_path.startswith('figures/'):
                            full_path = os.path.join(project_root, image_path)
                        else:
                            # å¦‚æœè·¯å¾„ä¸ä»¥figures/å¼€å¤´ï¼Œå‡è®¾å®ƒæ˜¯ç›¸å¯¹äºfiguresç›®å½•çš„
                            full_path = os.path.join(figures_dir, os.path.basename(image_path))
                        
                        if os.path.exists(full_path):
                            try:
                                os.remove(full_path)
                                figure_deleted += 1
                                self.logger.info(f"é€šè¿‡MySQLè®°å½•åˆ é™¤å›¾ç‰‡æ–‡ä»¶: {full_path}")
                            except Exception as e:
                                self.logger.warning(f"åˆ é™¤å›¾ç‰‡æ–‡ä»¶å¤±è´¥: {full_path}, é”™è¯¯: {e}")
                
            except Exception as e:
                self.logger.warning(f"é€šè¿‡MySQLæŸ¥æ‰¾å›¾ç‰‡æ–‡ä»¶å¤±è´¥: {e}")
            
            # æ–¹æ³•2ï¼šæ¨¡å¼åŒ¹é…åˆ é™¤ï¼ˆä½œä¸ºå¤‡ç”¨æ–¹æ¡ˆï¼‰
            if os.path.exists(figures_dir):
                import glob
                # åŸºäºæ–‡ä»¶åæ¨¡å¼çš„åˆ é™¤ï¼ˆä½œä¸ºå¤‡ç”¨æ–¹æ¡ˆï¼‰
                figure_patterns = [
                    f"*{file_id}*",           # ç›´æ¥åŒ…å«ID
                    f"*_{file_id}_*",         # IDå‰åæœ‰ä¸‹åˆ’çº¿
                    f"figure-{file_id}-*",    # figure-ID-åºå· æ ¼å¼
                    f"*doc_{file_id}*"        # åŒ…å«doc_IDçš„æ ¼å¼
                ]
                
                pattern_deleted = 0
                for pattern in figure_patterns:
                    matching_files = glob.glob(os.path.join(figures_dir, pattern))
                    for file_path in matching_files:
                        try:
                            os.remove(file_path)
                            pattern_deleted += 1
                            self.logger.info(f"é€šè¿‡æ¨¡å¼åŒ¹é…åˆ é™¤å›¾ç‰‡æ–‡ä»¶: {file_path}")
                        except Exception as e:
                            self.logger.warning(f"åˆ é™¤å›¾ç‰‡æ–‡ä»¶å¤±è´¥: {file_path}, é”™è¯¯: {e}")
                
                figure_deleted += pattern_deleted
                
            results['figure_files'] = figure_deleted > 0
            self.logger.info(f"figuresç›®å½•è·¯å¾„: {figures_dir}, åˆ é™¤å›¾ç‰‡æ€»æ•°: {figure_deleted}")
            
            self.logger.info(f"æ–‡ä»¶æ¸…ç†å®Œæˆï¼Œæ–‡æ¡£ID: {file_id}, ç»“æœ: {results}")
            
        except Exception as e:
            self.logger.error(f"æ¸…ç†å¤„ç†æ–‡ä»¶æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
        
        return results
    
    def delete_file(self, file_id: int) -> bool:
        """
        å®Œæ•´åˆ é™¤æ–‡ä»¶åŠå…¶æ‰€æœ‰ç›¸å…³æ•°æ®
        
        Args:
            file_id: æ–‡ä»¶ID
            
        Returns:
            bool: åˆ é™¤æˆåŠŸè¿”å›True
        """
        try:
            # è·å–æ–‡ä»¶ä¿¡æ¯
            file_info = self.get_file_info(file_id)
            if not file_info:
                self.logger.warning(f"æ–‡ä»¶ä¸å­˜åœ¨ï¼ŒID: {file_id}")
                return False
            
            self.logger.info(f"å¼€å§‹åˆ é™¤æ–‡ä»¶ï¼š{file_info['filename']} (ID: {file_id})")
            
            # è®°å½•åˆ é™¤è¿‡ç¨‹ä¸­çš„é”™è¯¯ï¼Œä½†ä¸ä¸­æ–­åˆ é™¤è¿‡ç¨‹
            deletion_errors = []
            success_count = 0
            total_operations = 7
            
            # 1. åˆ é™¤Milvuså‘é‡æ•°æ®
            try:
                from utils.MilvusManager import MilvusManager
                milvus_manager = MilvusManager()
                if milvus_manager.delete_by_document_id(file_id):
                    success_count += 1
                    self.logger.info(f"âœ“ Milvuså‘é‡æ•°æ®åˆ é™¤æˆåŠŸ")
                else:
                    deletion_errors.append("Milvuså‘é‡æ•°æ®åˆ é™¤å¤±è´¥")
            except Exception as e:
                deletion_errors.append(f"Milvuså‘é‡æ•°æ®åˆ é™¤å¼‚å¸¸: {str(e)}")
                self.logger.warning(f"Milvuså‘é‡æ•°æ®åˆ é™¤å¼‚å¸¸: {str(e)}")
            
            # 1.5. åˆ é™¤OpenSearchç´¢å¼•æ•°æ®
            try:
                from app.service.pdf.PdfOpenSearchService import PdfOpenSearchService
                opensearch_service = PdfOpenSearchService()
                if opensearch_service.delete_document_from_opensearch(file_id):
                    success_count += 1
                    self.logger.info(f"âœ“ OpenSearchç´¢å¼•æ•°æ®åˆ é™¤æˆåŠŸ")
                else:
                    deletion_errors.append("OpenSearchç´¢å¼•æ•°æ®åˆ é™¤å¤±è´¥")
            except Exception as e:
                deletion_errors.append(f"OpenSearchç´¢å¼•æ•°æ®åˆ é™¤å¼‚å¸¸: {str(e)}")
                self.logger.warning(f"OpenSearchç´¢å¼•æ•°æ®åˆ é™¤å¼‚å¸¸: {str(e)}")
            
            # 2. åˆ é™¤Neo4jå›¾æ•°æ®
            try:
                from utils.Neo4jManager import Neo4jManager
                neo4j_manager = Neo4jManager()
                if neo4j_manager.delete_document_data(file_id):
                    success_count += 1
                    self.logger.info(f"âœ“ Neo4jå›¾æ•°æ®åˆ é™¤æˆåŠŸ")
                else:
                    deletion_errors.append("Neo4jå›¾æ•°æ®åˆ é™¤å¤±è´¥")
            except Exception as e:
                deletion_errors.append(f"Neo4jå›¾æ•°æ®åˆ é™¤å¼‚å¸¸: {str(e)}")
                self.logger.warning(f"Neo4jå›¾æ•°æ®åˆ é™¤å¼‚å¸¸: {str(e)}")
            
            # 3. åˆ é™¤å¤„ç†è¿‡ç¨‹ä¸­ç”Ÿæˆçš„æ–‡ä»¶ï¼ˆJSONã€BM25ã€å›¾ç‰‡ç­‰ï¼‰
            try:
                file_deletion_results = self._delete_processed_files(file_id, file_info['filename'])
                if any(file_deletion_results.values()):
                    success_count += 1
                    self.logger.info(f"âœ“ å¤„ç†æ–‡ä»¶åˆ é™¤æˆåŠŸ: {file_deletion_results}")
                else:
                    self.logger.info("æ²¡æœ‰æ‰¾åˆ°éœ€è¦åˆ é™¤çš„å¤„ç†æ–‡ä»¶")
                    success_count += 1  # æ²¡æœ‰æ–‡ä»¶ä¹Ÿç®—æˆåŠŸ
            except Exception as e:
                deletion_errors.append(f"å¤„ç†æ–‡ä»¶åˆ é™¤å¼‚å¸¸: {str(e)}")
                self.logger.warning(f"å¤„ç†æ–‡ä»¶åˆ é™¤å¼‚å¸¸: {str(e)}")
            
            # 4. åˆ é™¤åŸå§‹ç‰©ç†æ–‡ä»¶
            try:
                if os.path.exists(file_info['file_path']):
                    os.remove(file_info['file_path'])
                    success_count += 1
                    self.logger.info(f"âœ“ åŸå§‹æ–‡ä»¶åˆ é™¤æˆåŠŸ: {file_info['file_path']}")
                else:
                    self.logger.info("åŸå§‹æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè·³è¿‡åˆ é™¤")
                    success_count += 1  # æ–‡ä»¶ä¸å­˜åœ¨ä¹Ÿç®—æˆåŠŸ
            except Exception as e:
                deletion_errors.append(f"åŸå§‹æ–‡ä»¶åˆ é™¤å¤±è´¥: {str(e)}")
                self.logger.warning(f"åŸå§‹æ–‡ä»¶åˆ é™¤å¤±è´¥: {str(e)}")
            
            # 5. åˆ é™¤MySQLæ•°æ®åº“è®°å½•ï¼ˆè¿™ä¼šè§¦å‘çº§è”åˆ é™¤ï¼‰
            try:
                mysql_success = self.mysql_manager.delete_data(
                    'documents',
                    'id = :file_id',
                    {'file_id': file_id}
                )
                if mysql_success:
                    success_count += 1
                    self.logger.info(f"âœ“ MySQLæ•°æ®åˆ é™¤æˆåŠŸï¼ˆåŒ…æ‹¬çº§è”åˆ é™¤sectionsã€figuresã€tablesã€table_rowsï¼‰")
                else:
                    deletion_errors.append("MySQLæ•°æ®åˆ é™¤å¤±è´¥")
            except Exception as e:
                deletion_errors.append(f"MySQLæ•°æ®åˆ é™¤å¼‚å¸¸: {str(e)}")
                self.logger.warning(f"MySQLæ•°æ®åˆ é™¤å¼‚å¸¸: {str(e)}")
            
            # 6. è®°å½•æ“ä½œæ—¥å¿—ï¼ˆè¿™å·²ç»é€šè¿‡æ•°æ®åº“è§¦å‘å™¨è‡ªåŠ¨è®°å½•äº†ï¼‰
            success_count += 1
            
            # è¯„ä¼°åˆ é™¤ç»“æœ
            if success_count >= 4:  # è‡³å°‘å®Œæˆæ ¸å¿ƒåˆ é™¤æ“ä½œ
                self.logger.info(f"æ–‡ä»¶åˆ é™¤æˆåŠŸï¼ŒID: {file_id}, æˆåŠŸæ“ä½œ: {success_count}/{total_operations}")
                if deletion_errors:
                    self.logger.warning(f"åˆ é™¤è¿‡ç¨‹ä¸­çš„è­¦å‘Š: {'; '.join(deletion_errors)}")
                return True
            else:
                self.logger.error(f"æ–‡ä»¶åˆ é™¤å¤±è´¥ï¼ŒID: {file_id}, æˆåŠŸæ“ä½œ: {success_count}/{total_operations}")
                self.logger.error(f"åˆ é™¤é”™è¯¯: {'; '.join(deletion_errors)}")
                return False
            
        except Exception as e:
            self.logger.error(f"åˆ é™¤æ–‡ä»¶è¿‡ç¨‹ä¸­å‘ç”Ÿä¸¥é‡é”™è¯¯: {str(e)}")
            return False
    
    def get_file_stats(self) -> Dict[str, Any]:
        """
        è·å–æ–‡ä»¶ç»Ÿè®¡ä¿¡æ¯
        
        Returns:
            Dict[str, Any]: ç»Ÿè®¡ä¿¡æ¯
        """
        try:
            queries = {
                'total_files': "SELECT COUNT(*) as count FROM documents",
                'total_size': "SELECT SUM(file_size) as size FROM documents",
                'by_type': """
                    SELECT file_type, COUNT(*) as count, SUM(file_size) as size
                    FROM documents 
                    GROUP BY file_type
                """,
                'by_status': """
                    SELECT process_status, COUNT(*) as count
                    FROM documents 
                    GROUP BY process_status
                """
            }
            
            stats = {}
            for key, query in queries.items():
                result = self.mysql_manager.execute_query(query)
                if key in ['total_files', 'total_size']:
                    stats[key] = result[0][list(result[0].keys())[0]] if result else 0
                else:
                    stats[key] = result
            
            return stats
            
        except Exception as e:
            self.logger.error(f"è·å–æ–‡ä»¶ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {str(e)}")
            return {}
    
    def cleanup_temp_files(self, max_age_hours: int = 24) -> int:
        """
        æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        
        Args:
            max_age_hours: æœ€å¤§ä¿ç•™æ—¶é—´ï¼ˆå°æ—¶ï¼‰
            
        Returns:
            int: æ¸…ç†çš„æ–‡ä»¶æ•°é‡
        """
        try:
            temp_folder = self.file_config['temp_folder']
            if not os.path.exists(temp_folder):
                return 0
            
            current_time = datetime.now(timezone(timedelta(hours=8)))
            max_age_seconds = max_age_hours * 3600
            cleaned_count = 0
            
            for filename in os.listdir(temp_folder):
                file_path = os.path.join(temp_folder, filename)
                if os.path.isfile(file_path):
                    file_age = current_time.timestamp() - os.path.getmtime(file_path)
                    if file_age > max_age_seconds:
                        os.remove(file_path)
                        cleaned_count += 1
                        self.logger.info(f"æ¸…ç†ä¸´æ—¶æ–‡ä»¶: {filename}")
            
            self.logger.info(f"ä¸´æ—¶æ–‡ä»¶æ¸…ç†å®Œæˆï¼Œå…±æ¸…ç†{cleaned_count}ä¸ªæ–‡ä»¶")
            return cleaned_count
            
        except Exception as e:
            self.logger.error(f"æ¸…ç†ä¸´æ—¶æ–‡ä»¶å¤±è´¥: {str(e)}")
            return 0
    
    def _async_process_file(self, file_id: int, file_path: str) -> None:
        """
        å¼‚æ­¥å¤„ç†æ–‡ä»¶ï¼ˆæå–å†…å®¹ã€å‘é‡åŒ–ã€çŸ¥è¯†å›¾è°±ï¼‰
        
        Args:
            file_id: æ–‡ä»¶ID
            file_path: æ–‡ä»¶è·¯å¾„
        """
        try:
            # å¯¼å…¥å¤„ç†æœåŠ¡
            from app.service.pdf.PdfExtractService import PdfExtractService
            from app.service.pdf.PdfFormatElementsToJson import PdfFormatElementsToJson
            from app.service.pdf.PdfVectorService import PdfVectorService
            from app.service.pdf.PdfOpenSearchService import PdfOpenSearchService
            from app.service.pdf.PdfGraphService import PdfGraphService
            from app.service.pdf.PdfMysqlService import PdfMysqlService
            
            pdf_extract_service = PdfExtractService()
            pdf_format_service = PdfFormatElementsToJson()
            pdf_vector_service = PdfVectorService()
            pdf_opensearch_service = PdfOpenSearchService()
            pdf_graph_service = PdfGraphService()
            pdf_mysql_service = PdfMysqlService()
            
            self.logger.info(f"å¼€å§‹å¼‚æ­¥å¤„ç†æ–‡ä»¶ï¼ŒID: {file_id}")
            
            # æ­¥éª¤1ï¼šå†…å®¹æå– (10% -> 25%)
            self.update_file_status(file_id, 'extracting')
            elements = pdf_extract_service.extract_pdf_content(file_path, file_id)
            
            if elements is None:
                self.update_file_status(file_id, 'extract_failed')
                self.logger.error(f"æ–‡ä»¶å†…å®¹æå–å¤±è´¥ï¼ŒID: {file_id}")
                return

            # æ­¥éª¤1.1ï¼šæ ¼å¼åŒ–elements (25% -> 35%)
            format_result = pdf_format_service.format_elements_to_json(elements, file_id, file_path)
            
            if not format_result['success']:
                self.update_file_status(file_id, 'extract_failed')
                self.logger.error(f"å…ƒç´ æ ¼å¼åŒ–å¤±è´¥ï¼ŒID: {file_id}, é”™è¯¯: {format_result['message']}")
                return
            
            json_data = format_result['json_data']
            self.logger.info(f"å…ƒç´ æ ¼å¼åŒ–å®Œæˆï¼ŒID: {file_id}")
            
            # æ­¥éª¤1.2ï¼šä¿å­˜JSONæ–‡ä»¶ (35% -> 40%)
            json_file_path = self._save_json_data(json_data, file_path, file_id)
            
            if not json_file_path:
                self.update_file_status(file_id, 'extract_failed')
                self.logger.error(f"ä¿å­˜JSONæ–‡ä»¶å¤±è´¥ï¼ŒID: {file_id}")
                return
                
            self.update_file_status(file_id, 'extracted')
            self.logger.info(f"JSONæ–‡ä»¶ä¿å­˜å®Œæˆï¼ŒID: {file_id}, è·¯å¾„: {json_file_path}")
            
            # æ­¥éª¤2ï¼šå‘é‡åŒ– (40% -> 60%)
            self.update_file_status(file_id, 'vectorizing')
            vector_result = pdf_vector_service.process_pdf_json_to_vectors(json_data, file_id)
            
            if not vector_result['success']:
                self.update_file_status(file_id, 'vectorize_failed')
                self.logger.error(f"æ–‡ä»¶å‘é‡åŒ–å¤±è´¥ï¼ŒID: {file_id}, é”™è¯¯: {vector_result['message']}")
                return
                
            self.update_file_status(file_id, 'vectorized')
            self.logger.info(f"æ–‡ä»¶å‘é‡åŒ–å®Œæˆï¼ŒID: {file_id}")
            
            # æ­¥éª¤3ï¼šOpenSearchç´¢å¼• (60% -> 70%)
            self.update_file_status(file_id, 'bm25_processing')
            opensearch_result = pdf_opensearch_service.process_pdf_json_to_opensearch(json_data, file_id)
            
            if not opensearch_result['success']:
                self.update_file_status(file_id, 'bm25_failed')
                self.logger.error(f"OpenSearchç´¢å¼•å¤„ç†å¤±è´¥ï¼ŒID: {file_id}, é”™è¯¯: {opensearch_result['message']}")
                return
                
            self.update_file_status(file_id, 'bm25_completed')
            self.logger.info(f"OpenSearchç´¢å¼•å¤„ç†å®Œæˆï¼ŒID: {file_id}")
            
            # æ­¥éª¤4ï¼šçŸ¥è¯†å›¾è°±æ„å»º (70% -> 85%)
            self.update_file_status(file_id, 'graph_processing')
            graph_result = pdf_graph_service.process_pdf_json_to_graph(json_data, file_id)
            
            if not graph_result['success']:
                self.update_file_status(file_id, 'graph_failed')
                self.logger.error(f"çŸ¥è¯†å›¾è°±æ„å»ºå¤±è´¥ï¼ŒID: {file_id}, é”™è¯¯: {graph_result['message']}")
                return
                
            self.update_file_status(file_id, 'graph_completed')
            self.logger.info(f"çŸ¥è¯†å›¾è°±æ„å»ºå®Œæˆï¼ŒID: {file_id}")
            
            # æ­¥éª¤5ï¼šMySQLä¿å­˜ (85% -> 100%)
            self.update_file_status(file_id, 'mysql_processing')
            mysql_result = pdf_mysql_service.process_pdf_json_to_mysql(json_data, file_id)
            
            if not mysql_result['success']:
                self.update_file_status(file_id, 'mysql_failed')
                self.logger.error(f"MySQLä¿å­˜å¤±è´¥ï¼ŒID: {file_id}, é”™è¯¯: {mysql_result['message']}")
                return
                
            # æ‰€æœ‰æ­¥éª¤å®Œæˆ
            self.update_file_status(file_id, 'completed')
            self.logger.info(f"æ–‡ä»¶å¤„ç†å…¨éƒ¨å®Œæˆï¼ŒID: {file_id}")
            
        except Exception as e:
            self.logger.error(f"å¼‚æ­¥æ–‡ä»¶å¤„ç†å¤±è´¥ï¼ŒID: {file_id}, é”™è¯¯: {str(e)}")
            self.update_file_status(file_id, 'process_failed')
    
    def _get_json_file_path(self, pdf_file_path: str, file_id: int) -> Optional[str]:
        """
        è·å–æå–åçš„JSONæ–‡ä»¶è·¯å¾„
        
        Args:
            pdf_file_path: PDFæ–‡ä»¶è·¯å¾„
            file_id: æ–‡ä»¶ID
            
        Returns:
            Optional[str]: JSONæ–‡ä»¶è·¯å¾„
        """
        try:
            # æ ¹æ®PDFæ–‡ä»¶è·¯å¾„æ¨æµ‹JSONæ–‡ä»¶è·¯å¾„
            # é€šå¸¸ä¿å­˜åœ¨upload/jsonç›®å½•ä¸‹
            filename = os.path.basename(pdf_file_path)
            name_without_ext = os.path.splitext(filename)[0]
            
            # ğŸ”§ ä¿®å¤ï¼šä½¿ç”¨æ­£ç¡®çš„æ–‡ä»¶IDç”Ÿæˆæ–‡ä»¶å
            possible_names = [
                f"{name_without_ext}_doc_{file_id}.json",  # ä½¿ç”¨å®é™…çš„file_id
                f"{name_without_ext}_content_units.json",
                f"{name_without_ext}_doc_1.json"  # ä¿ç•™å…¼å®¹æ€§
            ]
            
            json_dir = os.path.join(self.file_config['upload_folder'], 'json')
            
            for json_name in possible_names:
                json_path = os.path.join(json_dir, json_name)
                if os.path.exists(json_path):
                    self.logger.info(f"æ‰¾åˆ°JSONæ–‡ä»¶: {json_path}")
                    return json_path
            
            # ğŸ”§ å¢å¼ºï¼šå¦‚æœæ‰¾ä¸åˆ°ï¼Œè®°å½•è¯¦ç»†ä¿¡æ¯
            self.logger.warning(f"åœ¨ç›®å½• {json_dir} ä¸­æ‰¾ä¸åˆ°ä»¥ä¸‹ä»»ä½•æ–‡ä»¶: {possible_names}")
            return None
            
        except Exception as e:
            self.logger.error(f"è·å–JSONæ–‡ä»¶è·¯å¾„å¤±è´¥: {str(e)}")
            return None

    
    def _save_json_data(self, json_data: Dict[str, Any], file_path: str, document_id: int) -> Optional[str]:
        """
        ä¿å­˜JSONæ•°æ®åˆ°æ–‡ä»¶
        
        Args:
            json_data: JSONæ•°æ®
            file_path: åŸå§‹PDFæ–‡ä»¶è·¯å¾„
            document_id: æ–‡æ¡£ID
            
        Returns:
            Optional[str]: ä¿å­˜çš„JSONæ–‡ä»¶è·¯å¾„ï¼Œå¤±è´¥è¿”å›None
        """
        try:
            # è·å–JSONè¾“å‡ºç›®å½•
            json_dir = os.path.join(self.file_config['upload_folder'], 'json')
            
            # ç¡®ä¿ç›®å½•å­˜åœ¨
            if not os.path.exists(json_dir):
                os.makedirs(json_dir, exist_ok=True)
                self.logger.info(f"åˆ›å»ºJSONè¾“å‡ºç›®å½•: {json_dir}")
            
            # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶å
            pdf_filename = os.path.basename(file_path)
            pdf_name_without_ext = os.path.splitext(pdf_filename)[0]
            json_filename = f"{pdf_name_without_ext}_doc_{document_id}.json"
            json_file_path = os.path.join(json_dir, json_filename)
            
            # ä¿å­˜JSONæ–‡ä»¶
            with open(json_file_path, 'w', encoding='utf-8') as f:
                json.dump(json_data, f, ensure_ascii=False, indent=2, default=str)
            
            self.logger.info(f"JSONæ•°æ®å·²ä¿å­˜åˆ°: {json_file_path}")
            return json_file_path
            
        except Exception as e:
            self.logger.error(f"ä¿å­˜JSONæ•°æ®åˆ°æ–‡ä»¶å¤±è´¥: {str(e)}")
            return None