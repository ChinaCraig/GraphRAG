"""
æ–‡ä»¶ç®¡ç†æœåŠ¡
è´Ÿè´£æ–‡ä»¶çš„ä¸Šä¼ ã€å¤„ç†ã€å­˜å‚¨å’Œç®¡ç†åŠŸèƒ½
"""

import os
import hashlib
import logging
import yaml
from typing import Optional, Dict, Any, List
from datetime import datetime
from werkzeug.datastructures import FileStorage
from werkzeug.utils import secure_filename
import json
import threading

from utils.MySQLManager import MySQLManager


class FileService:
    """æ–‡ä»¶ç®¡ç†æœåŠ¡ç±»"""
    
    def __init__(self, config_path: str = 'config/config.yaml'):
        """
        åˆå§‹åŒ–æ–‡ä»¶æœåŠ¡
        
        Args:
            config_path: é…ç½®æ–‡ä»¶è·¯å¾„
        """
        self.config_path = config_path
        self.logger = logging.getLogger(__name__)
        
        # åŠ è½½é…ç½®
        self._load_config()
        
        # åˆå§‹åŒ–æ•°æ®åº“ç®¡ç†å™¨
        self.mysql_manager = MySQLManager()
        
        # åˆ›å»ºå¿…è¦çš„ç›®å½•
        self._create_directories()
    
    def _load_config(self) -> None:
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        try:
            with open(self.config_path, 'r', encoding='utf-8') as file:
                config = yaml.safe_load(file)
                self.file_config = config['file']
                self.logger.info("æ–‡ä»¶æœåŠ¡é…ç½®åŠ è½½æˆåŠŸ")
        except Exception as e:
            self.logger.error(f"åŠ è½½æ–‡ä»¶æœåŠ¡é…ç½®å¤±è´¥: {str(e)}")
            raise
    
    def _create_directories(self) -> None:
        """åˆ›å»ºå¿…è¦çš„ç›®å½•"""
        directories = [
            self.file_config['upload_folder'],
            self.file_config['temp_folder']
        ]
        
        # æ·»åŠ æ–‡ä»¶ç±»å‹å­ç›®å½•
        upload_folder = self.file_config['upload_folder']
        file_type_dirs = ['pdf', 'doc', 'docx', 'xlsx', 'xls', 'pptx', 'ppt', 'txt', 'md', 'images']
        for file_type in file_type_dirs:
            directories.append(os.path.join(upload_folder, file_type))
        
        for directory in directories:
            if not os.path.exists(directory):
                os.makedirs(directory, exist_ok=True)
                self.logger.info(f"åˆ›å»ºç›®å½•: {directory}")
    
    def _get_file_hash(self, file_path: str) -> str:
        """
        è®¡ç®—æ–‡ä»¶å“ˆå¸Œå€¼
        
        Args:
            file_path: æ–‡ä»¶è·¯å¾„
            
        Returns:
            str: æ–‡ä»¶çš„SHA256å“ˆå¸Œå€¼
        """
        hash_sha256 = hashlib.sha256()
        try:
            with open(file_path, "rb") as f:
                for chunk in iter(lambda: f.read(4096), b""):
                    hash_sha256.update(chunk)
            return hash_sha256.hexdigest()
        except Exception as e:
            self.logger.error(f"è®¡ç®—æ–‡ä»¶å“ˆå¸Œå¤±è´¥: {str(e)}")
            return ""
    
    def _is_allowed_file(self, filename: str) -> bool:
        """
        æ£€æŸ¥æ–‡ä»¶ç±»å‹æ˜¯å¦å…è®¸
        
        Args:
            filename: æ–‡ä»¶å
            
        Returns:
            bool: æ˜¯å¦å…è®¸ä¸Šä¼ 
        """
        if '.' not in filename:
            return False
        
        file_ext = filename.rsplit('.', 1)[1].lower()
        return file_ext in self.file_config['allowed_extensions']
    
    def upload_file(self, file: FileStorage, metadata: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """
        ä¸Šä¼ æ–‡ä»¶
        
        Args:
            file: ä¸Šä¼ çš„æ–‡ä»¶å¯¹è±¡
            metadata: æ–‡ä»¶å…ƒæ•°æ®
            
        Returns:
            Dict[str, Any]: ä¸Šä¼ ç»“æœ
        """
        try:
            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            if not file or file.filename == '':
                return {
                    'success': False,
                    'message': 'æœªé€‰æ‹©æ–‡ä»¶',
                    'file_id': None
                }
            
            # æ£€æŸ¥æ–‡ä»¶ç±»å‹
            if not self._is_allowed_file(file.filename):
                return {
                    'success': False,
                    'message': f'ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹',
                    'file_id': None
                }
            
            # æ£€æŸ¥æ–‡ä»¶å¤§å°
            file.seek(0, os.SEEK_END)
            file_size = file.tell()
            file.seek(0)
            
            if file_size > self.file_config['max_file_size']:
                return {
                    'success': False,
                    'message': f'æ–‡ä»¶å¤§å°è¶…è¿‡é™åˆ¶ ({self.file_config["max_file_size"]} bytes)',
                    'file_id': None
                }
            
            # ç”Ÿæˆå®‰å…¨çš„æ–‡ä»¶å
            filename = secure_filename(file.filename)
            file_ext = filename.rsplit('.', 1)[1].lower()
            
            # ç”Ÿæˆå”¯ä¸€æ–‡ä»¶å
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            unique_filename = f"{timestamp}_{filename}"
            
            # æ ¹æ®æ–‡ä»¶ç±»å‹é€‰æ‹©å­ç›®å½•
            if file_ext in ['jpg', 'jpeg', 'png', 'gif', 'bmp']:
                sub_dir = 'images'
            else:
                sub_dir = file_ext
            
            # ä¿å­˜æ–‡ä»¶åˆ°ç›¸åº”çš„å­ç›®å½•
            file_dir = os.path.join(self.file_config['upload_folder'], sub_dir)
            file_path = os.path.join(file_dir, unique_filename)
            file.save(file_path)
            
            # è®¡ç®—æ–‡ä»¶å“ˆå¸Œ
            content_hash = self._get_file_hash(file_path)
            
            # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨ç›¸åŒå†…å®¹çš„æ–‡ä»¶
            existing_file = self._check_duplicate_file(content_hash)
            if existing_file:
                # åˆ é™¤åˆšä¿å­˜çš„é‡å¤æ–‡ä»¶
                os.remove(file_path)
                return {
                    'success': True,
                    'message': 'æ–‡ä»¶å·²å­˜åœ¨',
                    'file_id': existing_file['id'],
                    'duplicate': True
                }
            
            # ä¿å­˜æ–‡ä»¶ä¿¡æ¯åˆ°æ•°æ®åº“
            file_data = {
                'filename': filename,
                'file_path': file_path,
                'file_type': file_ext,
                'file_size': file_size,
                'upload_time': datetime.now(),
                'process_status': 'pending',
                'content_hash': content_hash,
                'metadata': json.dumps(metadata or {}, ensure_ascii=False)
            }
            
            success = self.mysql_manager.insert_data('documents', file_data)
            
            if success:
                # è·å–æ’å…¥çš„æ–‡ä»¶ID
                file_info = self._get_file_by_hash(content_hash)
                file_id = file_info['id'] if file_info else None
                
                self.logger.info(f"æ–‡ä»¶ä¸Šä¼ æˆåŠŸ: {filename}, ID: {file_id}")
                
                # å¼‚æ­¥å¯åŠ¨å¤„ç†æµç¨‹
                if file_ext == 'pdf':  # åªå¯¹PDFæ–‡ä»¶è¿›è¡Œåç»­å¤„ç†
                    threading.Thread(target=self._async_process_file, args=(file_id, file_path)).start()
                
                return {
                    'success': True,
                    'message': 'æ–‡ä»¶ä¸Šä¼ æˆåŠŸ',
                    'file_id': file_id,
                    'duplicate': False
                }
            else:
                # åˆ é™¤å·²ä¿å­˜çš„æ–‡ä»¶
                os.remove(file_path)
                return {
                    'success': False,
                    'message': 'ä¿å­˜æ–‡ä»¶ä¿¡æ¯å¤±è´¥',
                    'file_id': None
                }
                
        except Exception as e:
            self.logger.error(f"æ–‡ä»¶ä¸Šä¼ å¤±è´¥: {str(e)}")
            return {
                'success': False,
                'message': f'æ–‡ä»¶ä¸Šä¼ å¤±è´¥: {str(e)}',
                'file_id': None
            }
    
    def _check_duplicate_file(self, content_hash: str) -> Optional[Dict[str, Any]]:
        """
        æ£€æŸ¥é‡å¤æ–‡ä»¶
        
        Args:
            content_hash: æ–‡ä»¶å†…å®¹å“ˆå¸Œ
            
        Returns:
            Optional[Dict[str, Any]]: é‡å¤æ–‡ä»¶ä¿¡æ¯ï¼Œä¸å­˜åœ¨è¿”å›None
        """
        try:
            query = "SELECT * FROM documents WHERE content_hash = :hash LIMIT 1"
            result = self.mysql_manager.execute_query(query, {'hash': content_hash})
            return result[0] if result else None
        except Exception as e:
            self.logger.error(f"æ£€æŸ¥é‡å¤æ–‡ä»¶å¤±è´¥: {str(e)}")
            return None
    
    def _get_file_by_hash(self, content_hash: str) -> Optional[Dict[str, Any]]:
        """
        æ ¹æ®å“ˆå¸Œè·å–æ–‡ä»¶ä¿¡æ¯
        
        Args:
            content_hash: æ–‡ä»¶å†…å®¹å“ˆå¸Œ
            
        Returns:
            Optional[Dict[str, Any]]: æ–‡ä»¶ä¿¡æ¯
        """
        try:
            query = "SELECT * FROM documents WHERE content_hash = :hash LIMIT 1"
            result = self.mysql_manager.execute_query(query, {'hash': content_hash})
            return result[0] if result else None
        except Exception as e:
            self.logger.error(f"è·å–æ–‡ä»¶ä¿¡æ¯å¤±è´¥: {str(e)}")
            return None
    
    def get_file_info(self, file_id: int) -> Optional[Dict[str, Any]]:
        """
        è·å–æ–‡ä»¶ä¿¡æ¯
        
        Args:
            file_id: æ–‡ä»¶ID
            
        Returns:
            Optional[Dict[str, Any]]: æ–‡ä»¶ä¿¡æ¯
        """
        try:
            query = "SELECT * FROM documents WHERE id = :file_id"
            result = self.mysql_manager.execute_query(query, {'file_id': file_id})
            
            if result:
                file_info = result[0]
                # è§£æå…ƒæ•°æ®
                if file_info.get('metadata'):
                    file_info['metadata'] = json.loads(file_info['metadata'])
                return file_info
            return None
            
        except Exception as e:
            self.logger.error(f"è·å–æ–‡ä»¶ä¿¡æ¯å¤±è´¥: {str(e)}")
            return None
    
    def get_file_list(self, page: int = 1, page_size: int = 20, 
                     file_type: Optional[str] = None, 
                     process_status: Optional[str] = None) -> Dict[str, Any]:
        """
        è·å–æ–‡ä»¶åˆ—è¡¨
        
        Args:
            page: é¡µç 
            page_size: æ¯é¡µæ•°é‡
            file_type: æ–‡ä»¶ç±»å‹è¿‡æ»¤
            process_status: å¤„ç†çŠ¶æ€è¿‡æ»¤
            
        Returns:
            Dict[str, Any]: æ–‡ä»¶åˆ—è¡¨å’Œåˆ†é¡µä¿¡æ¯
        """
        try:
            # æ„å»ºæŸ¥è¯¢æ¡ä»¶
            where_conditions = []
            params = {}
            
            if file_type:
                where_conditions.append("file_type = :file_type")
                params['file_type'] = file_type
            
            if process_status:
                where_conditions.append("process_status = :process_status")
                params['process_status'] = process_status
            
            where_clause = " AND ".join(where_conditions) if where_conditions else "1=1"
            
            # è®¡ç®—æ€»æ•°
            count_query = f"SELECT COUNT(*) as total FROM documents WHERE {where_clause}"
            count_result = self.mysql_manager.execute_query(count_query, params)
            total = count_result[0]['total'] if count_result else 0
            
            # è®¡ç®—åç§»é‡
            offset = (page - 1) * page_size
            params['limit'] = page_size
            params['offset'] = offset
            
            # æŸ¥è¯¢æ–‡ä»¶åˆ—è¡¨
            list_query = f"""
            SELECT id, filename, file_type, file_size, upload_time, process_status
            FROM documents 
            WHERE {where_clause}
            ORDER BY upload_time DESC
            LIMIT :limit OFFSET :offset
            """
            
            files = self.mysql_manager.execute_query(list_query, params)
            
            return {
                'files': files,
                'total': total,
                'page': page,
                'page_size': page_size,
                'total_pages': (total + page_size - 1) // page_size
            }
            
        except Exception as e:
            self.logger.error(f"è·å–æ–‡ä»¶åˆ—è¡¨å¤±è´¥: {str(e)}")
            return {
                'files': [],
                'total': 0,
                'page': page,
                'page_size': page_size,
                'total_pages': 0
            }
    
    def update_file_status(self, file_id: int, status: str, process_time: Optional[datetime] = None) -> bool:
        """
        æ›´æ–°æ–‡ä»¶å¤„ç†çŠ¶æ€
        
        Args:
            file_id: æ–‡ä»¶ID
            status: æ–°çŠ¶æ€
            process_time: å¤„ç†æ—¶é—´
            
        Returns:
            bool: æ›´æ–°æˆåŠŸè¿”å›True
        """
        try:
            update_data = {'process_status': status}
            if process_time:
                update_data['process_time'] = process_time
            else:
                update_data['process_time'] = datetime.now()
            
            success = self.mysql_manager.update_data(
                'documents',
                update_data,
                'id = :file_id',
                {'file_id': file_id}
            )
            
            if success:
                self.logger.info(f"æ–‡ä»¶çŠ¶æ€æ›´æ–°æˆåŠŸï¼ŒID: {file_id}, çŠ¶æ€: {status}")
            
            return success
            
        except Exception as e:
            self.logger.error(f"æ›´æ–°æ–‡ä»¶çŠ¶æ€å¤±è´¥: {str(e)}")
            return False
    
    def delete_file(self, file_id: int) -> bool:
        """
        åˆ é™¤æ–‡ä»¶
        
        Args:
            file_id: æ–‡ä»¶ID
            
        Returns:
            bool: åˆ é™¤æˆåŠŸè¿”å›True
        """
        try:
            # è·å–æ–‡ä»¶ä¿¡æ¯
            file_info = self.get_file_info(file_id)
            if not file_info:
                return False
            
            # åˆ é™¤ç‰©ç†æ–‡ä»¶
            if os.path.exists(file_info['file_path']):
                os.remove(file_info['file_path'])
                self.logger.info(f"åˆ é™¤ç‰©ç†æ–‡ä»¶: {file_info['file_path']}")
            
            # åˆ é™¤æ•°æ®åº“è®°å½•
            success = self.mysql_manager.delete_data(
                'documents',
                'id = :file_id',
                {'file_id': file_id}
            )
            
            if success:
                self.logger.info(f"æ–‡ä»¶åˆ é™¤æˆåŠŸï¼ŒID: {file_id}")
            
            return success
            
        except Exception as e:
            self.logger.error(f"åˆ é™¤æ–‡ä»¶å¤±è´¥: {str(e)}")
            return False
    
    def get_file_stats(self) -> Dict[str, Any]:
        """
        è·å–æ–‡ä»¶ç»Ÿè®¡ä¿¡æ¯
        
        Returns:
            Dict[str, Any]: ç»Ÿè®¡ä¿¡æ¯
        """
        try:
            queries = {
                'total_files': "SELECT COUNT(*) as count FROM documents",
                'total_size': "SELECT SUM(file_size) as size FROM documents",
                'by_type': """
                    SELECT file_type, COUNT(*) as count, SUM(file_size) as size
                    FROM documents 
                    GROUP BY file_type
                """,
                'by_status': """
                    SELECT process_status, COUNT(*) as count
                    FROM documents 
                    GROUP BY process_status
                """
            }
            
            stats = {}
            for key, query in queries.items():
                result = self.mysql_manager.execute_query(query)
                if key in ['total_files', 'total_size']:
                    stats[key] = result[0][list(result[0].keys())[0]] if result else 0
                else:
                    stats[key] = result
            
            return stats
            
        except Exception as e:
            self.logger.error(f"è·å–æ–‡ä»¶ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {str(e)}")
            return {}
    
    def cleanup_temp_files(self, max_age_hours: int = 24) -> int:
        """
        æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        
        Args:
            max_age_hours: æœ€å¤§ä¿ç•™æ—¶é—´ï¼ˆå°æ—¶ï¼‰
            
        Returns:
            int: æ¸…ç†çš„æ–‡ä»¶æ•°é‡
        """
        try:
            temp_folder = self.file_config['temp_folder']
            if not os.path.exists(temp_folder):
                return 0
            
            current_time = datetime.now()
            max_age_seconds = max_age_hours * 3600
            cleaned_count = 0
            
            for filename in os.listdir(temp_folder):
                file_path = os.path.join(temp_folder, filename)
                if os.path.isfile(file_path):
                    file_age = current_time.timestamp() - os.path.getmtime(file_path)
                    if file_age > max_age_seconds:
                        os.remove(file_path)
                        cleaned_count += 1
                        self.logger.info(f"æ¸…ç†ä¸´æ—¶æ–‡ä»¶: {filename}")
            
            self.logger.info(f"ä¸´æ—¶æ–‡ä»¶æ¸…ç†å®Œæˆï¼Œå…±æ¸…ç†{cleaned_count}ä¸ªæ–‡ä»¶")
            return cleaned_count
            
        except Exception as e:
            self.logger.error(f"æ¸…ç†ä¸´æ—¶æ–‡ä»¶å¤±è´¥: {str(e)}")
            return 0
    
    def _async_process_file(self, file_id: int, file_path: str) -> None:
        """
        å¼‚æ­¥å¤„ç†æ–‡ä»¶ï¼ˆæå–å†…å®¹ã€å‘é‡åŒ–ã€çŸ¥è¯†å›¾è°±ï¼‰
        
        Args:
            file_id: æ–‡ä»¶ID
            file_path: æ–‡ä»¶è·¯å¾„
        """
        try:
            # å¯¼å…¥å¤„ç†æœåŠ¡
            from app.service.pdf.PdfExtractService import PdfExtractService
            from app.service.pdf.PdfVectorService import PdfVectorService
            from app.service.pdf.PdfGraphService import PdfGraphService
            
            pdf_extract_service = PdfExtractService()
            pdf_vector_service = PdfVectorService()
            pdf_graph_service = PdfGraphService()
            
            self.logger.info(f"å¼€å§‹å¼‚æ­¥å¤„ç†æ–‡ä»¶ï¼ŒID: {file_id}")
            
            # æ­¥éª¤1ï¼šå†…å®¹æå– (10% -> 40%)
            self.update_file_status(file_id, 'extracting')
            extract_result = pdf_extract_service.extract_pdf_content(file_path, file_id)
            
            if not extract_result['success']:
                self.update_file_status(file_id, 'extract_failed')
                self.logger.error(f"æ–‡ä»¶å†…å®¹æå–å¤±è´¥ï¼ŒID: {file_id}, é”™è¯¯: {extract_result['message']}")
                return
            
            # è·å–ç”Ÿæˆçš„JSONæ–‡ä»¶è·¯å¾„
            json_file_path = self._get_json_file_path(file_path, file_id)
            if not json_file_path or not os.path.exists(json_file_path):
                self.update_file_status(file_id, 'extract_failed')
                self.logger.error(f"æœªæ‰¾åˆ°æå–çš„JSONæ–‡ä»¶ï¼ŒID: {file_id}")
                return
                
            self.update_file_status(file_id, 'extracted')
            self.logger.info(f"æ–‡ä»¶å†…å®¹æå–å®Œæˆï¼ŒID: {file_id}")
            
            # æ­¥éª¤2ï¼šå‘é‡åŒ– (40% -> 70%)
            self.update_file_status(file_id, 'vectorizing')
            vector_result = pdf_vector_service.process_pdf_json_to_vectors(json_file_path, file_id)
            
            if not vector_result['success']:
                self.update_file_status(file_id, 'vectorize_failed')
                self.logger.error(f"æ–‡ä»¶å‘é‡åŒ–å¤±è´¥ï¼ŒID: {file_id}, é”™è¯¯: {vector_result['message']}")
                return
                
            self.update_file_status(file_id, 'vectorized')
            self.logger.info(f"æ–‡ä»¶å‘é‡åŒ–å®Œæˆï¼ŒID: {file_id}")
            
            # ğŸ”§ ä¿®å¤ï¼šè·å–å‘é‡åŒ–è¿‡ç¨‹ä¸­ç”Ÿæˆçš„content_units.jsonæ–‡ä»¶
            content_units_file_path = self._get_content_units_file_path(file_path, file_id)
            if not content_units_file_path or not os.path.exists(content_units_file_path):
                self.logger.warning(f"æœªæ‰¾åˆ°content_unitsæ–‡ä»¶ï¼Œä½¿ç”¨åŸå§‹JSONæ–‡ä»¶è¿›è¡Œå›¾è°±æ„å»ºï¼ŒID: {file_id}")
                content_units_file_path = json_file_path  # å›é€€åˆ°åŸå§‹æ–‡ä»¶
            else:
                self.logger.info(f"æ‰¾åˆ°content_unitsæ–‡ä»¶: {content_units_file_path}")
            
            # æ­¥éª¤3ï¼šçŸ¥è¯†å›¾è°±æ„å»º (70% -> 100%)
            self.update_file_status(file_id, 'graph_processing')
            graph_result = pdf_graph_service.process_pdf_json_to_graph(content_units_file_path, file_id)
            
            if not graph_result['success']:
                self.update_file_status(file_id, 'graph_failed')
                self.logger.error(f"çŸ¥è¯†å›¾è°±æ„å»ºå¤±è´¥ï¼ŒID: {file_id}, é”™è¯¯: {graph_result['message']}")
                return
                
            # æ‰€æœ‰æ­¥éª¤å®Œæˆ
            self.update_file_status(file_id, 'completed')
            self.logger.info(f"æ–‡ä»¶å¤„ç†å…¨éƒ¨å®Œæˆï¼ŒID: {file_id}")
            
        except Exception as e:
            self.logger.error(f"å¼‚æ­¥æ–‡ä»¶å¤„ç†å¤±è´¥ï¼ŒID: {file_id}, é”™è¯¯: {str(e)}")
            self.update_file_status(file_id, 'process_failed')
    
    def _get_json_file_path(self, pdf_file_path: str, file_id: int) -> Optional[str]:
        """
        è·å–æå–åçš„JSONæ–‡ä»¶è·¯å¾„
        
        Args:
            pdf_file_path: PDFæ–‡ä»¶è·¯å¾„
            file_id: æ–‡ä»¶ID
            
        Returns:
            Optional[str]: JSONæ–‡ä»¶è·¯å¾„
        """
        try:
            # æ ¹æ®PDFæ–‡ä»¶è·¯å¾„æ¨æµ‹JSONæ–‡ä»¶è·¯å¾„
            # é€šå¸¸ä¿å­˜åœ¨upload/jsonç›®å½•ä¸‹
            filename = os.path.basename(pdf_file_path)
            name_without_ext = os.path.splitext(filename)[0]
            
            # ğŸ”§ ä¿®å¤ï¼šä½¿ç”¨æ­£ç¡®çš„æ–‡ä»¶IDç”Ÿæˆæ–‡ä»¶å
            possible_names = [
                f"{name_without_ext}_doc_{file_id}.json",  # ä½¿ç”¨å®é™…çš„file_id
                f"{name_without_ext}_content_units.json",
                f"{name_without_ext}_doc_1.json"  # ä¿ç•™å…¼å®¹æ€§
            ]
            
            json_dir = os.path.join(self.file_config['upload_folder'], 'json')
            
            for json_name in possible_names:
                json_path = os.path.join(json_dir, json_name)
                if os.path.exists(json_path):
                    self.logger.info(f"æ‰¾åˆ°JSONæ–‡ä»¶: {json_path}")
                    return json_path
            
            # ğŸ”§ å¢å¼ºï¼šå¦‚æœæ‰¾ä¸åˆ°ï¼Œè®°å½•è¯¦ç»†ä¿¡æ¯
            self.logger.warning(f"åœ¨ç›®å½• {json_dir} ä¸­æ‰¾ä¸åˆ°ä»¥ä¸‹ä»»ä½•æ–‡ä»¶: {possible_names}")
            return None
            
        except Exception as e:
            self.logger.error(f"è·å–JSONæ–‡ä»¶è·¯å¾„å¤±è´¥: {str(e)}")
            return None
    
    def _get_content_units_file_path(self, pdf_file_path: str, file_id: int) -> Optional[str]:
        """
        è·å–å‘é‡åŒ–è¿‡ç¨‹ä¸­ç”Ÿæˆçš„content_units.jsonæ–‡ä»¶è·¯å¾„
        
        Args:
            pdf_file_path: PDFæ–‡ä»¶è·¯å¾„
            file_id: æ–‡ä»¶ID
            
        Returns:
            Optional[str]: content_units.jsonæ–‡ä»¶è·¯å¾„
        """
        try:
            # æ ¹æ®PDFæ–‡ä»¶è·¯å¾„æ¨æµ‹content_units.jsonæ–‡ä»¶è·¯å¾„
            filename = os.path.basename(pdf_file_path)
            name_without_ext = os.path.splitext(filename)[0]
            
            # content_units.jsonæ–‡ä»¶å‘½åæ ¼å¼
            possible_names = [
                f"{name_without_ext}_content_units.json",  # ä¸»è¦æ ¼å¼
                f"{name_without_ext.split('_', 2)[-1]}_content_units.json" if '_' in name_without_ext else None  # å»é™¤æ—¶é—´æˆ³å‰ç¼€
            ]
            
            # è¿‡æ»¤æ‰Noneå€¼
            possible_names = [name for name in possible_names if name]
            
            json_dir = os.path.join(self.file_config['upload_folder'], 'json')
            
            for json_name in possible_names:
                json_path = os.path.join(json_dir, json_name)
                if os.path.exists(json_path):
                    self.logger.info(f"æ‰¾åˆ°content_unitsæ–‡ä»¶: {json_path}")
                    return json_path
            
            self.logger.debug(f"åœ¨ç›®å½• {json_dir} ä¸­æ‰¾ä¸åˆ°content_unitsæ–‡ä»¶: {possible_names}")
            return None
            
        except Exception as e:
            self.logger.error(f"è·å–content_unitsæ–‡ä»¶è·¯å¾„å¤±è´¥: {str(e)}")
            return None