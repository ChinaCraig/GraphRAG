"""
æ™ºèƒ½æ£€ç´¢æœåŠ¡
ä¸¥æ ¼æŒ‰ç…§é‡æ„è¦æ±‚å®ç°å®Œæ•´çš„æ£€ç´¢æµç¨‹ï¼š
â‘  è§„èŒƒåŒ– â†’ â‘¡ æ„å›¾åˆ¤åˆ« â†’ â‘¢ å€™é€‰å¬å› â†’ â‘£ èšåˆèåˆ â†’ â‘¤ é‡æ’ â†’ â‘¦ æ‰©å±• â†’ â‘§ è¡¥å›¾è¡¨ â†’ â‘¨ æµå¼æ¸²æŸ“
"""

import json
import logging
import re
import yaml
import numpy as np
import os
from typing import Dict, List, Optional, Generator, Any
from datetime import datetime
from collections import defaultdict
from sqlalchemy import text
import requests
from time import sleep

# é…ç½®æ—¥å¿—
logger = logging.getLogger(__name__)


class SearchService:
    """æ™ºèƒ½æ£€ç´¢æœåŠ¡ç±» - å®Œæ•´å®ç°"""
    
    def __init__(self):
        """åˆå§‹åŒ–æœç´¢æœåŠ¡"""
        self._load_config()
        self._init_clients()
        self._init_models()
        self._init_patterns()
        
    def _load_config(self):
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        try:
            with open('config/model.yaml', 'r', encoding='utf-8') as f:
                self.model_config = yaml.safe_load(f)
            with open('config/db.yaml', 'r', encoding='utf-8') as f:
                self.db_config = yaml.safe_load(f)
            with open('config/prompt.yaml', 'r', encoding='utf-8') as f:
                self.prompt_config = yaml.safe_load(f)
            logger.info("é…ç½®æ–‡ä»¶åŠ è½½æˆåŠŸ")
        except Exception as e:
            logger.error(f"åŠ è½½é…ç½®æ–‡ä»¶å¤±è´¥: {str(e)}")
            self.model_config = {}
            self.db_config = {}
            self.prompt_config = {}
    
    def _init_clients(self):
        """åˆå§‹åŒ–å®¢æˆ·ç«¯"""
        try:
            # OpenSearchå®¢æˆ·ç«¯
            self._init_opensearch_client()
            
            # Milvuså‘é‡å®¢æˆ·ç«¯
            self._init_milvus_client()
            
            # Neo4jå›¾æ•°æ®åº“å®¢æˆ·ç«¯
            self._init_neo4j_client()
            
            # MySQLæ•°æ®åº“å®¢æˆ·ç«¯
            self._init_mysql_client()
            
            # LLMå®¢æˆ·ç«¯
            self._init_llm_client()
            
            logger.info("å®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            logger.error(f"å®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {str(e)}")
    
    def _init_opensearch_client(self):
        """åˆå§‹åŒ–OpenSearchå®¢æˆ·ç«¯"""
        try:
            from utils.OpenSearchManager import OpenSearchManager
            opensearch_config = self.db_config.get('opensearch', {})
            if opensearch_config:
                self.opensearch_client = OpenSearchManager('config/db.yaml')
                self.index_name = opensearch_config.get('index_name', 'graphrag_documents')
                logger.info("OpenSearchå®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ")
            else:
                self.opensearch_client = None
                logger.warning("OpenSearché…ç½®æœªæ‰¾åˆ°")
        except Exception as e:
            logger.error(f"OpenSearchå®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {str(e)}")
            self.opensearch_client = None
    
    def _init_milvus_client(self):
        """åˆå§‹åŒ–Milvuså®¢æˆ·ç«¯"""
        try:
            from utils.MilvusManager import MilvusManager
            milvus_config = self.db_config.get('milvus', {})
            if milvus_config:
                self.milvus_client = MilvusManager()
                logger.info("Milvuså®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ")
            else:
                self.milvus_client = None
                logger.warning("Milvusé…ç½®æœªæ‰¾åˆ°")
        except Exception as e:
            logger.error(f"Milvuså®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {str(e)}")
            self.milvus_client = None
    
    def _init_neo4j_client(self):
        """åˆå§‹åŒ–Neo4jå®¢æˆ·ç«¯"""
        try:
            from neo4j import GraphDatabase
            neo4j_config = self.db_config.get('neo4j', {})
            if neo4j_config:
                self.neo4j_client = GraphDatabase.driver(
                    neo4j_config.get('uri', 'bolt://localhost:7687'),
                    auth=(
                        neo4j_config.get('username', 'neo4j'),
                        neo4j_config.get('password', 'password')
                    )
                )
                logger.info("Neo4jå®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ")
            else:
                self.neo4j_client = None
                logger.warning("Neo4jé…ç½®æœªæ‰¾åˆ°")
        except Exception as e:
            logger.error(f"Neo4jå®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {str(e)}")
            self.neo4j_client = None
    
    def _init_mysql_client(self):
        """åˆå§‹åŒ–MySQLå®¢æˆ·ç«¯"""
        try:
            from utils.MySQLManager import MySQLManager
            mysql_config = self.db_config.get('mysql', {})
            if mysql_config:
                self.mysql_client = MySQLManager('config/db.yaml')
                logger.info("MySQLå®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ")
            else:
                self.mysql_client = None
                logger.warning("MySQLé…ç½®æœªæ‰¾åˆ°")
        except Exception as e:
            logger.error(f"MySQLå®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {str(e)}")
            self.mysql_client = None
    
    def _init_llm_client(self):
        """åˆå§‹åŒ–LLMå®¢æˆ·ç«¯"""
        try:
            deepseek_config = self.model_config.get('deepseek', {})
            self.llm_config = {
                'api_url': deepseek_config.get('api_url', 'https://api.deepseek.com'),
                'api_key': deepseek_config.get('api_key', ''),
                'model_name': deepseek_config.get('model_name', 'deepseek-chat'),
                'max_tokens': deepseek_config.get('max_tokens', 4096),
                'temperature': deepseek_config.get('temperature', 0.7)
            }
            if self.llm_config['api_key']:
                logger.info("LLMå®¢æˆ·ç«¯é…ç½®æˆåŠŸ")
            else:
                logger.warning("LLM APIå¯†é’¥æœªé…ç½®")
        except Exception as e:
            logger.error(f"LLMå®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {str(e)}")
            self.llm_config = {}
    
    def _init_models(self):
        """åˆå§‹åŒ–æ¨¡å‹"""
        try:
            # åµŒå…¥æ¨¡å‹
            from sentence_transformers import SentenceTransformer
            embedding_config = self.model_config.get('embedding', {})
            if embedding_config:
                model_name = embedding_config.get('model_name')
                cache_dir = embedding_config.get('cache_dir')
                
                os.environ['HF_HOME'] = os.path.abspath(cache_dir)
                os.environ['TRANSFORMERS_CACHE'] = os.path.abspath(cache_dir)
                os.environ['SENTENCE_TRANSFORMERS_HOME'] = os.path.abspath(cache_dir)
                
                self.embedding_model = SentenceTransformer(model_name, cache_folder=cache_dir)
                self.normalize = embedding_config.get('normalize', True)
                logger.info(f"åµŒå…¥æ¨¡å‹åˆå§‹åŒ–æˆåŠŸ: {model_name}")
            else:
                self.embedding_model = None
            
            # é‡æ’æ¨¡å‹
            reranker_config = self.model_config.get('reranker', {})
            if reranker_config.get('enabled', False):
                from sentence_transformers import CrossEncoder
                model_name = reranker_config.get('model_name', 'BAAI/bge-reranker-large')
                device = reranker_config.get('device', 'cpu')
                
                self.reranker = CrossEncoder(model_name, device=device)
                self.reranker_config = reranker_config
                logger.info(f"é‡æ’æ¨¡å‹åˆå§‹åŒ–æˆåŠŸ: {model_name}")
            else:
                self.reranker = None
                self.reranker_config = {}
                
        except Exception as e:
            logger.error(f"æ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {str(e)}")
            self.embedding_model = None
            self.reranker = None
    
    def _init_patterns(self):
        """åˆå§‹åŒ–æ¨¡å¼å’Œè¯å…¸"""
        # å®ä½“è¯†åˆ«æ¨¡å¼
        self.entity_patterns = {
            "bio_entity": [
                r"HCP|å®¿ä¸»ç»†èƒè›‹ç™½",
                r"CHO|ä¸­å›½ä»“é¼ åµå·¢", 
                r"ç»†èƒæ ª|cell\s*line",
                r"åŸ¹å…»åŸº|medium",
                r"æŠ—ä½“|antibody",
                r"è›‹ç™½è´¨|protein"
            ],
            "product_model": [
                r"[A-Z]{2,5}\d{2,6}",
                r"[A-Z]+\-\d+",
                r"v\d+\.\d+"
            ]
        }
        
        # åŒä¹‰è¯è¯å…¸
        self.synonym_dict = {
            "HCP": ["å®¿ä¸»ç»†èƒè›‹ç™½", "Host Cell Protein", "host cell protein"],
            "CHO": ["ä¸­å›½ä»“é¼ åµå·¢", "Chinese Hamster Ovary", "ä¸­å›½ä»“é¼ åµå·¢ç»†èƒ"],
            "CHO-K1": "CHOK1",
            "CHO K1": "CHOK1"
        }
    
    def intelligent_search(self, query: str, filters: Dict = None) -> Generator[Dict, None, None]:
        """
        æ™ºèƒ½æ£€ç´¢ä¸»æµç¨‹
        ä¸¥æ ¼æŒ‰ç…§æ–‡æ¡£è¦æ±‚ï¼šâ‘  â†’ â‘¡ â†’ â‘¢ â†’ â‘£ â†’ â‘¤ â†’ â‘¦ â†’ â‘§ â†’ â‘¨
        """
        try:
            logger.info(f"å¼€å§‹æ™ºèƒ½æ£€ç´¢: {query}")
            
            # â‘  è§„èŒƒåŒ–ï¼ˆQuery Normalizationï¼‰
            yield {"type": "stage_update", "stage": "normalization", "message": "ğŸ”§ æ­£åœ¨è§„èŒƒåŒ–æŸ¥è¯¢...", "progress": 10}
            normalized_query = self._normalize_query(query)
            
            # â‘¡ æ„å›¾åˆ¤åˆ«ï¼ˆæ ‡é¢˜é—®æ³• or ç¢å¥é—®æ³•ï¼‰
            yield {"type": "stage_update", "stage": "intent", "message": "ğŸ¯ æ­£åœ¨åˆ¤åˆ«æŸ¥è¯¢æ„å›¾...", "progress": 20}
            intent_type = self._classify_intent(normalized_query)
            
            # ç”Ÿæˆæ£€ç´¢é…ç½®
            retrieval_config = self._configure_retrieval(normalized_query, intent_type)
            understanding_result = {
                "original_query": query,
                "normalized_query": normalized_query,
                "intent_type": intent_type,
                "retrieval_config": retrieval_config,
                "entities": self._extract_entities(normalized_query),
                "rewrite_result": self._rewrite_and_expand(normalized_query, intent_type)
            }
            
            # â‘¢ å€™é€‰å¬å›ï¼ˆå¿«è€Œå¹¿ï¼‰
            yield {"type": "stage_update", "stage": "retrieval", "message": "ğŸ“š æ­£åœ¨å¬å›å€™é€‰å†…å®¹...", "progress": 40}
            bm25_results = self._bm25_retrieval(understanding_result, filters)
            vector_results = self._vector_retrieval(understanding_result, filters)
            graph_results = self._graph_retrieval(understanding_result, filters)
            
            # â‘£ æ„å›¾æ„ŸçŸ¥çš„èšåˆä¸åˆ†æ•°èåˆ
            yield {"type": "stage_update", "stage": "aggregation", "message": "ğŸ”— æ­£åœ¨èšåˆå’Œèåˆç»“æœ...", "progress": 55}
            candidates = self._aggregate_by_section(bm25_results, vector_results, graph_results, understanding_result)
            
            # â‘¤ é‡æ’ï¼ˆæŠŠ"æœ€ç›¸å…³çš„ä¸€èŠ‚"æ”¾åˆ°ç¬¬ä¸€ï¼‰
            yield {"type": "stage_update", "stage": "reranking", "message": "ğŸ¯ æ­£åœ¨é‡æ’é€‰æ‹©æœ€ä½³ç« èŠ‚...", "progress": 70}
            top_section = self._rerank_sections(candidates, understanding_result)
            
            if not top_section:
                yield {"type": "error", "message": "æœªæ‰¾åˆ°ç›¸å…³å†…å®¹"}
                return
            
            # â‘¦ æ‰©å±•ï¼ˆæŠŠ"ä¸€å®¶å­"æ‹‰é½ï¼‰
            yield {"type": "stage_update", "stage": "expansion", "message": "ğŸ” æ­£åœ¨æ‰©å±•ç« èŠ‚å†…å®¹...", "progress": 80}
            expanded_content = self._expand_section_content(top_section)
            
            # â‘§ å›¾è¡¨ç»†èŠ‚ï¼ˆMySQLï¼‰
            yield {"type": "stage_update", "stage": "enrichment", "message": "ğŸ–¼ï¸ æ­£åœ¨è¡¥å……å›¾è¡¨ç»†èŠ‚...", "progress": 85}
            multimodal_content = self._enrich_multimodal_details(top_section)
            
            # â‘¨ ç»„è£…/æ¸²æŸ“ï¼ˆå¯æµå¼ï¼‰
            yield {"type": "stage_update", "stage": "rendering", "message": "âœï¸ æ­£åœ¨ç”Ÿæˆç­”æ¡ˆ...", "progress": 90}
            
            # æµå¼è¾“å‡ºç»“æœ
            yield from self._stream_render_answer(query, top_section, multimodal_content, understanding_result)
            
        except Exception as e:
            logger.error(f"æ™ºèƒ½æ£€ç´¢å¤±è´¥: {str(e)}")
            yield {"type": "error", "message": f"æ£€ç´¢å¤±è´¥: {str(e)}"}
    
    def _normalize_query(self, query: str) -> str:
        """â‘  è§„èŒƒåŒ–ï¼ˆQuery Normalizationï¼‰"""
        try:
            normalized = query.strip()
            
            # å…¨è§’/åŠè§’æ ‡å‡†åŒ–
            import unicodedata
            normalized = unicodedata.normalize('NFKC', normalized)
            
            # ç©ºç™½ä¸æ ‡ç‚¹æ ‡å‡†åŒ–
            normalized = re.sub(r'\s+', ' ', normalized)
            normalized = normalized.replace('ï¼Œ', ',').replace('ã€‚', '.').replace('ï¼›', ';')
            
            # ä¸­è‹±æ–‡ä¹‹é—´åŠ ç©ºæ ¼
            normalized = re.sub(r'([\u4e00-\u9fff])([a-zA-Z])', r'\1 \2', normalized)
            normalized = re.sub(r'([a-zA-Z])([\u4e00-\u9fff])', r'\1 \2', normalized)
            
            # åŒä¹‰è¯æ ‡å‡†åŒ–
            for synonym, standard in self.synonym_dict.items():
                if isinstance(standard, str):
                    normalized = normalized.replace(synonym, standard)
                else:
                    for syn in standard:
                        normalized = normalized.replace(syn, synonym)
            
            # ç§»é™¤ä½ä¿¡æ¯è¯
            low_info_words = ["å¸®æˆ‘", "è¯·", "æŸ¥è¯¢", "æŸ¥æ‰¾", "æœç´¢", "ä¸€ä¸‹", "ç›¸å…³", "å†…å®¹"]
            words = normalized.split()
            filtered_words = [word for word in words if word not in low_info_words and len(word.strip()) > 0]
            
            if filtered_words:
                normalized = " ".join(filtered_words)
            
            logger.debug(f"æŸ¥è¯¢è§„èŒƒåŒ–: '{query}' -> '{normalized}'")
            return normalized
            
        except Exception as e:
            logger.error(f"æŸ¥è¯¢è§„èŒƒåŒ–å¤±è´¥: {str(e)}")
            return query
    
    def _classify_intent(self, query: str) -> str:
        """â‘¡ æ„å›¾åˆ¤åˆ«ï¼ˆæ ‡é¢˜é—®æ³• or ç¢å¥é—®æ³•ï¼‰"""
        try:
            # # è§„åˆ™1ï¼šé•¿åº¦â‰¤8å­—ä¸”åŒ…å«ç‰¹å®šå…³é”®è¯ â†’ æ ‡é¢˜é—®æ³•
            # if len(query) <= 8 and any(keyword in query for keyword in
            #     ["ç®€ä»‹", "è¯´æ˜", "æ˜¯ä»€ä¹ˆ", "å®šä¹‰", "äº§å“è¯´æ˜", "æ¦‚è¿°", "ä»‹ç»"]):
            #     return "title"
            #
            # # ğŸ”§ è§„åˆ™2ï¼šåŒ…å«æ˜ç¡®çš„æ ‡é¢˜æ€§æŸ¥è¯¢è¯ â†’ æ ‡é¢˜é—®æ³• (æ‰©å±•ç‰ˆ)
            # title_indicators = ["ä»€ä¹ˆæ˜¯", "å®šä¹‰", "æ¦‚å¿µ", "ç®€ä»‹", "æ¦‚è¿°", "ä»‹ç»", "æ¡ˆä¾‹", "åˆ†äº«", "ç‰¹ç‚¹", "ä¼˜åŠ¿", "åº”ç”¨"]
            # if any(indicator in query for indicator in title_indicators):
            #     logger.info(f"æ„å›¾åˆ¤åˆ«ï¼šæ£€æµ‹åˆ°æ ‡é¢˜æ€§å…³é”®è¯ '{[ind for ind in title_indicators if ind in query]}' â†’ title")
            #     return "title"
            #
            # # è§„åˆ™3ï¼šåŒ…å«æ˜ç¡®çš„å†…å®¹æ€§æŸ¥è¯¢è¯ â†’ ç¢å¥é—®æ³•
            # content_indicators = ["å¦‚ä½•", "æ€ä¹ˆ", "æ­¥éª¤", "æµç¨‹", "æ–¹æ³•", "è¿‡ç¨‹", "å…·ä½“", "è¯¦ç»†"]
            # if any(indicator in query for indicator in content_indicators):
            #     return "fragment"
            
            # ğŸ”§ è§„åˆ™4ï¼šåŸºäºå‘é‡æ•°æ®åº“çš„æ„å›¾åˆ¤åˆ«ï¼ˆä¸»è¦æ–¹æ³•ï¼‰
            vector_intent = self._vector_based_intent_classification(query)
            if vector_intent:
                logger.info(f"æ„å›¾åˆ¤åˆ«ï¼šå‘é‡ç›¸ä¼¼åº¦åˆ†æ â†’ {vector_intent}")
                return vector_intent
            
            # è§„åˆ™5ï¼šå‘é‡ç›¸ä¼¼åº¦åˆ¤æ–­ï¼ˆç®€åŒ–å®ç°ï¼Œå…œåº•ï¼‰
            similarity_score = self._calculate_title_similarity(query)
            
            if similarity_score >= 0.45:
                return "title"
            elif similarity_score >= 0.40:
                return "hybrid"  # ä¸¤è·¯å¹¶è·‘
            else:
                return "fragment"
                
        except Exception as e:
            logger.error(f"æ„å›¾åˆ¤åˆ«å¤±è´¥: {str(e)}")
            return "fragment"
    
    def _calculate_title_similarity(self, query: str) -> float:
        """è®¡ç®—æŸ¥è¯¢ä¸æ ‡é¢˜æ€§å†…å®¹çš„ç›¸ä¼¼åº¦"""
        title_keywords = [
            "HCP", "CHO", "è›‹ç™½", "ç»†èƒ", "åŸ¹å…»", "æ£€æµ‹", "åˆ†æ", "è´¨é‡", "æ ‡å‡†",
            "è¯•å‰‚", "äº§å“", "è®¾å¤‡", "æ–¹æ³•", "æŠ€æœ¯", "ç³»ç»Ÿ", "å¹³å°", "æœåŠ¡"
        ]
        
        query_words = set(query.split())
        title_word_set = set(title_keywords)
        
        intersection = query_words.intersection(title_word_set)
        union = query_words.union(title_word_set)
        
        if not union:
            return 0.0
            
        similarity = len(intersection) / len(union)
        
        # æ ¹æ®æŸ¥è¯¢é•¿åº¦è°ƒæ•´ç›¸ä¼¼åº¦
        if len(query) <= 5:
            similarity += 0.1
        elif len(query) > 20:
            similarity -= 0.1
            
        return min(similarity, 1.0)
    
    def _vector_based_intent_classification(self, query: str) -> Optional[str]:
        """ğŸ”§ åŸºäºå‘é‡æ•°æ®åº“çš„æ„å›¾åˆ¤åˆ«"""
        try:
            if not self.milvus_client or not self.embedding_model:
                logger.debug("å‘é‡æ„å›¾åˆ¤åˆ«ï¼šMilvuså®¢æˆ·ç«¯æˆ–åµŒå…¥æ¨¡å‹æœªåˆå§‹åŒ–")
                return None
            
            # ç¼–ç æŸ¥è¯¢å‘é‡
            query_vector = self.embedding_model.encode(
                query, 
                normalize_embeddings=self.normalize
            ).tolist()
            
            # åˆ†åˆ«æœç´¢æ ‡é¢˜å’Œç‰‡æ®µå‘é‡
            try:
                # ğŸ”§ æœç´¢æ ‡é¢˜å’Œå®Œæ•´sectionå‘é‡ï¼ˆä½¿ç”¨æ–°çš„content_typeå­—æ®µï¼‰
                title_results = self.milvus_client.search_vectors(
                    query_vectors=[query_vector],
                    top_k=5,
                    expr="content_type in ['title', 'section']"
                )
                
                # æœç´¢ç‰‡æ®µå‘é‡
                fragment_results = self.milvus_client.search_vectors(
                    query_vectors=[query_vector],
                    top_k=5, 
                    expr="content_type == 'fragment'"
                )
                
                # æå–åˆ†æ•°
                title_scores = []
                fragment_scores = []
                
                # ğŸ”§ ä¿®å¤ï¼šMilvusManager.search_vectorsè¿”å›çš„æ˜¯å­—å…¸åˆ—è¡¨ï¼Œä¸æ˜¯åµŒå¥—åˆ—è¡¨
                if title_results and len(title_results) > 0:
                    title_scores = [hit.get('score', 0) for hit in title_results]
                    
                if fragment_results and len(fragment_results) > 0:
                    fragment_scores = [hit.get('score', 0) for hit in fragment_results]
                
                # è®¡ç®—ç»Ÿè®¡æŒ‡æ ‡
                title_max = max(title_scores) if title_scores else 0
                title_avg = sum(title_scores) / len(title_scores) if title_scores else 0
                
                fragment_max = max(fragment_scores) if fragment_scores else 0
                fragment_avg = sum(fragment_scores) / len(fragment_scores) if fragment_scores else 0
                
                # åˆ¤åˆ«é€»è¾‘
                score_diff = title_max - fragment_max
                avg_diff = title_avg - fragment_avg
                
                logger.debug(f"å‘é‡æ„å›¾åˆ†æ: title_max={title_max:.3f}, fragment_max={fragment_max:.3f}, "
                           f"score_diff={score_diff:.3f}, avg_diff={avg_diff:.3f}")
                
                # é˜ˆå€¼åˆ¤åˆ«
                if score_diff > 0.1 and avg_diff > 0.05:
                    return "title"
                elif score_diff < -0.1 and avg_diff < -0.05:
                    return "fragment"
                elif abs(score_diff) <= 0.05:
                    return "hybrid"
                else:
                    return "title" if title_max > fragment_max else "fragment"
                    
            except Exception as e:
                logger.warning(f"å‘é‡æœç´¢å¤±è´¥ï¼Œå¯èƒ½æ˜¯filter_exprè¯­æ³•é—®é¢˜: {str(e)}")
                # é™çº§åˆ°metadataè¿‡æ»¤ï¼ˆå‘åå…¼å®¹ï¼‰
                return self._fallback_metadata_intent_classification(query_vector)
                
        except Exception as e:
            logger.warning(f"å‘é‡æ„å›¾åˆ¤åˆ«å¤±è´¥: {str(e)}")
            return None
    
    def _fallback_metadata_intent_classification(self, query_vector: List[float]) -> Optional[str]:
        """é™çº§åˆ°metadataè¿‡æ»¤çš„æ„å›¾åˆ¤åˆ«"""
        try:
            # æœç´¢æ‰€æœ‰å‘é‡ï¼Œç„¶ååœ¨ç»“æœä¸­è¿‡æ»¤
            all_results = self.milvus_client.search_vectors(
                query_vectors=[query_vector],
                top_k=20
            )
            
            if not all_results or len(all_results) == 0:
                return None
                
            title_scores = []
            fragment_scores = []
            
            # ğŸ”§ ä¿®å¤ï¼šall_resultsæ˜¯å­—å…¸åˆ—è¡¨ï¼Œä¸æ˜¯åµŒå¥—åˆ—è¡¨
            for hit in all_results:
                metadata_str = hit.get('metadata', '{}')
                try:
                    import json
                    metadata = json.loads(metadata_str) if isinstance(metadata_str, str) else metadata_str
                    content_type = metadata.get('content_type', 'fragment')
                    score = hit.get('score', 0)
                    
                    if content_type == 'title':
                        title_scores.append(score)
                    else:
                        fragment_scores.append(score)
                except:
                    continue
            
            # ç®€åŒ–åˆ¤åˆ«é€»è¾‘
            title_max = max(title_scores) if title_scores else 0
            fragment_max = max(fragment_scores) if fragment_scores else 0
            
            if title_max > fragment_max + 0.1:
                return "title"
            elif fragment_max > title_max + 0.1:
                return "fragment"
            else:
                return None
                
        except Exception as e:
            logger.warning(f"é™çº§æ„å›¾åˆ¤åˆ«ä¹Ÿå¤±è´¥: {str(e)}")
            return None
    
    def _configure_retrieval(self, query: str, intent_type: str) -> Dict:
        """â‘¢ å€™é€‰å¬å›é…ç½®"""
        if intent_type == "title":
            return {
                "vector_top_k": 20,
                "vector_target": "titles",
                "bm25_top_k": 20,
                "bm25_target": "sections",
                "strategy": "title_oriented"
            }
        elif intent_type == "fragment":
            return {
                "vector_top_k": 50,
                "vector_target": "fragments", 
                "bm25_top_k": 50,
                "bm25_target": "fragments",
                "strategy": "content_oriented"
            }
        elif intent_type == "hybrid":
            return {
                "vector_top_k": 35,
                "vector_target": "mixed",
                "bm25_top_k": 35,
                "bm25_target": "mixed",
                "strategy": "hybrid_dual_path"
            }
        else:
            return {
                "vector_top_k": 50,
                "vector_target": "fragments",
                "bm25_top_k": 50,
                "bm25_target": "fragments",
                "strategy": "default"
            }
    
    def _extract_entities(self, query: str) -> Dict[str, List[str]]:
        """å®ä½“è¯†åˆ«"""
        entities = {}
        
        for entity_type, patterns in self.entity_patterns.items():
            entities[entity_type] = []
            
            for pattern in patterns:
                matches = re.findall(pattern, query, re.IGNORECASE)
                if matches:
                    for match in matches:
                        if isinstance(match, tuple):
                            entities[entity_type].extend([m for m in match if m])
                        else:
                            entities[entity_type].append(match)
        
        # å»é‡å¹¶è¿‡æ»¤ç©ºå€¼
        for entity_type in entities:
            entities[entity_type] = list(set([e for e in entities[entity_type] if e]))
        
        if not any(entities.values()):
            entities["general"] = [query]
        
        return entities
    
    def _rewrite_and_expand(self, query: str, intent_type: str) -> Dict:
        """æ”¹å†™ä¸æ‰©å±•æŸ¥è¯¢"""
        # ç”ŸæˆBM25å‹å¥½çš„å…³é”®å­—
        keywords = re.findall(r'\w+', query)
        keywords = [w for w in keywords if len(w) > 1][:10]
        
        # ç”Ÿæˆå‘é‡æ£€ç´¢çš„è¯­ä¹‰åŒ–query
        if intent_type == "title":
            vector_query = f"{query} å®šä¹‰ æ¦‚å¿µ å«ä¹‰ ç®€ä»‹"
        elif intent_type == "fragment":
            vector_query = f"{query} è¯¦ç»† å…·ä½“ æ–¹æ³• æ“ä½œ æµç¨‹"
        else:
            vector_query = query
        
        return {
            "bm25_keywords": keywords,
            "vector_query": vector_query,
            "expanded_synonyms": self._expand_synonyms(keywords)
        }
    
    def _expand_synonyms(self, keywords: List[str]) -> List[str]:
        """æ‰©å±•åŒä¹‰è¯"""
        expanded = set(keywords)
        
        for keyword in keywords:
            if keyword in self.synonym_dict:
                synonyms = self.synonym_dict[keyword]
                if isinstance(synonyms, list):
                    expanded.update(synonyms)
                else:
                    expanded.add(synonyms)
        
        return list(expanded)
    
    def _bm25_retrieval(self, understanding_result: Dict, filters: Dict = None) -> List[Dict]:
        """â‘¢ BM25æ£€ç´¢"""
        try:
            if not self.opensearch_client:
                return self._mock_bm25_results()
            
            retrieval_config = understanding_result.get("retrieval_config", {})
            rewrite_result = understanding_result.get("rewrite_result", {})
            
            bm25_target = retrieval_config.get("bm25_target", "fragments")
            bm25_top_k = retrieval_config.get("bm25_top_k", 50)
            keywords = rewrite_result.get("bm25_keywords", [])
            
            # æ„å»ºæŸ¥è¯¢
            query_body = self._build_bm25_query(
                understanding_result["normalized_query"], 
                keywords, 
                bm25_target, 
                bm25_top_k, 
                filters
            )
            
            # æ‰§è¡Œæœç´¢
            response = self.opensearch_client.search(self.index_name, query_body)
            return self._process_bm25_results(response)
            
        except Exception as e:
            logger.error(f"BM25æ£€ç´¢å¤±è´¥: {str(e)}")
            return []
    
    def _build_bm25_query(self, query: str, keywords: List[str], target: str, size: int, filters: Dict = None) -> Dict:
        """æ„å»ºBM25æŸ¥è¯¢"""
        should_queries = []
        
        # ä¸»æŸ¥è¯¢
        should_queries.append({
            "multi_match": {
                "query": query,
                "fields": ["title^3", "content", "summary^2"],
                "boost": 2.0
            }
        })
        
        # å…³é”®è¯æŸ¥è¯¢
        for keyword in keywords:
            should_queries.append({
                "multi_match": {
                    "query": keyword,
                    "fields": ["title^3", "content^1.5", "summary^2"],
                    "boost": 1.5
                }
            })
        
        query_body = {
            "query": {
                "bool": {
                    "should": should_queries,
                    "minimum_should_match": 1
                }
            },
            "size": size,
            "highlight": {
                "fields": {
                    "title": {},
                    "content": {"fragment_size": 100, "number_of_fragments": 3}
                }
            },
            "sort": [{"_score": {"order": "desc"}}]
        }
        
        # æ·»åŠ è¿‡æ»¤æ¡ä»¶
        if filters:
            filter_conditions = []
            if filters.get('doc_types'):
                filter_conditions.append({"terms": {"doc_type": filters['doc_types']}})
            if filters.get('content_types'):
                filter_conditions.append({"terms": {"content_type": filters['content_types']}})
            
            if filter_conditions:
                query_body["query"]["bool"]["filter"] = filter_conditions
        
        return query_body
    
    def _process_bm25_results(self, response: Dict) -> List[Dict]:
        """å¤„ç†BM25æœç´¢ç»“æœ"""
        results = []
        hits = response.get('hits', {}).get('hits', [])
        
        for hit in hits:
            result = {
                "doc_id": hit['_source'].get("doc_id", ""),
                "section_id": hit['_source'].get("section_id", ""),
                "element_id": hit['_source'].get("element_id", ""),
                "title": hit['_source'].get("title", ""),
                "content": hit['_source'].get("content", ""),
                "content_type": hit['_source'].get("content_type", "text"),
                "page_number": hit['_source'].get("page_number", 1),
                "bbox": hit['_source'].get("bbox", {}),
                "score": hit['_score'],
                "source": "bm25",
                "highlight": hit.get('highlight', {}),
                "metadata": hit['_source'].get("metadata", {})
            }
            results.append(result)
        
        return results
    
    def _mock_bm25_results(self) -> List[Dict]:
        """æ¨¡æ‹ŸBM25ç»“æœ"""
        return [
            {
                "doc_id": "doc_001",
                "section_id": "section_001_001",
                "element_id": "element_001_001_001",
                "title": "HCPæ£€æµ‹æ–¹æ³•",
                "content": "ä½¿ç”¨ELISAæ–¹æ³•æ£€æµ‹å®¿ä¸»ç»†èƒè›‹ç™½å«é‡ï¼Œæ£€æµ‹é™ä¸º10ng/ml...",
                "content_type": "fragment",
                "page_number": 1,
                "bbox": {"x": 100, "y": 200, "width": 400, "height": 50},
                "score": 9.2,
                "source": "bm25",
                "highlight": {"content": ["<mark>HCP</mark>æ£€æµ‹"]},
                "metadata": {}
            }
        ]
    
    def _vector_retrieval(self, understanding_result: Dict, filters: Dict = None) -> List[Dict]:
        """â‘¢ å‘é‡æ£€ç´¢"""
        try:
            if not self.milvus_client or not self.embedding_model:
                return self._mock_vector_results()
            
            retrieval_config = understanding_result.get("retrieval_config", {})
            rewrite_result = understanding_result.get("rewrite_result", {})
            
            vector_query = rewrite_result.get("vector_query", "")
            vector_top_k = retrieval_config.get("vector_top_k", 50)
            
            # ç¼–ç æŸ¥è¯¢å‘é‡
            query_vector = self.embedding_model.encode(
                vector_query, 
                normalize_embeddings=self.normalize
            ).tolist()
            
            # æ‰§è¡Œå‘é‡æœç´¢
            results = self.milvus_client.search_vectors([query_vector], top_k=vector_top_k)
            return self._process_vector_results(results)
            
        except Exception as e:
            logger.error(f"å‘é‡æ£€ç´¢å¤±è´¥: {str(e)}")
            return []
    
    def _process_vector_results(self, results: List[Dict]) -> List[Dict]:
        """å¤„ç†å‘é‡æœç´¢ç»“æœ"""
        processed = []
        for result in results:
            # ä»metadataä¸­æå–é¢å¤–ä¿¡æ¯
            metadata = result.get("metadata", {})
            
            processed_result = {
                "doc_id": result.get("document_id", ""),
                "section_id": metadata.get("section_id", ""),
                "element_id": result.get("element_id", ""),
                "title": metadata.get("title", ""),
                "content": result.get("content", ""),
                "content_type": metadata.get("content_type", "text"),
                "page_number": metadata.get("page_number", 1),
                "bbox": metadata.get("bbox", {}),
                "score": result.get("score", 0.0),
                "source": "vector",
                "metadata": metadata
            }
            processed.append(processed_result)
        
        return processed
    
    def _mock_vector_results(self) -> List[Dict]:
        """æ¨¡æ‹Ÿå‘é‡ç»“æœ"""
        return [
            {
                "doc_id": "doc_002",
                "section_id": "section_002_001",
                "element_id": "element_002_001_001",
                "title": "ç”Ÿç‰©åˆ¶å“è´¨é‡æ§åˆ¶",
                "content": "è›‹ç™½è´¨çº¯åº¦æ£€æµ‹æ˜¯ç¡®ä¿ç”Ÿç‰©åˆ¶å“å®‰å…¨æ€§å’Œæœ‰æ•ˆæ€§çš„å…³é”®æ­¥éª¤...",
                "content_type": "fragment",
                "page_number": 2,
                "bbox": {"x": 100, "y": 300, "width": 400, "height": 60},
                "score": 0.89,
                "source": "vector",
                "metadata": {}
            }
        ]
    
    def _graph_retrieval(self, understanding_result: Dict, filters: Dict = None) -> List[Dict]:
        """â‘¢ å›¾è°±æ£€ç´¢"""
        try:
            if not self.neo4j_client:
                return []
            
            # ç®€åŒ–çš„å›¾è°±æŸ¥è¯¢é€»è¾‘
            entities = understanding_result.get("entities", {})
            if not entities:
                return []
            
            # æ‰§è¡Œå›¾è°±æŸ¥è¯¢ - ä¿®å¤æŸ¥è¯¢é€»è¾‘ä»¥é€‚åº”å®é™…æ•°æ®ç»“æ„
            with self.neo4j_client.session() as session:
                # å®‰å…¨åœ°æå–å®ä½“åç§°
                entity_names = []
                if entities:
                    for entity_list in entities.values():
                        if entity_list and len(entity_list) > 0:
                            entity_names.extend(entity_list)
                
                if not entity_names:
                    return []  # æ²¡æœ‰å®ä½“åˆ™è¿”å›ç©ºç»“æœ
                
                # æ‰©å±•åŒä¹‰è¯
                expanded_entities = self._expand_entity_synonyms(entity_names)
                logger.info(f"å›¾è°±æ£€ç´¢å®ä½“: {entity_names} -> æ‰©å±•å: {expanded_entities}")
                
                # ç­–ç•¥1: éå†æ‰€æœ‰æ‰©å±•å®ä½“è¿›è¡Œå…³ç³»æŸ¥è¯¢
                all_graph_results = []
                all_entity_results = []
                
                for entity_name in expanded_entities:
                    # æŸ¥è¯¢å®ä½“å…³ç³»
                    cypher_query = """
                    MATCH (a:Entity)-[r]->(b:Entity)
                    WHERE a.canonical CONTAINS $entity_name OR b.canonical CONTAINS $entity_name
                    RETURN a, b, type(r) as relation
                    LIMIT 5
                    """
                    
                    result = session.run(cypher_query, entity_name=entity_name)
                    graph_results = list(result)
                    all_graph_results.extend(graph_results)
                    
                    # å¦‚æœæ²¡æœ‰å…³ç³»ï¼ŒæŸ¥è¯¢å•ä¸ªå®ä½“
                    if not graph_results:
                        cypher_query2 = """
                        MATCH (n:Entity)
                        WHERE n.canonical CONTAINS $entity_name
                        RETURN n
                        LIMIT 3
                        """
                        
                        result2 = session.run(cypher_query2, entity_name=entity_name)
                        entity_results = list(result2)
                        all_entity_results.extend(entity_results)
                
                # å¤„ç†ç»“æœ
                if all_graph_results:
                    logger.info(f"å›¾è°±æ£€ç´¢æ‰¾åˆ°{len(all_graph_results)}ä¸ªå…³ç³»")
                    return self._process_graph_results(all_graph_results)
                
                if all_entity_results:
                    logger.info(f"å›¾è°±æ£€ç´¢æ‰¾åˆ°{len(all_entity_results)}ä¸ªç›¸å…³å®ä½“")
                    return self._process_single_entity_results(all_entity_results)
                
                logger.info(f"å›¾è°±æ£€ç´¢æœªæ‰¾åˆ°ä¸'{entity_names}'ç›¸å…³çš„å†…å®¹")
                return []
                
        except Exception as e:
            logger.error(f"å›¾è°±æ£€ç´¢å¤±è´¥: {str(e)}")
            return []
    
    def _process_graph_results(self, results: List) -> List[Dict]:
        """å¤„ç†å›¾è°±æœç´¢ç»“æœ"""
        processed = []
        for record in results:
            a_node = dict(record["a"])
            b_node = dict(record["b"]) 
            relation = record["relation"]
            
            # ä½¿ç”¨canonicalå­—æ®µï¼Œå› ä¸ºnameå­—æ®µä¸ºç©º
            a_name = a_node.get('canonical', '') or a_node.get('name', '') or 'å®ä½“A'
            b_name = b_node.get('canonical', '') or b_node.get('name', '') or 'å®ä½“B'
            
            content = f"{a_name} {relation} {b_name}"
            
            processed.append({
                "doc_id": f"graph_{hash(content)}",
                "section_id": f"graph_section_{hash(content)}",
                "element_id": f"graph_element_{hash(content)}",
                "title": f"å›¾è°±å…³ç³»ï¼š{relation}",
                "content": content,
                "content_type": "graph",
                "page_number": 1,
                "bbox": {},
                "score": 0.8,
                "source": "graph",
                "metadata": {"relation": relation, "source_node": a_node, "target_node": b_node}
            })
        
        return processed
    
    def _process_single_entity_results(self, results: List) -> List[Dict]:
        """å¤„ç†å•ä¸ªå®ä½“æœç´¢ç»“æœ"""
        processed = []
        for record in results:
            entity = dict(record["n"])
            
            # ä½¿ç”¨canonicalå­—æ®µï¼Œå› ä¸ºnameå­—æ®µä¸ºç©º
            entity_name = entity.get('canonical', '') or entity.get('name', '') or 'æœªçŸ¥å®ä½“'
            entity_type = entity.get('entity_type', '') or entity.get('type', '') or 'æœªçŸ¥ç±»å‹'
            
            content = f"ç›¸å…³å®ä½“: {entity_name} (ç±»å‹: {entity_type})"
            
            processed.append({
                "doc_id": f"entity_{hash(entity_name)}",
                "section_id": f"entity_section_{hash(entity_name)}",
                "element_id": f"entity_element_{hash(entity_name)}",
                "title": entity_name,
                "content": content,
                "content_type": "entity",
                "page_number": 1,
                "bbox": {},
                "score": 0.6,
                "source": "graph_entity",
                "metadata": {
                    "entity_type": entity_type,
                    "entity_data": entity
                }
            })
        
        return processed
    
    def _expand_entity_synonyms(self, entity_names: List[str]) -> List[str]:
        """æ‰©å±•å®ä½“åŒä¹‰è¯"""
        expanded = set()
        
        for entity_name in entity_names:
            # æ·»åŠ åŸå§‹å®ä½“
            expanded.add(entity_name)
            
            # æŸ¥æ‰¾åŒä¹‰è¯
            if entity_name in self.synonym_dict:
                synonyms = self.synonym_dict[entity_name]
                if isinstance(synonyms, list):
                    expanded.update(synonyms)
                else:
                    expanded.add(synonyms)
            
            # åå‘æŸ¥æ‰¾
            for key, synonyms in self.synonym_dict.items():
                if isinstance(synonyms, list):
                    if entity_name in synonyms:
                        expanded.add(key)
                        expanded.update(synonyms)
                else:
                    if entity_name == synonyms:
                        expanded.add(key)
        
        # æ·»åŠ ç‰¹æ®Šæ˜ å°„è§„åˆ™
        entity_mappings = {
            "HCP": ["å®¿ä¸»ç»†èƒè›‹ç™½", "Host Cell Protein"],
            "CHO": ["ä¸­å›½ä»“é¼ åµå·¢", "CHO-K1"],
            "æ¡ˆä¾‹åˆ†äº«": ["æ¡ˆä¾‹", "åˆ†äº«", "ç»éªŒ"],
            "è®¢è´§ä¿¡æ¯": ["è®¢è´§", "é‡‡è´­", "è®¢å•"]
        }
        
        for entity_name in entity_names:
            if entity_name in entity_mappings:
                expanded.update(entity_mappings[entity_name])
        
        return list(expanded)


    def _aggregate_by_section(self, bm25_results: List[Dict], vector_results: List[Dict], 
                            graph_results: List[Dict], understanding_result: Dict) -> List[Dict]:
        """â‘£ èšåˆä¸åˆ†æ•°èåˆï¼ˆåˆ° section ç²’åº¦ï¼‰"""
        try:
            all_results = bm25_results + vector_results + graph_results
            section_groups = {}
            
            for result in all_results:
                section_id = result.get("section_id", "")
                if not section_id:
                    continue
                
                if section_id not in section_groups:
                    section_groups[section_id] = {
                        "section_id": section_id,
                        "doc_id": result.get("doc_id", ""),
                        "title": result.get("title", ""),
                        "bm25_scores": [],
                        "vector_scores": [],
                        "graph_scores": [],
                        "evidence_elements": [],
                        "all_sources": set(),
                        "metadata": {"page_numbers": set(), "content_types": set()},
                        "has_title_match": False  # è·Ÿè¸ªæ˜¯å¦åŒ…å«titleç±»å‹çš„åŒ¹é…
                    }
                
                group = section_groups[section_id]
                source = result.get("source", "unknown")
                score = result.get("score", 0)
                
                # ğŸ”§ æ„å›¾æ„ŸçŸ¥çš„åˆ†æ•°åŠ æƒ
                intent_type = understanding_result.get("intent_type", "fragment")
                content_type = result.get("content_type", "")
                
                # å¦‚æœæ˜¯titleæ„å›¾ä¸”å‘½ä¸­äº†titleç±»å‹çš„å†…å®¹ï¼Œç»™äºˆæ›´é«˜æƒé‡
                if intent_type == "title" and content_type == "title":
                    score = score * 1.5  # titleæ„å›¾ä¸‹titleå†…å®¹åŠ æƒ150%
                    group["has_title_match"] = True  # æ ‡è®°è¿™ä¸ªsectionåŒ…å«titleåŒ¹é…
                    logger.debug(f"Titleæ„å›¾æ£€æµ‹åˆ°titleå†…å®¹åŒ¹é…ï¼Œåˆ†æ•°ä»åŸå§‹å€¼åŠ æƒåˆ°: {score}")
                
                # æŒ‰æ¥æºåˆ†ç±»åˆ†æ•°
                if source == "bm25":
                    group["bm25_scores"].append(score)
                elif source == "vector":
                    group["vector_scores"].append(score)
                elif source == "graph":
                    group["graph_scores"].append(score)
                
                group["all_sources"].add(source)
                
                # æ”¶é›†è¯æ®å…ƒç´ 
                evidence = {
                    "element_id": result.get("element_id", ""),
                    "content": result.get("content", ""),
                    "score": score,
                    "source": source,
                    "highlight": result.get("highlight", {}),
                    "bbox": result.get("bbox", {}),
                    "page_number": result.get("page_number", 1)
                }
                group["evidence_elements"].append(evidence)
                
                # æ›´æ–°å…ƒæ•°æ®
                if result.get("page_number"):
                    group["metadata"]["page_numbers"].add(result.get("page_number"))
                if result.get("content_type"):
                    group["metadata"]["content_types"].add(result.get("content_type"))
            
            # å¯¹æ¯ä¸ªsectionè¿›è¡Œåˆ†æ•°èåˆ
            section_candidates = []
            for section_id, group in section_groups.items():
                # å½’ä¸€åŒ–å„è·¯åˆ†æ•°
                bm25_norm = self._normalize_scores_list(group["bm25_scores"])
                vector_norm = self._normalize_scores_list(group["vector_scores"])
                graph_norm = self._normalize_scores_list(group["graph_scores"])
                
                # ğŸ”§ æ„å›¾æ„ŸçŸ¥çš„åˆ†æ•°èåˆç­–ç•¥
                intent_type = understanding_result.get("intent_type", "fragment")
                if intent_type == "title":
                    # titleæ„å›¾ï¼šæ›´é‡è§†BM25çš„ç²¾ç¡®åŒ¹é…ï¼ˆå› ä¸ºtitleé€šå¸¸æ˜¯å…³é”®è¯åŒ¹é…ï¼‰
                    final_score = 0.6 * bm25_norm + 0.4 * vector_norm + 0.0 * graph_norm
                else:
                    # fragmentæ„å›¾ï¼šæ›´é‡è§†è¯­ä¹‰åŒ¹é…
                    final_score = 0.4 * bm25_norm + 0.6 * vector_norm + 0.0 * graph_norm
                
                # é€‰æ‹©Top-1è¯æ®å…ƒç´ 
                top_evidence = sorted(group["evidence_elements"], 
                                    key=lambda x: x["score"], reverse=True)[:1]
                
                section_candidate = {
                    "section_id": section_id,
                    "doc_id": group["doc_id"],
                    "title": group["title"],
                    "final_score": final_score,
                    "bm25_score": bm25_norm,
                    "vector_score": vector_norm,
                    "graph_score": graph_norm,
                    "sources": list(group["all_sources"]),
                    "evidence_elements": top_evidence,
                    "evidence_count": len(group["evidence_elements"]),
                    "metadata": {
                        **group["metadata"],
                        "page_numbers": list(group["metadata"]["page_numbers"]),
                        "content_types": list(group["metadata"]["content_types"]),
                        "aggregation_type": "section",
                        "has_title_match": group["has_title_match"],
                        "intent_type": intent_type
                    }
                }
                
                section_candidates.append(section_candidate)
            
            # æŒ‰æœ€ç»ˆåˆ†æ•°æ’åºï¼Œå–Top-50ä¸ªsectionä½œä¸ºé‡æ’å€™é€‰
            section_candidates.sort(key=lambda x: x["final_score"], reverse=True)
            return section_candidates[:50]
            
        except Exception as e:
            logger.error(f"èšåˆå¤±è´¥: {str(e)}")
            return []
    
    def _normalize_scores_list(self, scores: List[float]) -> float:
        """å½’ä¸€åŒ–åˆ†æ•°åˆ—è¡¨ - ä¿ç•™åˆ†æ•°çš„ç›¸å¯¹é‡è¦æ€§"""
        if not scores:
            return 0.0
        
        if len(scores) == 1:
            return scores[0]
        
        # ğŸ”§ ä¿®å¤ï¼šä½¿ç”¨åŠ æƒå¹³å‡è€Œä¸æ˜¯ç®€å•çš„Min-Maxå½’ä¸€åŒ–
        # è¿™æ ·å¯ä»¥ä¿ç•™é«˜åˆ†æ•°çš„ä¼˜åŠ¿ï¼Œä¸ä¼šè¢«è¿‡åº¦å‹ç¼©
        total_score = sum(scores)
        if total_score == 0:
            return 0.0
        
        # ä½¿ç”¨åŠ æƒå¹³å‡ï¼šæ¯ä¸ªåˆ†æ•°çš„æƒé‡ = åˆ†æ•°åœ¨æ€»åˆ†ä¸­çš„å æ¯”
        weights = [score / total_score for score in scores]
        weighted_average = sum(score * weight for score, weight in zip(scores, weights))
        
        return weighted_average
    
    def _rerank_sections(self, candidates: List[Dict], understanding_result: Dict) -> Optional[Dict]:
        """â‘¤ æ„å›¾æ„ŸçŸ¥çš„é‡æ’ï¼ˆæŠŠ"æœ€ç›¸å…³çš„å†…å®¹"æ”¾åˆ°ç¬¬ä¸€ï¼‰"""
        try:
            if not candidates:
                return None
            
            original_query = understanding_result.get("normalized_query", "")
            
            if self.reranker:
                # ä½¿ç”¨çœŸå®çš„é‡æ’æ¨¡å‹
                query_section_pairs = []
                for candidate in candidates:
                    rerank_text = self._build_rerank_text(candidate)
                    query_section_pairs.append([original_query, rerank_text])
                
                # æ‰¹é‡é‡æ’
                batch_size = self.reranker_config.get('batch_size', 16)
                rerank_scores = []
                
                for i in range(0, len(query_section_pairs), batch_size):
                    batch = query_section_pairs[i:i+batch_size]
                    batch_scores = self.reranker.predict(batch)
                    rerank_scores.extend(batch_scores)
                
                # æ›´æ–°åˆ†æ•°å¹¶æ’åº
                for i, candidate in enumerate(candidates):
                    candidate["rerank_score"] = float(rerank_scores[i])
                    candidate["final_score"] = candidate["final_score"] * 0.3 + candidate["rerank_score"] * 0.7
            else:
                # ğŸ”§ ä½¿ç”¨æ„å›¾æ„ŸçŸ¥çš„ç®€å•è¯„åˆ†
                intent_type = understanding_result.get("intent_type", "fragment")
                for candidate in candidates:
                    title = candidate.get("title", "")
                    evidence_text = " ".join([ev.get("content", "") for ev in candidate.get("evidence_elements", [])])
                    
                    # è®¡ç®—æŸ¥è¯¢è¯åŒ¹é…åº¦
                    query_words = set(original_query.lower().split())
                    title_words = set(title.lower().split())
                    evidence_words = set(evidence_text.lower().split())
                    
                    title_match = len(query_words.intersection(title_words)) / len(query_words) if query_words else 0
                    evidence_match = len(query_words.intersection(evidence_words)) / len(query_words) if query_words else 0
                    
                    # ğŸ”§ æ ¹æ®æ„å›¾ç±»å‹è°ƒæ•´é‡æ’æƒé‡
                    if intent_type == "title":
                        # titleæ„å›¾ï¼šæé‡è§†æ ‡é¢˜åŒ¹é…
                        rerank_score = title_match * 3 + evidence_match * 0.5
                        final_weight = 0.7  # é‡æ’æƒé‡æ›´é«˜
                    else:
                        # fragmentæ„å›¾ï¼šå¹³è¡¡æ ‡é¢˜å’Œå†…å®¹åŒ¹é…
                        rerank_score = title_match * 1.5 + evidence_match
                        final_weight = 0.5  # æ ‡å‡†æƒé‡
                    
                    candidate["rerank_score"] = rerank_score
                    candidate["final_score"] = candidate["final_score"] * (1 - final_weight) + rerank_score * final_weight
            
            # æ’åºå¹¶è¿”å›Top-1
            candidates.sort(key=lambda x: x["final_score"], reverse=True)
            top_section = candidates[0]
            
            # ç‰‡æ®µçº§é«˜äº®
            top_section["evidence_highlights"] = self._select_evidence_highlights(top_section, original_query)
            
            return top_section
            
        except Exception as e:
            logger.error(f"é‡æ’å¤±è´¥: {str(e)}")
            return candidates[0] if candidates else None
    
    def _build_rerank_text(self, candidate: Dict) -> str:
        """æ„å»ºé‡æ’ç”¨çš„æ–‡æœ¬"""
        title = candidate.get("title", "")
        evidence_elements = candidate.get("evidence_elements", [])
        
        # å–å‰1ä¸ªæœ€ç›¸å…³çš„ç‰‡æ®µ
        top_evidence = evidence_elements[:1]
        evidence_texts = [ev.get("content", "") for ev in top_evidence]
        
        # ç»„åˆæ–‡æœ¬
        rerank_text = title
        if evidence_texts:
            rerank_text += " " + " ".join(evidence_texts)
        
        # æˆªæ–­åˆ°512 tokensï¼ˆç²—ç•¥æŒ‰å­—ç¬¦æ•°ä¼°ç®—ï¼‰
        max_chars = 512 * 2
        if len(rerank_text) > max_chars:
            rerank_text = rerank_text[:max_chars] + "..."
        
        return rerank_text
    
    def _select_evidence_highlights(self, top_section: Dict, query: str) -> List[Dict]:
        """ç‰‡æ®µçº§é«˜äº®é€‰æ‹©"""
        evidence_elements = top_section.get("evidence_elements", [])
        
        if not evidence_elements:
            return []
        
        # è®¡ç®—é«˜äº®åˆ†æ•°
        for evidence in evidence_elements:
            content = evidence.get("content", "")
            query_words = set(query.lower().split())
            content_words = set(content.lower().split())
            
            match_score = len(query_words.intersection(content_words)) / len(query_words) if query_words else 0
            evidence["highlight_score"] = evidence.get("score", 0) * 0.7 + match_score * 0.3
        
        # æŒ‰é«˜äº®åˆ†æ•°æ’åºï¼Œé€‰æ‹©1æ¡
        evidence_elements.sort(key=lambda x: x.get("highlight_score", 0), reverse=True)
        return evidence_elements[:1]
    
    def _expand_section_content(self, top_section: Dict) -> List[Dict]:
        """â‘· æ‰©å±•ï¼ˆæŠŠ"ä¸€å®¶å­"æ‹‰é½ï¼‰- å¤šæ•°æ®æºèåˆ"""
        try:
            section_id = top_section.get("section_id")
            if not section_id:
                return []
            
            expanded_elements = []
            
            # ğŸ”§ ç¬¬ä¸€æ­¥ï¼šä»OpenSearch/MySQLæŸ¥è¯¢è¡¨æ ¼å’Œå›¾ç‰‡å†…å®¹
            multimodal_elements = self._query_section_multimodal_content(section_id, top_section)
            if multimodal_elements:
                expanded_elements.extend(multimodal_elements)
            
            # ğŸ”§ ç¬¬äºŒæ­¥ï¼šä»Neo4jæŸ¥è¯¢å®ä½“å…³ç³»å†…å®¹
            if self.neo4j_client:
                entity_elements = self._query_actual_graph_structure(section_id, top_section)
                if entity_elements:
                    expanded_elements.extend(entity_elements)
            
            # ğŸ”§ ç¬¬ä¸‰æ­¥ï¼šå¦‚æœéƒ½æ²¡æœ‰æ•°æ®ï¼Œä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®
            if not expanded_elements:
                logger.info(f"æœªæ‰¾åˆ°section_id={section_id}çš„æ‰©å±•å†…å®¹ï¼Œä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®")
                return self._mock_section_expansion(top_section)
            
            return expanded_elements
                
        except Exception as e:
            logger.error(f"å†…å®¹æ‰©å±•å¤±è´¥: {str(e)}")
            return self._mock_section_expansion(top_section)
    
    def _query_section_multimodal_content(self, section_id: str, top_section: Dict) -> List[Dict]:
        """æŸ¥è¯¢sectionç›¸å…³çš„è¡¨æ ¼å’Œå›¾ç‰‡å†…å®¹"""
        try:
            multimodal_elements = []
            
            # ğŸ”§ ç­–ç•¥1ï¼šä»OpenSearchæŸ¥è¯¢è¡¨æ ¼å’Œå›¾ç‰‡
            if self.opensearch_client:
                try:
                    # æŸ¥è¯¢è¯¥sectionä¸‹çš„è¡¨æ ¼å’Œå›¾ç‰‡
                    query_body = {
                        "query": {
                            "bool": {
                                "must": [
                                    {"term": {"section_id.keyword": section_id}},
                                    {"terms": {"content_type.keyword": ["table", "image"]}}
                                ]
                            }
                        },
                        "size": 50
                    }
                    
                    response = self.opensearch_client.search(self.index_name, query_body)
                    
                    if response and 'hits' in response and 'hits' in response['hits']:
                        for hit in response['hits']['hits']:
                            source = hit['_source']
                            element = {
                                "element_id": source.get("element_id", ""),
                                "content_type": source.get("content_type", ""),
                                "content": source.get("content", ""),
                                "title": source.get("title", ""),
                                "order": len(multimodal_elements) + 1,
                                "page_number": source.get("page_number", 1),
                                "bbox": source.get("bbox", {}),
                                "metadata": {
                                    "doc_id": source.get("doc_id", ""),
                                    "section_id": section_id,
                                    "source": "opensearch_multimodal"
                                }
                            }
                            multimodal_elements.append(element)
                    
                except Exception as e:
                    logger.warning(f"OpenSearchæŸ¥è¯¢è¡¨æ ¼å›¾ç‰‡å¤±è´¥: {str(e)}")
            
            # ğŸ”§ ç­–ç•¥2ï¼šä»MySQLæŸ¥è¯¢ï¼ˆå¦‚æœæœ‰MySQLè¿æ¥ï¼‰
            # TODO: è¿™é‡Œå¯ä»¥æ·»åŠ MySQLæŸ¥è¯¢é€»è¾‘
            
            logger.info(f"æ‰¾åˆ°{len(multimodal_elements)}ä¸ªå¤šåª’ä½“å…ƒç´ ")
            return multimodal_elements
            
        except Exception as e:
            logger.error(f"æŸ¥è¯¢å¤šåª’ä½“å†…å®¹å¤±è´¥: {str(e)}")
            return []
    
    def _query_actual_graph_structure(self, section_id: str, top_section: Dict) -> List[Dict]:
        """åŸºäºå®é™…æ•°æ®åº“ç»“æ„æŸ¥è¯¢ç›¸å…³å†…å®¹"""
        try:
            expanded_elements = []
            
            with self.neo4j_client.session() as session:
                # ç­–ç•¥1: ä»section_idæå–çœŸå®çš„doc_id
                actual_doc_id = self._extract_doc_id_from_section(section_id, top_section)
                logger.info(f"æå–åˆ°çš„doc_id: {actual_doc_id}")
                
                if actual_doc_id:
                    # æŸ¥è¯¢ä¸è¯¥æ–‡æ¡£ç›¸å…³çš„æ‰€æœ‰å®ä½“
                    cypher_query = """
                    MATCH (d:Document {id: $doc_id})-[r:CONTAINS]->(e:Entity)
                    RETURN e, type(r) as relation_type
                    LIMIT 20
                    """
                    result = session.run(cypher_query, doc_id=actual_doc_id)
                    
                    for i, record in enumerate(result):
                        entity = dict(record["e"])
                        entity_name = entity.get('name', '') or entity.get('canonical', '') or f"å®ä½“_{i+1}"
                        entity_type = entity.get('entity_type', '') or entity.get('type', '') or "æœªçŸ¥ç±»å‹"
                        
                        element = {
                            "element_id": f"{section_id}_entity_{i}",
                            "content_type": "entity",
                            "content": f"å®ä½“: {entity_name} (ç±»å‹: {entity_type})",
                            "title": entity_name,
                            "order": i + 1,
                            "page_number": entity.get("page_number", 1),
                            "bbox": {},
                            "metadata": {
                                "doc_id": actual_doc_id,
                                "section_id": section_id,
                                "entity_type": entity_type,
                                "entity_id": entity.get('entity_id', ''),
                                "confidence": entity.get('confidence', 0.0),
                                "source": "neo4j_entity"
                            }
                        }
                        expanded_elements.append(element)
                
                # ç­–ç•¥2: å¦‚æœæ‰¾ä¸åˆ°Documentï¼Œå°è¯•ç›´æ¥æŸ¥æ‰¾ç›¸å…³å®ä½“
                if not expanded_elements:
                    # ä»sectionæ ‡é¢˜ä¸­æå–å…³é”®è¯ï¼ŒæŸ¥æ‰¾ç›¸å…³å®ä½“
                    section_title = top_section.get("title", "")
                    if section_title:
                        # æå–å¯èƒ½çš„å®ä½“åç§°
                        keywords = [word for word in section_title.split() if len(word) > 2]
                        
                        for keyword in keywords[:3]:  # é™åˆ¶å…³é”®è¯æ•°é‡
                            cypher_query = """
                            MATCH (e:Entity)
                            WHERE e.name CONTAINS $keyword OR e.canonical CONTAINS $keyword
                            RETURN e
                            LIMIT 5
                            """
                            result = session.run(cypher_query, keyword=keyword)
                            
                            for i, record in enumerate(result):
                                entity = dict(record["e"])
                                entity_name = entity.get('name', '') or entity.get('canonical', '') or f"ç›¸å…³å®ä½“_{i+1}"
                                entity_type = entity.get('entity_type', '') or entity.get('type', '') or "æœªçŸ¥ç±»å‹"
                                
                                element = {
                                    "element_id": f"{section_id}_related_{keyword}_{i}",
                                    "content_type": "related_entity",
                                    "content": f"ç›¸å…³å®ä½“: {entity_name} (åŒ¹é…å…³é”®è¯: {keyword})",
                                    "title": entity_name,
                                    "order": len(expanded_elements) + i + 1,
                                    "page_number": entity.get("page_number", 1),
                                    "bbox": {},
                                    "metadata": {
                                        "doc_id": "related",
                                        "section_id": section_id,
                                        "entity_type": entity_type,
                                        "entity_id": entity.get('entity_id', ''),
                                        "match_keyword": keyword,
                                        "source": "neo4j_related"
                                    }
                                }
                                expanded_elements.append(element)
                
                # ç­–ç•¥3: æ··åˆå®é™…æ•°æ®å’Œç»“æ„åŒ–å†…å®¹
                if expanded_elements:
                    # å¦‚æœæ‰¾åˆ°å®é™…å®ä½“ï¼Œæ·»åŠ ç»“æ„åŒ–çš„ç« èŠ‚å†…å®¹
                    section_title = top_section.get("title", "")
                    if section_title:
                        # æ·»åŠ æ ‡é¢˜å…ƒç´ 
                        title_element = {
                            "element_id": f"{section_id}_title",
                            "content_type": "title",
                            "content": section_title,
                            "title": section_title,
                            "order": 0,
                            "page_number": 1,
                            "bbox": {},
                            "metadata": {
                                "doc_id": actual_doc_id,
                                "section_id": section_id,
                                "source": "mixed_content"
                            }
                        }
                        expanded_elements.insert(0, title_element)
                        
                        # é‡æ–°è°ƒæ•´order
                        for i, element in enumerate(expanded_elements[1:], 1):
                            element["order"] = i
                
                logger.info(f"ä»å®é™…å›¾æ•°æ®åº“ç»“æ„æŸ¥è¯¢åˆ°{len(expanded_elements)}ä¸ªç›¸å…³å…ƒç´ ")
                return expanded_elements
                
        except Exception as e:
            logger.error(f"å®é™…å›¾æ•°æ®åº“ç»“æ„æŸ¥è¯¢å¤±è´¥: {str(e)}")
            return []
    
    def _extract_doc_id_from_section(self, section_id: str, top_section: Dict) -> any:
        """ä»section_idæå–çœŸå®çš„doc_id"""
        try:
            # ç­–ç•¥1: ä»top_sectionè·å–ï¼Œä½†éœ€è¦éªŒè¯æ ¼å¼
            doc_id_from_section = top_section.get("doc_id", "")
            
            # ç­–ç•¥2: ä»section_idè§£æ
            # section_idæ ¼å¼: 20250818_170435_05dc2896_doc#2025-08-18#7_0009
            if "#" in section_id:
                parts = section_id.split("#")
                if len(parts) >= 3:
                    # ä»æœ€åä¸€éƒ¨åˆ†æå–æ•°å­— (å¦‚: 7_0009 -> 7)
                    last_part = parts[-1]
                    if "_" in last_part:
                        doc_id_str = last_part.split("_")[0]
                        try:
                            doc_id = int(doc_id_str)
                            logger.info(f"ä»section_idè§£æåˆ°doc_id: {doc_id}")
                            return doc_id
                        except ValueError:
                            pass
            
            # ç­–ç•¥3: æ£€æŸ¥æ˜¯å¦æ˜¯æµ‹è¯•æ•°æ®ï¼Œè½¬æ¢ä¸ºå®é™…ID
            if doc_id_from_section == "test_doc_001" or not doc_id_from_section:
                # æŸ¥è¯¢æ•°æ®åº“ä¸­å®é™…å­˜åœ¨çš„ç¬¬ä¸€ä¸ªDocument ID
                with self.neo4j_client.session() as session:
                    result = session.run("MATCH (d:Document) RETURN d.id as doc_id LIMIT 1")
                    for record in result:
                        actual_id = record["doc_id"]
                        logger.info(f"ä½¿ç”¨æ•°æ®åº“ä¸­çš„å®é™…doc_id: {actual_id}")
                        return actual_id
            
            # ç­–ç•¥4: å°è¯•ç›´æ¥ä½¿ç”¨åŸå§‹doc_id
            if doc_id_from_section:
                logger.info(f"ä½¿ç”¨åŸå§‹doc_id: {doc_id_from_section}")
                return doc_id_from_section
            
            logger.warning("æ— æ³•æå–æœ‰æ•ˆçš„doc_id")
            return None
            
        except Exception as e:
            logger.error(f"æå–doc_idå¤±è´¥: {str(e)}")
            return None
    
    def _mock_section_expansion(self, top_section: Dict) -> List[Dict]:
        """æ¨¡æ‹Ÿsectionæ‰©å±•å†…å®¹"""
        section_id = top_section.get("section_id", "")
        doc_id = top_section.get("doc_id", "")
        
        return [
            {
                "element_id": f"{section_id}_title",
                "content_type": "title",
                "content": top_section.get("title", ""),
                "title": top_section.get("title", ""),
                "order": 1,
                "page_number": 1,
                "bbox": {},
                "metadata": {
                    "doc_id": doc_id, 
                    "section_id": section_id,
                    "source": "mock_data"
                }
            },
            {
                "element_id": f"{section_id}_paragraph_001",
                "content_type": "paragraph",
                "content": "è¿™æ˜¯è¯¥ç« èŠ‚çš„ç¬¬ä¸€æ®µå†…å®¹ï¼Œè¯¦ç»†æè¿°äº†ç›¸å…³çš„æŠ€æœ¯è¦ç‚¹å’Œæ“ä½œè§„èŒƒ...",
                "title": "",
                "order": 2,
                "page_number": 1,
                "bbox": {"x": 100, "y": 200, "width": 400, "height": 50},
                "metadata": {
                    "doc_id": doc_id, 
                    "section_id": section_id,
                    "source": "mock_data"
                }
            },
            {
                "element_id": f"{section_id}_table_001",
                "content_type": "table",
                "content": "å‚æ•°åç§° | æ ‡å‡†å€¼ | æ£€æµ‹æ–¹æ³•\nHCPå«é‡ | <100ng/mg | ELISA\npHå€¼ | 7.0Â±0.2 | pHè®¡",
                "title": "å…³é”®å‚æ•°è¡¨",
                "order": 3,
                "page_number": 1,
                "bbox": {"x": 100, "y": 300, "width": 400, "height": 100},
                "metadata": {
                    "doc_id": doc_id, 
                    "section_id": section_id,
                    "source": "mock_data"
                }
            },
            {
                "element_id": f"{section_id}_image_001",
                "content_type": "image",
                "content": "å›¾1ï¼šHCPæ£€æµ‹æµç¨‹ç¤ºæ„å›¾",
                "title": "æ£€æµ‹æµç¨‹å›¾",
                "order": 4,
                "page_number": 2,
                "bbox": {"x": 100, "y": 100, "width": 400, "height": 300},
                "metadata": {
                    "doc_id": doc_id, 
                    "section_id": section_id,
                    "source": "mock_data",
                    "image_path": "/images/hcp_process.jpg"
                }
            }
        ]
    
    def _enrich_multimodal_details(self, top_section: Dict) -> List[Dict]:
        """â‘§ å›¾è¡¨ç»†èŠ‚ï¼ˆMySQLï¼‰- åŸºäºsectionæŸ¥è¯¢MySQLè·å–å›¾è¡¨è¯¦ç»†ä¿¡æ¯"""
        try:
            section_id = top_section.get("section_id")
            if not section_id:
                logger.warning("section_idä¸ºç©ºï¼Œæ— æ³•æŸ¥è¯¢å›¾è¡¨ç»†èŠ‚")
                return []
            
            enriched_content = []
            
            # ğŸ”§ ç¬¬ä¸€æ­¥ï¼šæŸ¥è¯¢figuresè¡¨è·å–å›¾ç‰‡ä¿¡æ¯
            figures = self._query_figures_from_mysql(section_id)
            enriched_content.extend(figures)
            
            # ğŸ”§ ç¬¬äºŒæ­¥ï¼šæŸ¥è¯¢tablesè¡¨è·å–è¡¨æ ¼ä¿¡æ¯
            tables = self._query_tables_from_mysql(section_id)
            enriched_content.extend(tables)
            
            logger.info(f"ä»MySQLæŸ¥è¯¢åˆ°{len(enriched_content)}ä¸ªå›¾è¡¨å…ƒç´ ")
            return enriched_content
            
        except Exception as e:
            logger.error(f"å›¾è¡¨ç»†èŠ‚è¡¥å……å¤±è´¥: {str(e)}")
            return []
    
    def _query_figures_from_mysql(self, section_id: str) -> List[Dict]:
        """ä»MySQL figuresè¡¨æŸ¥è¯¢å›¾ç‰‡ä¿¡æ¯"""
        try:
            if not hasattr(self, 'mysql_client') or not self.mysql_client:
                logger.debug("MySQLå®¢æˆ·ç«¯æœªåˆå§‹åŒ–ï¼Œè·³è¿‡figuresæŸ¥è¯¢")
                return []
            
            session = self.mysql_client.get_session()
            try:
                # æŸ¥è¯¢è¯¥sectionä¸‹çš„æ‰€æœ‰å›¾ç‰‡
                query = """
                SELECT elem_id, section_id, image_path, caption, page, bbox_norm, bind_to_elem_id
                FROM figures 
                WHERE section_id = :section_id
                ORDER BY page, elem_id
                """
                
                result = session.execute(text(query), {"section_id": section_id})
                figures = []
                
                for row in result:
                    figure_element = {
                        "element_id": row.elem_id,
                        "content_type": "image",
                        "content": row.caption or f"å›¾ç‰‡ {row.elem_id}",
                        "title": row.caption or "å›¾ç‰‡",
                        "order": len(figures) + 1,
                        "page_number": row.page,
                        "bbox": row.bbox_norm or {},
                        "metadata": {
                            "section_id": section_id,
                            "source": "mysql_figures",
                            "bind_to_elem_id": row.bind_to_elem_id
                        },
                        "image_details": {
                            "image_path": row.image_path,
                            "caption": row.caption,
                            "alt_text": row.caption or f"å›¾ç‰‡ {row.elem_id}",
                            "page": row.page,
                            "bbox": row.bbox_norm,
                            "source": "mysql"
                        }
                    }
                    figures.append(figure_element)
                
                logger.info(f"ä»MySQLæŸ¥è¯¢åˆ°{len(figures)}å¼ å›¾ç‰‡")
                return figures
                
            finally:
                session.close()
                
        except Exception as e:
            logger.error(f"æŸ¥è¯¢figuresè¡¨å¤±è´¥: {str(e)}")
            return []
    
    def _query_tables_from_mysql(self, section_id: str) -> List[Dict]:
        """ä»MySQL tablesè¡¨æŸ¥è¯¢è¡¨æ ¼ä¿¡æ¯"""
        try:
            if not hasattr(self, 'mysql_client') or not self.mysql_client:
                logger.debug("MySQLå®¢æˆ·ç«¯æœªåˆå§‹åŒ–ï¼Œè·³è¿‡tablesæŸ¥è¯¢")
                return []
            
            session = self.mysql_client.get_session()
            try:
                # æŸ¥è¯¢è¯¥sectionä¸‹çš„æ‰€æœ‰è¡¨æ ¼
                tables_query = """
                SELECT elem_id, section_id, table_html, n_rows, n_cols
                FROM tables 
                WHERE section_id = :section_id
                ORDER BY elem_id
                """
                
                result = session.execute(text(tables_query), {"section_id": section_id})
                tables = []
                
                for row in result:
                    # æŸ¥è¯¢è¡¨æ ¼çš„è¯¦ç»†è¡Œæ•°æ®
                    table_rows = self._query_table_rows(session, row.elem_id)
                    
                    table_element = {
                        "element_id": row.elem_id,
                        "content_type": "table",
                        "content": f"è¡¨æ ¼ {row.elem_id} ({row.n_rows}è¡ŒÃ—{row.n_cols}åˆ—)",
                        "title": f"è¡¨æ ¼ {len(tables) + 1}",
                        "order": len(tables) + 1,
                        "page_number": 1,  # å¯ä»¥ä»å…¶ä»–åœ°æ–¹è·å–
                        "bbox": {},
                        "metadata": {
                            "section_id": section_id,
                            "source": "mysql_tables",
                            "table_elem_id": row.elem_id
                        },
                        "table_details": {
                            "elem_id": row.elem_id,
                            "rows": row.n_rows,
                            "columns": row.n_cols,
                            "html": row.table_html,
                            "data": table_rows,
                            "source": "mysql"
                        }
                    }
                    tables.append(table_element)
                
                logger.info(f"ä»MySQLæŸ¥è¯¢åˆ°{len(tables)}å¼ è¡¨æ ¼")
                return tables
                
            finally:
                session.close()
                
        except Exception as e:
            logger.error(f"æŸ¥è¯¢tablesè¡¨å¤±è´¥: {str(e)}")
            return []
    
    def _query_table_rows(self, session, table_elem_id: str) -> List[Dict]:
        """æŸ¥è¯¢è¡¨æ ¼çš„è¯¦ç»†è¡Œæ•°æ®"""
        try:
            rows_query = """
            SELECT row_index, row_text, row_json
            FROM table_rows 
            WHERE table_elem_id = :table_elem_id
            ORDER BY row_index
            """
            
            result = session.execute(text(rows_query), {"table_elem_id": table_elem_id})
            rows_data = []
            
            for row in result:
                row_data = {
                    "row_index": row.row_index,
                    "row_text": row.row_text,
                    "row_json": row.row_json
                }
                rows_data.append(row_data)
            
            return rows_data
            
        except Exception as e:
            logger.error(f"æŸ¥è¯¢è¡¨æ ¼è¡Œæ•°æ®å¤±è´¥: {str(e)}")
            return []
    
    def _stream_render_answer(self, query: str, top_section: Dict, multimodal_content: List[Dict], 
                            understanding_result: Dict) -> Generator[Dict, None, None]:
        """â‘¨ ç»„è£…/æ¸²æŸ“ï¼ˆå¯æµå¼ï¼‰- åŸºäºtop_sectionçš„å®Œæ•´æ–‡æœ¬ç­”æ¡ˆå’Œå¤šæ¨¡æ€å†…å®¹"""
        try:
            # ä»top_sectionè·å–å®Œæ•´çš„æ–‡æœ¬ç­”æ¡ˆ
            evidence_elements = top_section.get("evidence_elements", [])
            evidence_highlights = top_section.get("evidence_highlights", [])
            section_title = top_section.get("title", "ç›¸å…³ç« èŠ‚")
            
            # é¦–å±è¾“å‡ºï¼šç« èŠ‚æ ‡é¢˜
            yield {
                "type": "answer_chunk",
                "content": f"## {section_title}\n\n"
            }
            
            # ğŸ”§ æµå¼è¾“å‡ºæ–‡æœ¬ç­”æ¡ˆï¼ˆåŸºäºevidence_elementså’Œevidence_highlightsï¼‰
            if evidence_elements:
                # è¾“å‡ºæœ€ç›¸å…³çš„è¯æ®å†…å®¹ï¼ˆå·²ç»æ˜¯Top-1ï¼‰
                for evidence in evidence_elements:
                    content = evidence.get("content", "")
                    if content:
                        # åº”ç”¨é«˜äº®æ ‡è®°
                        highlighted_content = self._apply_evidence_highlighting_to_content(
                            content, evidence_highlights, evidence.get("element_id", "")
                        )
                        
                        yield {
                            "type": "answer_chunk",
                            "content": highlighted_content + "\n\n"
                        }
                        sleep(0.1)  # æµå¼æ•ˆæœ
            
            # ğŸ”§ æ·±åº¦åˆ†æå¹¶è¾“å‡ºå¤šæ¨¡æ€å†…å®¹
            if multimodal_content:
                # æŒ‰ç±»å‹åˆ†ç»„å¤šæ¨¡æ€å†…å®¹
                images = [item for item in multimodal_content if item.get("content_type") == "image"]
                tables = [item for item in multimodal_content if item.get("content_type") == "table"]
                charts = [item for item in multimodal_content if item.get("content_type") == "chart"]
                
                # æµå¼è¾“å‡ºå›¾ç‰‡
                for image in images:
                    yield {
                        "type": "multimodal_content",
                        "content_type": "image",
                        "data": self._format_image_for_stream(image)
                    }
                    sleep(0.2)  # å›¾ç‰‡åŠ è½½é—´éš”
                
                # æµå¼è¾“å‡ºè¡¨æ ¼
                for table in tables:
                    yield {
                        "type": "multimodal_content", 
                        "content_type": "table",
                        "data": self._format_table_for_stream(table)
                    }
                    sleep(0.2)  # è¡¨æ ¼æ¸²æŸ“é—´éš”
                
                # æµå¼è¾“å‡ºå›¾è¡¨
                for chart in charts:
                    yield {
                        "type": "multimodal_content",
                        "content_type": "chart", 
                        "data": self._format_chart_for_stream(chart)
                    }
                    sleep(0.2)  # å›¾è¡¨æ¸²æŸ“é—´éš”
            
            # ç”Ÿæˆå‚è€ƒæ¥æº
            references = self._build_references_from_section(top_section, multimodal_content)
            if references:
                yield {
                    "type": "answer_chunk",
                    "content": f"\n**å‚è€ƒæ¥æºï¼š**\n{references}\n"
                }
            
            # ç”Ÿæˆæœ€ç»ˆå®Œæ•´ç­”æ¡ˆ
            final_answer = {
                "query": query,
                "intent_type": understanding_result.get("intent_type", ""),
                "selected_section": {
                    "section_id": top_section.get("section_id"),
                    "score": top_section.get("final_score", 0),
                    "title": section_title
                },
                "evidence_highlights": evidence_highlights,
                "evidence_count": len(evidence_elements),
                "multimodal_summary": {
                    "images": len([item for item in multimodal_content if item.get("content_type") == "image"]),
                    "tables": len([item for item in multimodal_content if item.get("content_type") == "table"]), 
                    "charts": len([item for item in multimodal_content if item.get("content_type") == "chart"])
                },
                "generation_time": datetime.now().isoformat()
            }
            
            yield {
                "type": "final_answer",
                "content": final_answer,
                "metadata": {
                    "generation_method": "evidence_based_rendering",
                    "has_multimodal": len(multimodal_content) > 0,
                    "text_source": "evidence_elements",
                    "multimodal_source": "mysql_enrichment"
                }
            }
            
        except Exception as e:
            logger.error(f"æµå¼æ¸²æŸ“å¤±è´¥: {str(e)}")
            yield {"type": "error", "message": f"ç­”æ¡ˆç”Ÿæˆå¤±è´¥: {str(e)}"}
    
    def _get_section_title(self, content: List[Dict]) -> str:
        """ä»å†…å®¹ä¸­æå–sectionæ ‡é¢˜"""
        for element in content:
            if element.get("content_type") == "title":
                return element.get("content", "æœªçŸ¥ç« èŠ‚")
        
        if content:
            return content[0].get("title", "æœªçŸ¥ç« èŠ‚")
        
        return "æœªçŸ¥ç« èŠ‚"
    
    def _apply_evidence_highlighting_to_content(self, content: str, evidence_highlights: List[Dict], element_id: str) -> str:
        """å¯¹æ–‡æœ¬å†…å®¹è¿›è¡Œé«˜äº®æ ‡è®°"""
        if not content:
            return ""
        
        # æ£€æŸ¥å½“å‰å…ƒç´ æ˜¯å¦åœ¨é«˜äº®è¯æ®ä¸­
        is_highlighted = any(ev.get("element_id") == element_id for ev in evidence_highlights)
        
        if is_highlighted:
            return f"<mark style='padding: 2px 4px; border-radius: 3px;'>{content}</mark>"
        
        return content
    
    def _apply_evidence_highlighting(self, element: Dict, evidence_highlights: List[Dict]) -> str:
        """å¯¹è¯æ®è¿›è¡Œé«˜äº®æ ‡è®°"""
        content = element.get("content", "")
        element_id = element.get("element_id", "")
        
        return self._apply_evidence_highlighting_to_content(content, evidence_highlights, element_id)
    
    def _format_table_for_stream(self, table_element: Dict) -> Dict:
        """æ ¼å¼åŒ–è¡¨æ ¼ç”¨äºæµå¼è¾“å‡º"""
        table_details = table_element.get("table_details", {})
        metadata = table_element.get("metadata", {})
        
        return {
            "element_id": table_element.get("element_id", ""),
            "title": table_element.get("title", "æ•°æ®è¡¨"),
            "description": table_element.get("content", ""),
            "html_content": table_details.get("html", ""),
            "structured_data": table_details.get("data", []),
            "headers": table_details.get("headers", []),
            "rows": table_details.get("rows", 0),
            "columns": table_details.get("columns", 0),
            "page_number": table_element.get("page_number", 1),
            "bbox": table_element.get("bbox", {})
        }
    
    def _format_image_for_stream(self, image_element: Dict) -> Dict:
        """æ ¼å¼åŒ–å›¾ç‰‡ç”¨äºæµå¼è¾“å‡º"""
        image_details = image_element.get("image_details", {})
        metadata = image_element.get("metadata", {})
        
        # æ„å»ºå›¾ç‰‡URL
        image_path = image_details.get("image_path", "")
        image_url = ""
        if image_path:
            if image_path.startswith('http'):
                image_url = image_path
            elif image_path.startswith('/'):
                image_url = image_path
            elif image_path.startswith('figures/'):
                # å¦‚æœè·¯å¾„å·²ç»ä»¥figures/å¼€å¤´ï¼Œç›´æ¥ä½¿ç”¨
                image_url = f"/static/uploads/{image_path}"
            else:
                # å…¶ä»–æƒ…å†µï¼Œæ·»åŠ å®Œæ•´å‰ç¼€
                image_url = f"/static/uploads/{image_path}"
        
        return {
            "element_id": image_element.get("element_id", ""),
            "title": image_element.get("title", "å›¾ç‰‡"),
            "description": image_element.get("content", ""),
            "caption": image_details.get("caption", ""),
            "alt_text": image_details.get("alt_text", image_element.get("content", "")),
            "image_path": image_path,
            "image_url": image_url,
            "url": image_url,  # å…¼å®¹å­—æ®µ
            "width": image_details.get("width", 0),
            "height": image_details.get("height", 0),
            "format": image_details.get("format", ""),
            "page_number": image_element.get("page_number", 1),
            "bbox": image_element.get("bbox", {})
        }
    
    def _format_chart_for_stream(self, chart_element: Dict) -> Dict:
        """æ ¼å¼åŒ–å›¾è¡¨ç”¨äºæµå¼è¾“å‡º"""
        chart_details = chart_element.get("chart_details", {})
        metadata = chart_element.get("metadata", {})
        
        return {
            "element_id": chart_element.get("element_id", ""),
            "title": chart_element.get("title", "å›¾è¡¨"),
            "description": chart_element.get("content", ""),
            "chart_type": chart_details.get("chart_type", ""),
            "data_source": chart_details.get("data_source", ""),
            "page_number": chart_element.get("page_number", 1),
            "bbox": chart_element.get("bbox", {})
        }
    
    def _build_references_from_content(self, content: List[Dict], evidence_highlights: List[Dict]) -> str:
        """ä»å†…å®¹æ„å»ºå‚è€ƒæ¥æº"""
        references = []
        doc_info = {}
        
        # æ”¶é›†æ–‡æ¡£ä¿¡æ¯
        for element in content:
            metadata = element.get("metadata", {})
            doc_id = metadata.get("doc_id", "")
            section_id = metadata.get("section_id", "")
            
            if doc_id and doc_id not in doc_info:
                doc_info[doc_id] = {
                    "section_id": section_id,
                    "title": element.get("title", ""),
                    "doc_name": self._get_document_name_by_id(doc_id),
                    "page_numbers": set()
                }
            
            if doc_id and element.get("page_number"):
                doc_info[doc_id]["page_numbers"].add(element.get("page_number"))
        
        # ç”Ÿæˆå¼•ç”¨æ ¼å¼
        for i, (doc_id, info) in enumerate(doc_info.items(), 1):
            pages = sorted(list(info["page_numbers"]))
            page_text = f"ç¬¬{', '.join(map(str, pages))}é¡µ" if pages else ""
            
            # æ„å»ºå¼•ç”¨æ ¼å¼ï¼š[åºå·] æ–‡æ¡£å - ç« èŠ‚æ ‡é¢˜ (é¡µç ä¿¡æ¯)
            doc_name = info.get('doc_name', 'æœªçŸ¥æ–‡æ¡£')
            title = info.get('title', '')
            
            if title and title != doc_name:
                ref = f"[{i}] {doc_name} - {title} ({page_text})"
            else:
                ref = f"[{i}] {doc_name} ({page_text})"
            
            references.append(ref)
        
        return "\n".join(references)
    
    def _format_image_for_frontend(self, image_element: Dict) -> Dict:
        """æ ¼å¼åŒ–å›¾ç‰‡æ•°æ®ä¾›å‰ç«¯æ¸²æŸ“"""
        image_details = image_element.get("image_details", {})
        metadata = image_element.get("metadata", {})
        
        return {
            "element_id": image_element.get("element_id", ""),
            "title": image_element.get("title", "å›¾ç‰‡"),
            "description": image_element.get("content", ""),
            "caption": image_details.get("caption", ""),
            "alt_text": image_details.get("alt_text", image_element.get("content", "")),
            "image_path": image_details.get("image_path", ""),
            "page_number": image_element.get("page_number", 1),
            "bbox": image_element.get("bbox", {}),
            "doc_id": metadata.get("doc_id", ""),
            "section_id": metadata.get("section_id", ""),
            # å‰ç«¯æ¸²æŸ“æ‰€éœ€çš„URLå’Œæ ·å¼ä¿¡æ¯
            "display_url": self._build_image_display_url(image_details, metadata),
            "thumbnail_url": self._build_image_thumbnail_url(image_details, metadata),
            "view_original_url": f"/api/file/view/{metadata.get('doc_id')}?page={image_element.get('page_number')}&highlight=image",
            "render_config": {
                "max_width": "100%",
                "max_height": "400px", 
                "border_radius": "8px",
                "box_shadow": "0 2px 8px rgba(0,0,0,0.1)"
            }
        }
    
    def _format_table_for_frontend(self, table_element: Dict) -> Dict:
        """æ ¼å¼åŒ–è¡¨æ ¼æ•°æ®ä¾›å‰ç«¯æ¸²æŸ“"""
        table_details = table_element.get("table_details", {})
        metadata = table_element.get("metadata", {})
        
        # æ·±åº¦åˆ†æè¡¨æ ¼ç»“æ„
        table_data = table_details.get("data", [])
        table_html = table_details.get("html", "")
        
        # æ„å»ºå‰ç«¯å¯ç›´æ¥æ¸²æŸ“çš„è¡¨æ ¼ç»“æ„
        formatted_table = {
            "element_id": table_element.get("element_id", ""),
            "title": table_element.get("title", "æ•°æ®è¡¨"),
            "description": table_element.get("content", ""),
            "rows": table_details.get("rows", 0),
            "columns": table_details.get("columns", 0),
            "page_number": table_element.get("page_number", 1),
            "bbox": table_element.get("bbox", {}),
            "doc_id": metadata.get("doc_id", ""),
            "section_id": metadata.get("section_id", ""),
            
            # å‰ç«¯æ¸²æŸ“çš„æ ¸å¿ƒæ•°æ®
            "html_content": table_html,
            "structured_data": self._parse_table_data_for_frontend(table_data),
            "headers": self._extract_table_headers(table_data, table_html),
            
            # å‰ç«¯æ¸²æŸ“é…ç½®
            "render_config": {
                "enable_sorting": True,
                "enable_search": len(table_data) > 10,
                "pagination": len(table_data) > 20,
                "page_size": 20,
                "responsive": True,
                "striped_rows": True,
                "bordered": True,
                "hover_effect": True,
                "css_classes": ["table", "table-striped", "table-bordered", "table-hover"]
            },
            
            # æ“ä½œé“¾æ¥
            "view_original_url": f"/api/file/view/{metadata.get('doc_id')}?page={table_element.get('page_number')}&highlight=table",
            "export_csv_url": f"/api/table/export/{table_element.get('element_id')}/csv",
            "export_excel_url": f"/api/table/export/{table_element.get('element_id')}/excel"
        }
        
        return formatted_table
    
    def _format_chart_for_frontend(self, chart_element: Dict) -> Dict:
        """æ ¼å¼åŒ–å›¾è¡¨æ•°æ®ä¾›å‰ç«¯æ¸²æŸ“"""
        chart_details = chart_element.get("chart_details", {})
        metadata = chart_element.get("metadata", {})
        
        return {
            "element_id": chart_element.get("element_id", ""),
            "title": chart_element.get("title", "å›¾è¡¨"),
            "description": chart_element.get("content", ""),
            "chart_type": chart_details.get("chart_type", "unknown"),
            "page_number": chart_element.get("page_number", 1),
            "bbox": chart_element.get("bbox", {}),
            "doc_id": metadata.get("doc_id", ""),
            "section_id": metadata.get("section_id", ""),
            
            # å›¾è¡¨æ•°æ®å’Œé…ç½®
            "chart_data": chart_details.get("data", {}),
            "chart_config": chart_details.get("config", {}),
            "image_url": chart_details.get("image_path", ""),
            
            # å‰ç«¯æ¸²æŸ“é…ç½®
            "render_config": {
                "width": "100%",
                "height": "300px",
                "responsive": True,
                "interactive": True,
                "theme": "light"
            },
            
            # æ“ä½œé“¾æ¥
            "view_original_url": f"/api/file/view/{metadata.get('doc_id')}?page={chart_element.get('page_number')}&highlight=chart",
            "download_image_url": f"/api/chart/download/{chart_element.get('element_id')}/png"
        }
    
    def _get_document_name_by_id(self, doc_id: str) -> str:
        """æ ¹æ®doc_idè·å–æ–‡æ¡£åç§°"""
        try:
            logger.info(f"ğŸ” è·å–æ–‡æ¡£åç§° - è¾“å…¥doc_id: {repr(doc_id)} (ç±»å‹: {type(doc_id)})")
            
            if not doc_id:
                logger.warning("âŒ doc_idä¸ºç©ºï¼Œè¿”å›é»˜è®¤å€¼")
                return "æœªçŸ¥æ–‡æ¡£"
                
            # å°è¯•è½¬æ¢ä¸ºæ•´æ•°ID
            try:
                doc_id_int = int(doc_id)
                logger.info(f"âœ… doc_idè½¬æ¢ä¸ºæ•´æ•°: {doc_id_int}")
            except (ValueError, TypeError):
                # å¦‚æœä¸æ˜¯æ•°å­—ï¼Œå¯èƒ½æ˜¯å­—ç¬¦ä¸²IDï¼Œç›´æ¥ä½¿ç”¨
                doc_id_int = doc_id
                logger.info(f"âš ï¸ doc_idä¿æŒä¸ºå­—ç¬¦ä¸²: {doc_id_int}")
            
            # æŸ¥è¯¢æ•°æ®åº“è·å–æ–‡æ¡£åç§°
            query = "SELECT filename FROM documents WHERE id = :doc_id"
            logger.info(f"ğŸ” æ‰§è¡ŒæŸ¥è¯¢: {query} (å‚æ•°: {doc_id_int})")
            
            result = self.mysql_client.execute_query(query, {'doc_id': doc_id_int})
            logger.info(f"ğŸ“Š æŸ¥è¯¢ç»“æœ: {result}")
            
            if result and len(result) > 0:
                filename = result[0].get('filename', '')
                logger.info(f"ğŸ“ è·å–åˆ°filename: {filename}")
                
                if filename:
                    # å»æ‰æ–‡ä»¶æ‰©å±•åï¼Œåªä¿ç•™æ–‡æ¡£å
                    import os
                    doc_name = os.path.splitext(filename)[0]
                    logger.info(f"âœ… å¤„ç†åçš„æ–‡æ¡£å: {doc_name}")
                    return doc_name
            
            fallback_name = f"æ–‡æ¡£{doc_id}"
            logger.warning(f"âš ï¸ æœªæ‰¾åˆ°æ–‡æ¡£ï¼Œè¿”å›é»˜è®¤åç§°: {fallback_name}")
            return fallback_name
            
        except Exception as e:
            error_msg = f"è·å–æ–‡æ¡£åç§°å¤±è´¥ (doc_id: {doc_id}): {str(e)}"
            logger.error(error_msg)
            return f"æ–‡æ¡£{doc_id}"
    
    def _build_references_from_section(self, top_section: Dict, multimodal_content: List[Dict]) -> str:
        """ä»sectionå’Œå¤šæ¨¡æ€å†…å®¹æ„å»ºå‚è€ƒæ¥æº"""
        references = []
        doc_info = {}
        
        # ä»top_sectionæ”¶é›†ä¿¡æ¯
        section_doc_id = top_section.get("doc_id", "")
        section_title = top_section.get("title", "")
        
        if section_doc_id:
            doc_info[section_doc_id] = {
                "title": section_title,
                "doc_name": self._get_document_name_by_id(section_doc_id),
                "page_numbers": set(),
                "elements": []
            }
            
            # ä»evidence_elementsæ”¶é›†é¡µç 
            for evidence in top_section.get("evidence_elements", []):
                if evidence.get("page_number"):
                    doc_info[section_doc_id]["page_numbers"].add(evidence.get("page_number"))
        
        # ä»å¤šæ¨¡æ€å†…å®¹æ”¶é›†ä¿¡æ¯
        for item in multimodal_content:
            metadata = item.get("metadata", {})
            doc_id = metadata.get("doc_id", "")
            
            if doc_id and doc_id not in doc_info:
                doc_info[doc_id] = {
                    "title": item.get("title", ""),
                    "doc_name": self._get_document_name_by_id(doc_id),
                    "page_numbers": set(),
                    "elements": []
                }
            
            if doc_id and item.get("page_number"):
                doc_info[doc_id]["page_numbers"].add(item.get("page_number"))
                doc_info[doc_id]["elements"].append({
                    "type": item.get("content_type", ""),
                    "title": item.get("title", "")
                })
        
        # ç”Ÿæˆå¼•ç”¨æ ¼å¼
        for i, (doc_id, info) in enumerate(doc_info.items(), 1):
            pages = sorted(list(info["page_numbers"]))
            page_text = f"ç¬¬{', '.join(map(str, pages))}é¡µ" if pages else ""
            
            elements_text = ""
            if info["elements"]:
                element_types = {}
                for elem in info["elements"]:
                    elem_type = elem["type"]
                    if elem_type not in element_types:
                        element_types[elem_type] = 0
                    element_types[elem_type] += 1
                
                type_texts = []
                for elem_type, count in element_types.items():
                    type_name = {"image": "å›¾ç‰‡", "table": "è¡¨æ ¼", "chart": "å›¾è¡¨"}.get(elem_type, elem_type)
                    type_texts.append(f"{count}ä¸ª{type_name}")
                
                if type_texts:
                    elements_text = f" (åŒ…å«{', '.join(type_texts)})"
            
            # æ„å»ºå¼•ç”¨æ ¼å¼ï¼š[åºå·] æ–‡æ¡£å - ç« èŠ‚æ ‡é¢˜ é¡µç ä¿¡æ¯ (å¤šæ¨¡æ€å†…å®¹)
            doc_name = info.get('doc_name', 'æœªçŸ¥æ–‡æ¡£')
            title = info.get('title', '')
            
            if title and title != doc_name:
                ref = f"[{i}] {doc_name} - {title} {page_text}{elements_text}"
            else:
                ref = f"[{i}] {doc_name} {page_text}{elements_text}"
            
            references.append(ref)
        
        return "\n".join(references)
    
    def _build_image_display_url(self, image_details: Dict, metadata: Dict) -> str:
        """æ„å»ºå›¾ç‰‡æ˜¾ç¤ºURL"""
        image_path = image_details.get("image_path", "")
        if image_path:
            # å¦‚æœæœ‰ç›´æ¥çš„å›¾ç‰‡è·¯å¾„ï¼Œä½¿ç”¨é™æ€æ–‡ä»¶æœåŠ¡
            return f"/api/static/images/{image_path}"
        else:
            # å¦åˆ™ä½¿ç”¨PDFé¡µé¢æˆªå›¾
            doc_id = metadata.get("doc_id", "")
            page_no = image_details.get("page", 1)
            return f"/api/file/view/{doc_id}?page={page_no}&format=image"
    
    def _build_image_thumbnail_url(self, image_details: Dict, metadata: Dict) -> str:
        """æ„å»ºå›¾ç‰‡ç¼©ç•¥å›¾URL"""
        display_url = self._build_image_display_url(image_details, metadata)
        return f"{display_url}&thumbnail=true&size=200x150"
    
    def _parse_table_data_for_frontend(self, table_data: List[Dict]) -> List[List[str]]:
        """è§£æè¡¨æ ¼æ•°æ®ä¸ºå‰ç«¯å¯æ¸²æŸ“çš„äºŒç»´æ•°ç»„"""
        if not table_data:
            return []
        
        parsed_data = []
        for row in table_data:
            if isinstance(row, dict):
                # å¦‚æœæ˜¯å­—å…¸æ ¼å¼ï¼Œæå–row_textæˆ–row_json
                row_text = row.get("row_text", "")
                if row_text:
                    # ç®€å•åˆ†å‰²ï¼Œå®é™…å¯èƒ½éœ€è¦æ›´å¤æ‚çš„è§£æ
                    cells = [cell.strip() for cell in row_text.split("|") if cell.strip()]
                    parsed_data.append(cells)
            elif isinstance(row, list):
                # å¦‚æœå·²ç»æ˜¯åˆ—è¡¨æ ¼å¼
                parsed_data.append([str(cell) for cell in row])
            elif isinstance(row, str):
                # å¦‚æœæ˜¯å­—ç¬¦ä¸²ï¼Œå°è¯•åˆ†å‰²
                cells = [cell.strip() for cell in row.split("|") if cell.strip()]
                parsed_data.append(cells)
        
        return parsed_data
    
    def _extract_table_headers(self, table_data: List[Dict], table_html: str) -> List[str]:
        """æå–è¡¨æ ¼æ ‡é¢˜è¡Œ"""
        if table_data and len(table_data) > 0:
            first_row = table_data[0]
            if isinstance(first_row, dict):
                row_text = first_row.get("row_text", "")
                if row_text:
                    return [cell.strip() for cell in row_text.split("|") if cell.strip()]
            elif isinstance(first_row, list):
                return [str(cell) for cell in first_row]
        
        # å¦‚æœæ— æ³•ä»æ•°æ®ä¸­æå–ï¼Œå°è¯•ä»HTMLä¸­æå–
        if table_html:
            # ç®€å•çš„HTMLè§£æï¼Œå®é™…å¯èƒ½éœ€è¦æ›´å¤æ‚çš„å¤„ç†
            import re
            th_pattern = r'<th[^>]*>(.*?)</th>'
            headers = re.findall(th_pattern, table_html, re.IGNORECASE | re.DOTALL)
            if headers:
                return [re.sub(r'<[^>]+>', '', header).strip() for header in headers]
        
        return []
