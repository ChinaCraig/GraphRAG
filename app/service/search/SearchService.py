"""
æ™ºèƒ½æ£€ç´¢æœåŠ¡
ä¸¥æ ¼æŒ‰ç…§é‡æ„è¦æ±‚å®ç°å®Œæ•´çš„æ£€ç´¢æµç¨‹ï¼š
â‘  è§„èŒƒåŒ– â†’ â‘¡ æ„å›¾åˆ¤åˆ« â†’ â‘¢ å€™é€‰å¬å› â†’ â‘£ èšåˆèåˆ â†’ â‘¤ é‡æ’ â†’ â‘¦ æ‰©å±• â†’ â‘§ è¡¥å›¾è¡¨ â†’ â‘¨ æµå¼æ¸²æŸ“
"""

import json
import logging
import re
import yaml
import numpy as np
import os
from typing import Dict, List, Optional, Generator, Any
from datetime import datetime
from collections import defaultdict
import requests
from time import sleep

# é…ç½®æ—¥å¿—
logger = logging.getLogger(__name__)


class SearchService:
    """æ™ºèƒ½æ£€ç´¢æœåŠ¡ç±» - å®Œæ•´å®ç°"""
    
    def __init__(self):
        """åˆå§‹åŒ–æœç´¢æœåŠ¡"""
        self._load_config()
        self._init_clients()
        self._init_models()
        self._init_patterns()
        
    def _load_config(self):
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        try:
            with open('config/model.yaml', 'r', encoding='utf-8') as f:
                self.model_config = yaml.safe_load(f)
            with open('config/db.yaml', 'r', encoding='utf-8') as f:
                self.db_config = yaml.safe_load(f)
            with open('config/prompt.yaml', 'r', encoding='utf-8') as f:
                self.prompt_config = yaml.safe_load(f)
            logger.info("é…ç½®æ–‡ä»¶åŠ è½½æˆåŠŸ")
        except Exception as e:
            logger.error(f"åŠ è½½é…ç½®æ–‡ä»¶å¤±è´¥: {str(e)}")
            self.model_config = {}
            self.db_config = {}
            self.prompt_config = {}
    
    def _init_clients(self):
        """åˆå§‹åŒ–å®¢æˆ·ç«¯"""
        try:
            # OpenSearchå®¢æˆ·ç«¯
            self._init_opensearch_client()
            
            # Milvuså‘é‡å®¢æˆ·ç«¯
            self._init_milvus_client()
            
            # Neo4jå›¾æ•°æ®åº“å®¢æˆ·ç«¯
            self._init_neo4j_client()
            
            # LLMå®¢æˆ·ç«¯
            self._init_llm_client()
            
            logger.info("å®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            logger.error(f"å®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {str(e)}")
    
    def _init_opensearch_client(self):
        """åˆå§‹åŒ–OpenSearchå®¢æˆ·ç«¯"""
        try:
            from utils.OpenSearchManager import OpenSearchManager
            opensearch_config = self.db_config.get('opensearch', {})
            if opensearch_config:
                self.opensearch_client = OpenSearchManager('config/db.yaml')
                self.index_name = opensearch_config.get('index_name', 'graphrag_documents')
                logger.info("OpenSearchå®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ")
            else:
                self.opensearch_client = None
                logger.warning("OpenSearché…ç½®æœªæ‰¾åˆ°")
        except Exception as e:
            logger.error(f"OpenSearchå®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {str(e)}")
            self.opensearch_client = None
    
    def _init_milvus_client(self):
        """åˆå§‹åŒ–Milvuså®¢æˆ·ç«¯"""
        try:
            from utils.MilvusManager import MilvusManager
            milvus_config = self.db_config.get('milvus', {})
            if milvus_config:
                self.milvus_client = MilvusManager()
                logger.info("Milvuså®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ")
            else:
                self.milvus_client = None
                logger.warning("Milvusé…ç½®æœªæ‰¾åˆ°")
        except Exception as e:
            logger.error(f"Milvuså®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {str(e)}")
            self.milvus_client = None
    
    def _init_neo4j_client(self):
        """åˆå§‹åŒ–Neo4jå®¢æˆ·ç«¯"""
        try:
            from neo4j import GraphDatabase
            neo4j_config = self.db_config.get('neo4j', {})
            if neo4j_config:
                self.neo4j_client = GraphDatabase.driver(
                    neo4j_config.get('uri', 'bolt://localhost:7687'),
                    auth=(
                        neo4j_config.get('username', 'neo4j'),
                        neo4j_config.get('password', 'password')
                    )
                )
                logger.info("Neo4jå®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ")
            else:
                self.neo4j_client = None
                logger.warning("Neo4jé…ç½®æœªæ‰¾åˆ°")
        except Exception as e:
            logger.error(f"Neo4jå®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {str(e)}")
            self.neo4j_client = None
    
    def _init_llm_client(self):
        """åˆå§‹åŒ–LLMå®¢æˆ·ç«¯"""
        try:
            deepseek_config = self.model_config.get('deepseek', {})
            self.llm_config = {
                'api_url': deepseek_config.get('api_url', 'https://api.deepseek.com'),
                'api_key': deepseek_config.get('api_key', ''),
                'model_name': deepseek_config.get('model_name', 'deepseek-chat'),
                'max_tokens': deepseek_config.get('max_tokens', 4096),
                'temperature': deepseek_config.get('temperature', 0.7)
            }
            if self.llm_config['api_key']:
                logger.info("LLMå®¢æˆ·ç«¯é…ç½®æˆåŠŸ")
            else:
                logger.warning("LLM APIå¯†é’¥æœªé…ç½®")
        except Exception as e:
            logger.error(f"LLMå®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {str(e)}")
            self.llm_config = {}
    
    def _init_models(self):
        """åˆå§‹åŒ–æ¨¡å‹"""
        try:
            # åµŒå…¥æ¨¡å‹
            from sentence_transformers import SentenceTransformer
            embedding_config = self.model_config.get('embedding', {})
            if embedding_config:
                model_name = embedding_config.get('model_name')
                cache_dir = embedding_config.get('cache_dir')
                
                os.environ['HF_HOME'] = os.path.abspath(cache_dir)
                os.environ['TRANSFORMERS_CACHE'] = os.path.abspath(cache_dir)
                os.environ['SENTENCE_TRANSFORMERS_HOME'] = os.path.abspath(cache_dir)
                
                self.embedding_model = SentenceTransformer(model_name, cache_folder=cache_dir)
                self.normalize = embedding_config.get('normalize', True)
                logger.info(f"åµŒå…¥æ¨¡å‹åˆå§‹åŒ–æˆåŠŸ: {model_name}")
            else:
                self.embedding_model = None
            
            # é‡æ’æ¨¡å‹
            reranker_config = self.model_config.get('reranker', {})
            if reranker_config.get('enabled', False):
                from sentence_transformers import CrossEncoder
                model_name = reranker_config.get('model_name', 'BAAI/bge-reranker-large')
                device = reranker_config.get('device', 'cpu')
                
                self.reranker = CrossEncoder(model_name, device=device)
                self.reranker_config = reranker_config
                logger.info(f"é‡æ’æ¨¡å‹åˆå§‹åŒ–æˆåŠŸ: {model_name}")
            else:
                self.reranker = None
                self.reranker_config = {}
                
        except Exception as e:
            logger.error(f"æ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {str(e)}")
            self.embedding_model = None
            self.reranker = None
    
    def _init_patterns(self):
        """åˆå§‹åŒ–æ¨¡å¼å’Œè¯å…¸"""
        # å®ä½“è¯†åˆ«æ¨¡å¼
        self.entity_patterns = {
            "bio_entity": [
                r"HCP|å®¿ä¸»ç»†èƒè›‹ç™½",
                r"CHO|ä¸­å›½ä»“é¼ åµå·¢", 
                r"ç»†èƒæ ª|cell\s*line",
                r"åŸ¹å…»åŸº|medium",
                r"æŠ—ä½“|antibody",
                r"è›‹ç™½è´¨|protein"
            ],
            "product_model": [
                r"[A-Z]{2,5}\d{2,6}",
                r"[A-Z]+\-\d+",
                r"v\d+\.\d+"
            ]
        }
        
        # åŒä¹‰è¯è¯å…¸
        self.synonym_dict = {
            "HCP": ["å®¿ä¸»ç»†èƒè›‹ç™½", "Host Cell Protein", "host cell protein"],
            "CHO": ["ä¸­å›½ä»“é¼ åµå·¢", "Chinese Hamster Ovary", "ä¸­å›½ä»“é¼ åµå·¢ç»†èƒ"],
            "CHO-K1": "CHOK1",
            "CHO K1": "CHOK1"
        }
    
    def intelligent_search(self, query: str, filters: Dict = None) -> Generator[Dict, None, None]:
        """
        æ™ºèƒ½æ£€ç´¢ä¸»æµç¨‹
        ä¸¥æ ¼æŒ‰ç…§æ–‡æ¡£è¦æ±‚ï¼šâ‘  â†’ â‘¡ â†’ â‘¢ â†’ â‘£ â†’ â‘¤ â†’ â‘¦ â†’ â‘§ â†’ â‘¨
        """
        try:
            logger.info(f"å¼€å§‹æ™ºèƒ½æ£€ç´¢: {query}")
            
            # â‘  è§„èŒƒåŒ–ï¼ˆQuery Normalizationï¼‰
            yield {"type": "stage_update", "stage": "normalization", "message": "ğŸ”§ æ­£åœ¨è§„èŒƒåŒ–æŸ¥è¯¢...", "progress": 10}
            normalized_query = self._normalize_query(query)
            
            # â‘¡ æ„å›¾åˆ¤åˆ«ï¼ˆæ ‡é¢˜é—®æ³• or ç¢å¥é—®æ³•ï¼‰
            yield {"type": "stage_update", "stage": "intent", "message": "ğŸ¯ æ­£åœ¨åˆ¤åˆ«æŸ¥è¯¢æ„å›¾...", "progress": 20}
            intent_type = self._classify_intent(normalized_query)
            
            # ç”Ÿæˆæ£€ç´¢é…ç½®
            retrieval_config = self._configure_retrieval(normalized_query, intent_type)
            understanding_result = {
                "original_query": query,
                "normalized_query": normalized_query,
                "intent_type": intent_type,
                "retrieval_config": retrieval_config,
                "entities": self._extract_entities(normalized_query),
                "rewrite_result": self._rewrite_and_expand(normalized_query, intent_type)
            }
            
            # â‘¢ å€™é€‰å¬å›ï¼ˆå¿«è€Œå¹¿ï¼‰
            yield {"type": "stage_update", "stage": "retrieval", "message": "ğŸ“š æ­£åœ¨å¬å›å€™é€‰å†…å®¹...", "progress": 40}
            bm25_results = self._bm25_retrieval(understanding_result, filters)
            vector_results = self._vector_retrieval(understanding_result, filters)
            graph_results = self._graph_retrieval(understanding_result, filters)
            
            # â‘£ èšåˆä¸åˆ†æ•°èåˆï¼ˆåˆ° section ç²’åº¦ï¼‰
            yield {"type": "stage_update", "stage": "aggregation", "message": "ğŸ”— æ­£åœ¨èšåˆå’Œèåˆç»“æœ...", "progress": 55}
            section_candidates = self._aggregate_by_section(bm25_results, vector_results, graph_results, understanding_result)
            
            # â‘¤ é‡æ’ï¼ˆæŠŠ"æœ€ç›¸å…³çš„ä¸€èŠ‚"æ”¾åˆ°ç¬¬ä¸€ï¼‰
            yield {"type": "stage_update", "stage": "reranking", "message": "ğŸ¯ æ­£åœ¨é‡æ’é€‰æ‹©æœ€ä½³ç« èŠ‚...", "progress": 70}
            top_section = self._rerank_sections(section_candidates, understanding_result)
            
            if not top_section:
                yield {"type": "error", "message": "æœªæ‰¾åˆ°ç›¸å…³å†…å®¹"}
                return
            
            # â‘¦ æ‰©å±•ï¼ˆæŠŠ"ä¸€å®¶å­"æ‹‰é½ï¼‰
            yield {"type": "stage_update", "stage": "expansion", "message": "ğŸ” æ­£åœ¨æ‰©å±•ç« èŠ‚å†…å®¹...", "progress": 80}
            expanded_content = self._expand_section_content(top_section)
            
            # â‘§ å›¾è¡¨ç»†èŠ‚ï¼ˆMySQLï¼‰
            yield {"type": "stage_update", "stage": "enrichment", "message": "ğŸ–¼ï¸ æ­£åœ¨è¡¥å……å›¾è¡¨ç»†èŠ‚...", "progress": 85}
            enriched_content = self._enrich_multimodal_details(expanded_content)
            
            # â‘¨ ç»„è£…/æ¸²æŸ“ï¼ˆå¯æµå¼ï¼‰
            yield {"type": "stage_update", "stage": "rendering", "message": "âœï¸ æ­£åœ¨ç”Ÿæˆç­”æ¡ˆ...", "progress": 90}
            
            # æµå¼è¾“å‡ºç»“æœ
            yield from self._stream_render_answer(query, top_section, enriched_content, understanding_result)
            
        except Exception as e:
            logger.error(f"æ™ºèƒ½æ£€ç´¢å¤±è´¥: {str(e)}")
            yield {"type": "error", "message": f"æ£€ç´¢å¤±è´¥: {str(e)}"}
    
    def _normalize_query(self, query: str) -> str:
        """â‘  è§„èŒƒåŒ–ï¼ˆQuery Normalizationï¼‰"""
        try:
            normalized = query.strip()
            
            # å…¨è§’/åŠè§’æ ‡å‡†åŒ–
            import unicodedata
            normalized = unicodedata.normalize('NFKC', normalized)
            
            # ç©ºç™½ä¸æ ‡ç‚¹æ ‡å‡†åŒ–
            normalized = re.sub(r'\s+', ' ', normalized)
            normalized = normalized.replace('ï¼Œ', ',').replace('ã€‚', '.').replace('ï¼›', ';')
            
            # ä¸­è‹±æ–‡ä¹‹é—´åŠ ç©ºæ ¼
            normalized = re.sub(r'([\u4e00-\u9fff])([a-zA-Z])', r'\1 \2', normalized)
            normalized = re.sub(r'([a-zA-Z])([\u4e00-\u9fff])', r'\1 \2', normalized)
            
            # åŒä¹‰è¯æ ‡å‡†åŒ–
            for synonym, standard in self.synonym_dict.items():
                if isinstance(standard, str):
                    normalized = normalized.replace(synonym, standard)
                else:
                    for syn in standard:
                        normalized = normalized.replace(syn, synonym)
            
            # ç§»é™¤ä½ä¿¡æ¯è¯
            low_info_words = ["å¸®æˆ‘", "è¯·", "æŸ¥è¯¢", "æŸ¥æ‰¾", "æœç´¢", "ä¸€ä¸‹", "ç›¸å…³", "å†…å®¹"]
            words = normalized.split()
            filtered_words = [word for word in words if word not in low_info_words and len(word.strip()) > 0]
            
            if filtered_words:
                normalized = " ".join(filtered_words)
            
            logger.debug(f"æŸ¥è¯¢è§„èŒƒåŒ–: '{query}' -> '{normalized}'")
            return normalized
            
        except Exception as e:
            logger.error(f"æŸ¥è¯¢è§„èŒƒåŒ–å¤±è´¥: {str(e)}")
            return query
    
    def _classify_intent(self, query: str) -> str:
        """â‘¡ æ„å›¾åˆ¤åˆ«ï¼ˆæ ‡é¢˜é—®æ³• or ç¢å¥é—®æ³•ï¼‰"""
        try:
            # è§„åˆ™1ï¼šé•¿åº¦â‰¤8å­—ä¸”åŒ…å«ç‰¹å®šå…³é”®è¯ â†’ æ ‡é¢˜é—®æ³•
            if len(query) <= 8 and any(keyword in query for keyword in 
                ["ç®€ä»‹", "è¯´æ˜", "æ˜¯ä»€ä¹ˆ", "å®šä¹‰", "äº§å“è¯´æ˜", "æ¦‚è¿°", "ä»‹ç»"]):
                return "title"
            
            # è§„åˆ™2ï¼šåŒ…å«æ˜ç¡®çš„æ ‡é¢˜æ€§æŸ¥è¯¢è¯ â†’ æ ‡é¢˜é—®æ³•
            title_indicators = ["ä»€ä¹ˆæ˜¯", "å®šä¹‰", "æ¦‚å¿µ", "ç®€ä»‹", "æ¦‚è¿°", "ä»‹ç»"]
            if any(indicator in query for indicator in title_indicators):
                return "title"
            
            # è§„åˆ™3ï¼šåŒ…å«æ˜ç¡®çš„å†…å®¹æ€§æŸ¥è¯¢è¯ â†’ ç¢å¥é—®æ³•
            content_indicators = ["å¦‚ä½•", "æ€ä¹ˆ", "æ­¥éª¤", "æµç¨‹", "æ–¹æ³•", "è¿‡ç¨‹", "å…·ä½“", "è¯¦ç»†"]
            if any(indicator in query for indicator in content_indicators):
                return "fragment"
            
            # è§„åˆ™4ï¼šå‘é‡ç›¸ä¼¼åº¦åˆ¤æ–­ï¼ˆç®€åŒ–å®ç°ï¼‰
            similarity_score = self._calculate_title_similarity(query)
            
            if similarity_score >= 0.45:
                return "title"
            elif similarity_score >= 0.40:
                return "hybrid"  # ä¸¤è·¯å¹¶è·‘
            else:
                return "fragment"
                
        except Exception as e:
            logger.error(f"æ„å›¾åˆ¤åˆ«å¤±è´¥: {str(e)}")
            return "fragment"
    
    def _calculate_title_similarity(self, query: str) -> float:
        """è®¡ç®—æŸ¥è¯¢ä¸æ ‡é¢˜æ€§å†…å®¹çš„ç›¸ä¼¼åº¦"""
        title_keywords = [
            "HCP", "CHO", "è›‹ç™½", "ç»†èƒ", "åŸ¹å…»", "æ£€æµ‹", "åˆ†æ", "è´¨é‡", "æ ‡å‡†",
            "è¯•å‰‚", "äº§å“", "è®¾å¤‡", "æ–¹æ³•", "æŠ€æœ¯", "ç³»ç»Ÿ", "å¹³å°", "æœåŠ¡"
        ]
        
        query_words = set(query.split())
        title_word_set = set(title_keywords)
        
        intersection = query_words.intersection(title_word_set)
        union = query_words.union(title_word_set)
        
        if not union:
            return 0.0
            
        similarity = len(intersection) / len(union)
        
        # æ ¹æ®æŸ¥è¯¢é•¿åº¦è°ƒæ•´ç›¸ä¼¼åº¦
        if len(query) <= 5:
            similarity += 0.1
        elif len(query) > 20:
            similarity -= 0.1
            
        return min(similarity, 1.0)
    
    def _configure_retrieval(self, query: str, intent_type: str) -> Dict:
        """â‘¢ å€™é€‰å¬å›é…ç½®"""
        if intent_type == "title":
            return {
                "vector_top_k": 20,
                "vector_target": "titles",
                "bm25_top_k": 20,
                "bm25_target": "sections",
                "strategy": "title_oriented"
            }
        elif intent_type == "fragment":
            return {
                "vector_top_k": 50,
                "vector_target": "fragments", 
                "bm25_top_k": 50,
                "bm25_target": "fragments",
                "strategy": "content_oriented"
            }
        elif intent_type == "hybrid":
            return {
                "vector_top_k": 35,
                "vector_target": "mixed",
                "bm25_top_k": 35,
                "bm25_target": "mixed",
                "strategy": "hybrid_dual_path"
            }
        else:
            return {
                "vector_top_k": 50,
                "vector_target": "fragments",
                "bm25_top_k": 50,
                "bm25_target": "fragments",
                "strategy": "default"
            }
    
    def _extract_entities(self, query: str) -> Dict[str, List[str]]:
        """å®ä½“è¯†åˆ«"""
        entities = {}
        
        for entity_type, patterns in self.entity_patterns.items():
            entities[entity_type] = []
            
            for pattern in patterns:
                matches = re.findall(pattern, query, re.IGNORECASE)
                if matches:
                    for match in matches:
                        if isinstance(match, tuple):
                            entities[entity_type].extend([m for m in match if m])
                        else:
                            entities[entity_type].append(match)
        
        # å»é‡å¹¶è¿‡æ»¤ç©ºå€¼
        for entity_type in entities:
            entities[entity_type] = list(set([e for e in entities[entity_type] if e]))
        
        if not any(entities.values()):
            entities["general"] = [query]
        
        return entities
    
    def _rewrite_and_expand(self, query: str, intent_type: str) -> Dict:
        """æ”¹å†™ä¸æ‰©å±•æŸ¥è¯¢"""
        # ç”ŸæˆBM25å‹å¥½çš„å…³é”®å­—
        keywords = re.findall(r'\w+', query)
        keywords = [w for w in keywords if len(w) > 1][:10]
        
        # ç”Ÿæˆå‘é‡æ£€ç´¢çš„è¯­ä¹‰åŒ–query
        if intent_type == "title":
            vector_query = f"{query} å®šä¹‰ æ¦‚å¿µ å«ä¹‰ ç®€ä»‹"
        elif intent_type == "fragment":
            vector_query = f"{query} è¯¦ç»† å…·ä½“ æ–¹æ³• æ“ä½œ æµç¨‹"
        else:
            vector_query = query
        
        return {
            "bm25_keywords": keywords,
            "vector_query": vector_query,
            "expanded_synonyms": self._expand_synonyms(keywords)
        }
    
    def _expand_synonyms(self, keywords: List[str]) -> List[str]:
        """æ‰©å±•åŒä¹‰è¯"""
        expanded = set(keywords)
        
        for keyword in keywords:
            if keyword in self.synonym_dict:
                synonyms = self.synonym_dict[keyword]
                if isinstance(synonyms, list):
                    expanded.update(synonyms)
                else:
                    expanded.add(synonyms)
        
        return list(expanded)
    
    def _bm25_retrieval(self, understanding_result: Dict, filters: Dict = None) -> List[Dict]:
        """â‘¢ BM25æ£€ç´¢"""
        try:
            if not self.opensearch_client:
                return self._mock_bm25_results()
            
            retrieval_config = understanding_result.get("retrieval_config", {})
            rewrite_result = understanding_result.get("rewrite_result", {})
            
            bm25_target = retrieval_config.get("bm25_target", "fragments")
            bm25_top_k = retrieval_config.get("bm25_top_k", 50)
            keywords = rewrite_result.get("bm25_keywords", [])
            
            # æ„å»ºæŸ¥è¯¢
            query_body = self._build_bm25_query(
                understanding_result["normalized_query"], 
                keywords, 
                bm25_target, 
                bm25_top_k, 
                filters
            )
            
            # æ‰§è¡Œæœç´¢
            response = self.opensearch_client.search(self.index_name, query_body)
            return self._process_bm25_results(response)
            
        except Exception as e:
            logger.error(f"BM25æ£€ç´¢å¤±è´¥: {str(e)}")
            return []
    
    def _build_bm25_query(self, query: str, keywords: List[str], target: str, size: int, filters: Dict = None) -> Dict:
        """æ„å»ºBM25æŸ¥è¯¢"""
        should_queries = []
        
        # ä¸»æŸ¥è¯¢
        should_queries.append({
            "multi_match": {
                "query": query,
                "fields": ["title^3", "content", "summary^2"],
                "boost": 2.0
            }
        })
        
        # å…³é”®è¯æŸ¥è¯¢
        for keyword in keywords:
            should_queries.append({
                "multi_match": {
                    "query": keyword,
                    "fields": ["title^3", "content^1.5", "summary^2"],
                    "boost": 1.5
                }
            })
        
        query_body = {
            "query": {
                "bool": {
                    "should": should_queries,
                    "minimum_should_match": 1
                }
            },
            "size": size,
            "highlight": {
                "fields": {
                    "title": {},
                    "content": {"fragment_size": 100, "number_of_fragments": 3}
                }
            },
            "sort": [{"_score": {"order": "desc"}}]
        }
        
        # æ·»åŠ è¿‡æ»¤æ¡ä»¶
        if filters:
            filter_conditions = []
            if filters.get('doc_types'):
                filter_conditions.append({"terms": {"doc_type": filters['doc_types']}})
            if filters.get('content_types'):
                filter_conditions.append({"terms": {"content_type": filters['content_types']}})
            
            if filter_conditions:
                query_body["query"]["bool"]["filter"] = filter_conditions
        
        return query_body
    
    def _process_bm25_results(self, response: Dict) -> List[Dict]:
        """å¤„ç†BM25æœç´¢ç»“æœ"""
        results = []
        hits = response.get('hits', {}).get('hits', [])
        
        for hit in hits:
            result = {
                "doc_id": hit['_source'].get("doc_id", ""),
                "section_id": hit['_source'].get("section_id", ""),
                "element_id": hit['_source'].get("element_id", ""),
                "title": hit['_source'].get("title", ""),
                "content": hit['_source'].get("content", ""),
                "content_type": hit['_source'].get("content_type", "text"),
                "page_number": hit['_source'].get("page_number", 1),
                "bbox": hit['_source'].get("bbox", {}),
                "score": hit['_score'],
                "source": "bm25",
                "highlight": hit.get('highlight', {}),
                "metadata": hit['_source'].get("metadata", {})
            }
            results.append(result)
        
        return results
    
    def _mock_bm25_results(self) -> List[Dict]:
        """æ¨¡æ‹ŸBM25ç»“æœ"""
        return [
            {
                "doc_id": "doc_001",
                "section_id": "section_001_001",
                "element_id": "element_001_001_001",
                "title": "HCPæ£€æµ‹æ–¹æ³•",
                "content": "ä½¿ç”¨ELISAæ–¹æ³•æ£€æµ‹å®¿ä¸»ç»†èƒè›‹ç™½å«é‡ï¼Œæ£€æµ‹é™ä¸º10ng/ml...",
                "content_type": "fragment",
                "page_number": 1,
                "bbox": {"x": 100, "y": 200, "width": 400, "height": 50},
                "score": 9.2,
                "source": "bm25",
                "highlight": {"content": ["<mark>HCP</mark>æ£€æµ‹"]},
                "metadata": {}
            }
        ]
    
    def _vector_retrieval(self, understanding_result: Dict, filters: Dict = None) -> List[Dict]:
        """â‘¢ å‘é‡æ£€ç´¢"""
        try:
            if not self.milvus_client or not self.embedding_model:
                return self._mock_vector_results()
            
            retrieval_config = understanding_result.get("retrieval_config", {})
            rewrite_result = understanding_result.get("rewrite_result", {})
            
            vector_query = rewrite_result.get("vector_query", "")
            vector_top_k = retrieval_config.get("vector_top_k", 50)
            
            # ç¼–ç æŸ¥è¯¢å‘é‡
            query_vector = self.embedding_model.encode(
                vector_query, 
                normalize_embeddings=self.normalize
            ).tolist()
            
            # æ‰§è¡Œå‘é‡æœç´¢
            results = self.milvus_client.search_vectors([query_vector], top_k=vector_top_k)
            return self._process_vector_results(results)
            
        except Exception as e:
            logger.error(f"å‘é‡æ£€ç´¢å¤±è´¥: {str(e)}")
            return []
    
    def _process_vector_results(self, results: List[Dict]) -> List[Dict]:
        """å¤„ç†å‘é‡æœç´¢ç»“æœ"""
        processed = []
        for result in results:
            # ä»metadataä¸­æå–é¢å¤–ä¿¡æ¯
            metadata = result.get("metadata", {})
            
            processed_result = {
                "doc_id": result.get("document_id", ""),
                "section_id": metadata.get("section_id", ""),
                "element_id": result.get("element_id", ""),
                "title": metadata.get("title", ""),
                "content": result.get("content", ""),
                "content_type": metadata.get("content_type", "text"),
                "page_number": metadata.get("page_number", 1),
                "bbox": metadata.get("bbox", {}),
                "score": result.get("score", 0.0),
                "source": "vector",
                "metadata": metadata
            }
            processed.append(processed_result)
        
        return processed
    
    def _mock_vector_results(self) -> List[Dict]:
        """æ¨¡æ‹Ÿå‘é‡ç»“æœ"""
        return [
            {
                "doc_id": "doc_002",
                "section_id": "section_002_001",
                "element_id": "element_002_001_001",
                "title": "ç”Ÿç‰©åˆ¶å“è´¨é‡æ§åˆ¶",
                "content": "è›‹ç™½è´¨çº¯åº¦æ£€æµ‹æ˜¯ç¡®ä¿ç”Ÿç‰©åˆ¶å“å®‰å…¨æ€§å’Œæœ‰æ•ˆæ€§çš„å…³é”®æ­¥éª¤...",
                "content_type": "fragment",
                "page_number": 2,
                "bbox": {"x": 100, "y": 300, "width": 400, "height": 60},
                "score": 0.89,
                "source": "vector",
                "metadata": {}
            }
        ]
    
    def _graph_retrieval(self, understanding_result: Dict, filters: Dict = None) -> List[Dict]:
        """â‘¢ å›¾è°±æ£€ç´¢"""
        try:
            if not self.neo4j_client:
                return []
            
            # ç®€åŒ–çš„å›¾è°±æŸ¥è¯¢é€»è¾‘
            entities = understanding_result.get("entities", {})
            if not entities:
                return []
            
            # æ‰§è¡Œå›¾è°±æŸ¥è¯¢ - ä¿®å¤æŸ¥è¯¢é€»è¾‘ä»¥é€‚åº”å®é™…æ•°æ®ç»“æ„
            with self.neo4j_client.session() as session:
                # å®‰å…¨åœ°æå–å®ä½“åç§°
                entity_names = []
                if entities:
                    for entity_list in entities.values():
                        if entity_list and len(entity_list) > 0:
                            entity_names.extend(entity_list)
                
                if not entity_names:
                    return []  # æ²¡æœ‰å®ä½“åˆ™è¿”å›ç©ºç»“æœ
                
                # æ‰©å±•åŒä¹‰è¯
                expanded_entities = self._expand_entity_synonyms(entity_names)
                logger.info(f"å›¾è°±æ£€ç´¢å®ä½“: {entity_names} -> æ‰©å±•å: {expanded_entities}")
                
                # ç­–ç•¥1: éå†æ‰€æœ‰æ‰©å±•å®ä½“è¿›è¡Œå…³ç³»æŸ¥è¯¢
                all_graph_results = []
                all_entity_results = []
                
                for entity_name in expanded_entities:
                    # æŸ¥è¯¢å®ä½“å…³ç³»
                    cypher_query = """
                    MATCH (a:Entity)-[r]->(b:Entity)
                    WHERE a.canonical CONTAINS $entity_name OR b.canonical CONTAINS $entity_name
                    RETURN a, b, type(r) as relation
                    LIMIT 5
                    """
                    
                    result = session.run(cypher_query, entity_name=entity_name)
                    graph_results = list(result)
                    all_graph_results.extend(graph_results)
                    
                    # å¦‚æœæ²¡æœ‰å…³ç³»ï¼ŒæŸ¥è¯¢å•ä¸ªå®ä½“
                    if not graph_results:
                        cypher_query2 = """
                        MATCH (n:Entity)
                        WHERE n.canonical CONTAINS $entity_name
                        RETURN n
                        LIMIT 3
                        """
                        
                        result2 = session.run(cypher_query2, entity_name=entity_name)
                        entity_results = list(result2)
                        all_entity_results.extend(entity_results)
                
                # å¤„ç†ç»“æœ
                if all_graph_results:
                    logger.info(f"å›¾è°±æ£€ç´¢æ‰¾åˆ°{len(all_graph_results)}ä¸ªå…³ç³»")
                    return self._process_graph_results(all_graph_results)
                
                if all_entity_results:
                    logger.info(f"å›¾è°±æ£€ç´¢æ‰¾åˆ°{len(all_entity_results)}ä¸ªç›¸å…³å®ä½“")
                    return self._process_single_entity_results(all_entity_results)
                
                logger.info(f"å›¾è°±æ£€ç´¢æœªæ‰¾åˆ°ä¸'{entity_names}'ç›¸å…³çš„å†…å®¹")
                return []
                
        except Exception as e:
            logger.error(f"å›¾è°±æ£€ç´¢å¤±è´¥: {str(e)}")
            return []
    
    def _process_graph_results(self, results: List) -> List[Dict]:
        """å¤„ç†å›¾è°±æœç´¢ç»“æœ"""
        processed = []
        for record in results:
            a_node = dict(record["a"])
            b_node = dict(record["b"]) 
            relation = record["relation"]
            
            # ä½¿ç”¨canonicalå­—æ®µï¼Œå› ä¸ºnameå­—æ®µä¸ºç©º
            a_name = a_node.get('canonical', '') or a_node.get('name', '') or 'å®ä½“A'
            b_name = b_node.get('canonical', '') or b_node.get('name', '') or 'å®ä½“B'
            
            content = f"{a_name} {relation} {b_name}"
            
            processed.append({
                "doc_id": f"graph_{hash(content)}",
                "section_id": f"graph_section_{hash(content)}",
                "element_id": f"graph_element_{hash(content)}",
                "title": f"å›¾è°±å…³ç³»ï¼š{relation}",
                "content": content,
                "content_type": "graph",
                "page_number": 1,
                "bbox": {},
                "score": 0.8,
                "source": "graph",
                "metadata": {"relation": relation, "source_node": a_node, "target_node": b_node}
            })
        
        return processed
    
    def _process_single_entity_results(self, results: List) -> List[Dict]:
        """å¤„ç†å•ä¸ªå®ä½“æœç´¢ç»“æœ"""
        processed = []
        for record in results:
            entity = dict(record["n"])
            
            # ä½¿ç”¨canonicalå­—æ®µï¼Œå› ä¸ºnameå­—æ®µä¸ºç©º
            entity_name = entity.get('canonical', '') or entity.get('name', '') or 'æœªçŸ¥å®ä½“'
            entity_type = entity.get('entity_type', '') or entity.get('type', '') or 'æœªçŸ¥ç±»å‹'
            
            content = f"ç›¸å…³å®ä½“: {entity_name} (ç±»å‹: {entity_type})"
            
            processed.append({
                "doc_id": f"entity_{hash(entity_name)}",
                "section_id": f"entity_section_{hash(entity_name)}",
                "element_id": f"entity_element_{hash(entity_name)}",
                "title": entity_name,
                "content": content,
                "content_type": "entity",
                "page_number": 1,
                "bbox": {},
                "score": 0.6,
                "source": "graph_entity",
                "metadata": {
                    "entity_type": entity_type,
                    "entity_data": entity
                }
            })
        
        return processed
    
    def _expand_entity_synonyms(self, entity_names: List[str]) -> List[str]:
        """æ‰©å±•å®ä½“åŒä¹‰è¯"""
        expanded = set()
        
        for entity_name in entity_names:
            # æ·»åŠ åŸå§‹å®ä½“
            expanded.add(entity_name)
            
            # æŸ¥æ‰¾åŒä¹‰è¯
            if entity_name in self.synonym_dict:
                synonyms = self.synonym_dict[entity_name]
                if isinstance(synonyms, list):
                    expanded.update(synonyms)
                else:
                    expanded.add(synonyms)
            
            # åå‘æŸ¥æ‰¾
            for key, synonyms in self.synonym_dict.items():
                if isinstance(synonyms, list):
                    if entity_name in synonyms:
                        expanded.add(key)
                        expanded.update(synonyms)
                else:
                    if entity_name == synonyms:
                        expanded.add(key)
        
        # æ·»åŠ ç‰¹æ®Šæ˜ å°„è§„åˆ™
        entity_mappings = {
            "HCP": ["å®¿ä¸»ç»†èƒè›‹ç™½", "Host Cell Protein"],
            "CHO": ["ä¸­å›½ä»“é¼ åµå·¢", "CHO-K1"],
            "æ¡ˆä¾‹åˆ†äº«": ["æ¡ˆä¾‹", "åˆ†äº«", "ç»éªŒ"],
            "è®¢è´§ä¿¡æ¯": ["è®¢è´§", "é‡‡è´­", "è®¢å•"]
        }
        
        for entity_name in entity_names:
            if entity_name in entity_mappings:
                expanded.update(entity_mappings[entity_name])
        
        return list(expanded)
    
    def _aggregate_by_section(self, bm25_results: List[Dict], vector_results: List[Dict], 
                            graph_results: List[Dict], understanding_result: Dict) -> List[Dict]:
        """â‘£ èšåˆä¸åˆ†æ•°èåˆï¼ˆåˆ° section ç²’åº¦ï¼‰"""
        try:
            all_results = bm25_results + vector_results + graph_results
            section_groups = {}
            
            for result in all_results:
                section_id = result.get("section_id", "")
                if not section_id:
                    continue
                
                if section_id not in section_groups:
                    section_groups[section_id] = {
                        "section_id": section_id,
                        "doc_id": result.get("doc_id", ""),
                        "title": result.get("title", ""),
                        "bm25_scores": [],
                        "vector_scores": [],
                        "graph_scores": [],
                        "evidence_elements": [],
                        "all_sources": set(),
                        "metadata": {"page_numbers": set(), "content_types": set()}
                    }
                
                group = section_groups[section_id]
                source = result.get("source", "unknown")
                score = result.get("score", 0)
                
                # æŒ‰æ¥æºåˆ†ç±»åˆ†æ•°
                if source == "bm25":
                    group["bm25_scores"].append(score)
                elif source == "vector":
                    group["vector_scores"].append(score)
                elif source == "graph":
                    group["graph_scores"].append(score)
                
                group["all_sources"].add(source)
                
                # æ”¶é›†è¯æ®å…ƒç´ 
                evidence = {
                    "element_id": result.get("element_id", ""),
                    "content": result.get("content", "")[:150] + "..." if result.get("content", "") else "",
                    "score": score,
                    "source": source,
                    "highlight": result.get("highlight", {}),
                    "bbox": result.get("bbox", {}),
                    "page_number": result.get("page_number", 1)
                }
                group["evidence_elements"].append(evidence)
                
                # æ›´æ–°å…ƒæ•°æ®
                if result.get("page_number"):
                    group["metadata"]["page_numbers"].add(result.get("page_number"))
                if result.get("content_type"):
                    group["metadata"]["content_types"].add(result.get("content_type"))
            
            # å¯¹æ¯ä¸ªsectionè¿›è¡Œåˆ†æ•°èåˆ
            section_candidates = []
            for section_id, group in section_groups.items():
                # å½’ä¸€åŒ–å„è·¯åˆ†æ•°
                bm25_norm = self._normalize_scores_list(group["bm25_scores"])
                vector_norm = self._normalize_scores_list(group["vector_scores"])
                graph_norm = self._normalize_scores_list(group["graph_scores"])
                
                # çº¿æ€§åŠ æƒèåˆ
                final_score = 0.5 * bm25_norm + 0.5 * vector_norm + 0.0 * graph_norm
                
                # é€‰æ‹©Top-3è¯æ®å…ƒç´ 
                top_evidence = sorted(group["evidence_elements"], 
                                    key=lambda x: x["score"], reverse=True)[:3]
                
                section_candidate = {
                    "section_id": section_id,
                    "doc_id": group["doc_id"],
                    "title": group["title"],
                    "final_score": final_score,
                    "bm25_score": bm25_norm,
                    "vector_score": vector_norm,
                    "graph_score": graph_norm,
                    "sources": list(group["all_sources"]),
                    "evidence_elements": top_evidence,
                    "evidence_count": len(group["evidence_elements"]),
                    "metadata": {
                        **group["metadata"],
                        "page_numbers": list(group["metadata"]["page_numbers"]),
                        "content_types": list(group["metadata"]["content_types"])
                    }
                }
                
                section_candidates.append(section_candidate)
            
            # æŒ‰æœ€ç»ˆåˆ†æ•°æ’åºï¼Œå–Top-50ä¸ªsectionä½œä¸ºé‡æ’å€™é€‰
            section_candidates.sort(key=lambda x: x["final_score"], reverse=True)
            return section_candidates[:50]
            
        except Exception as e:
            logger.error(f"èšåˆå¤±è´¥: {str(e)}")
            return []
    
    def _normalize_scores_list(self, scores: List[float]) -> float:
        """å½’ä¸€åŒ–åˆ†æ•°åˆ—è¡¨"""
        if not scores:
            return 0.0
        
        if len(scores) == 1:
            return scores[0]
        
        # Min-Maxå½’ä¸€åŒ–
        min_score = min(scores)
        max_score = max(scores)
        
        if max_score == min_score:
            return 0.5
        
        normalized_scores = [(score - min_score) / (max_score - min_score) for score in scores]
        return sum(normalized_scores) / len(normalized_scores)
    
    def _rerank_sections(self, section_candidates: List[Dict], understanding_result: Dict) -> Optional[Dict]:
        """â‘¤ é‡æ’ï¼ˆæŠŠ"æœ€ç›¸å…³çš„ä¸€èŠ‚"æ”¾åˆ°ç¬¬ä¸€ï¼‰"""
        try:
            if not section_candidates:
                return None
            
            original_query = understanding_result.get("normalized_query", "")
            
            if self.reranker:
                # ä½¿ç”¨çœŸå®çš„é‡æ’æ¨¡å‹
                query_section_pairs = []
                for candidate in section_candidates:
                    rerank_text = self._build_rerank_text(candidate)
                    query_section_pairs.append([original_query, rerank_text])
                
                # æ‰¹é‡é‡æ’
                batch_size = self.reranker_config.get('batch_size', 16)
                rerank_scores = []
                
                for i in range(0, len(query_section_pairs), batch_size):
                    batch = query_section_pairs[i:i+batch_size]
                    batch_scores = self.reranker.predict(batch)
                    rerank_scores.extend(batch_scores)
                
                # æ›´æ–°åˆ†æ•°å¹¶æ’åº
                for i, candidate in enumerate(section_candidates):
                    candidate["rerank_score"] = float(rerank_scores[i])
                    candidate["final_score"] = candidate["final_score"] * 0.3 + candidate["rerank_score"] * 0.7
            else:
                # ä½¿ç”¨ç®€å•è¯„åˆ†
                for candidate in section_candidates:
                    title = candidate.get("title", "")
                    evidence_text = " ".join([ev.get("content", "") for ev in candidate.get("evidence_elements", [])])
                    
                    # è®¡ç®—æŸ¥è¯¢è¯åŒ¹é…åº¦
                    query_words = set(original_query.lower().split())
                    title_words = set(title.lower().split())
                    evidence_words = set(evidence_text.lower().split())
                    
                    title_match = len(query_words.intersection(title_words)) / len(query_words) if query_words else 0
                    evidence_match = len(query_words.intersection(evidence_words)) / len(query_words) if query_words else 0
                    
                    rerank_score = title_match * 2 + evidence_match
                    candidate["rerank_score"] = rerank_score
                    candidate["final_score"] = candidate["final_score"] * 0.5 + rerank_score * 0.5
            
            # æ’åºå¹¶è¿”å›Top-1
            section_candidates.sort(key=lambda x: x["final_score"], reverse=True)
            top_section = section_candidates[0]
            
            # ç‰‡æ®µçº§é«˜äº®
            top_section["evidence_highlights"] = self._select_evidence_highlights(top_section, original_query)
            
            return top_section
            
        except Exception as e:
            logger.error(f"é‡æ’å¤±è´¥: {str(e)}")
            return section_candidates[0] if section_candidates else None
    
    def _build_rerank_text(self, candidate: Dict) -> str:
        """æ„å»ºé‡æ’ç”¨çš„æ–‡æœ¬"""
        title = candidate.get("title", "")
        evidence_elements = candidate.get("evidence_elements", [])
        
        # å–å‰2-3ä¸ªæœ€ç›¸å…³çš„ç‰‡æ®µ
        top_evidence = evidence_elements[:3]
        evidence_texts = [ev.get("content", "") for ev in top_evidence]
        
        # ç»„åˆæ–‡æœ¬
        rerank_text = title
        if evidence_texts:
            rerank_text += " " + " ".join(evidence_texts)
        
        # æˆªæ–­åˆ°512 tokensï¼ˆç²—ç•¥æŒ‰å­—ç¬¦æ•°ä¼°ç®—ï¼‰
        max_chars = 512 * 2
        if len(rerank_text) > max_chars:
            rerank_text = rerank_text[:max_chars] + "..."
        
        return rerank_text
    
    def _select_evidence_highlights(self, top_section: Dict, query: str) -> List[Dict]:
        """ç‰‡æ®µçº§é«˜äº®é€‰æ‹©"""
        evidence_elements = top_section.get("evidence_elements", [])
        
        if not evidence_elements:
            return []
        
        # è®¡ç®—é«˜äº®åˆ†æ•°
        for evidence in evidence_elements:
            content = evidence.get("content", "")
            query_words = set(query.lower().split())
            content_words = set(content.lower().split())
            
            match_score = len(query_words.intersection(content_words)) / len(query_words) if query_words else 0
            evidence["highlight_score"] = evidence.get("score", 0) * 0.7 + match_score * 0.3
        
        # æŒ‰é«˜äº®åˆ†æ•°æ’åºï¼Œé€‰æ‹©1-3æ¡
        evidence_elements.sort(key=lambda x: x.get("highlight_score", 0), reverse=True)
        return evidence_elements[:3]
    
    def _expand_section_content(self, top_section: Dict) -> List[Dict]:
        """â‘¦ æ‰©å±•ï¼ˆæŠŠ"ä¸€å®¶å­"æ‹‰é½ï¼‰"""
        try:
            section_id = top_section.get("section_id")
            if not section_id:
                return []
            
            if self.neo4j_client:
                # åŸºäºå®é™…æ•°æ®åº“ç»“æ„æŸ¥è¯¢ç›¸å…³å†…å®¹
                expanded_elements = self._query_actual_graph_structure(section_id, top_section)
                if expanded_elements:
                    return expanded_elements
                else:
                    # å¦‚æœå›¾æ•°æ®åº“ä¸­æ²¡æœ‰æ‰¾åˆ°ï¼Œä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®
                    logger.info(f"å›¾æ•°æ®åº“ä¸­æœªæ‰¾åˆ°section_id={section_id}çš„å†…å®¹ï¼Œä½¿ç”¨æ¨¡æ‹Ÿæ‰©å±•")
                    return self._mock_section_expansion(top_section)
            else:
                # ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®
                return self._mock_section_expansion(top_section)
                
        except Exception as e:
            logger.error(f"å†…å®¹æ‰©å±•å¤±è´¥: {str(e)}")
            return self._mock_section_expansion(top_section)
    
    def _query_actual_graph_structure(self, section_id: str, top_section: Dict) -> List[Dict]:
        """åŸºäºå®é™…æ•°æ®åº“ç»“æ„æŸ¥è¯¢ç›¸å…³å†…å®¹"""
        try:
            expanded_elements = []
            
            with self.neo4j_client.session() as session:
                # ç­–ç•¥1: ä»section_idæå–çœŸå®çš„doc_id
                actual_doc_id = self._extract_doc_id_from_section(section_id, top_section)
                logger.info(f"æå–åˆ°çš„doc_id: {actual_doc_id}")
                
                if actual_doc_id:
                    # æŸ¥è¯¢ä¸è¯¥æ–‡æ¡£ç›¸å…³çš„æ‰€æœ‰å®ä½“
                    cypher_query = """
                    MATCH (d:Document {id: $doc_id})-[r:CONTAINS]->(e:Entity)
                    RETURN e, type(r) as relation_type
                    LIMIT 20
                    """
                    result = session.run(cypher_query, doc_id=actual_doc_id)
                    
                    for i, record in enumerate(result):
                        entity = dict(record["e"])
                        entity_name = entity.get('name', '') or entity.get('canonical', '') or f"å®ä½“_{i+1}"
                        entity_type = entity.get('entity_type', '') or entity.get('type', '') or "æœªçŸ¥ç±»å‹"
                        
                        element = {
                            "element_id": f"{section_id}_entity_{i}",
                            "content_type": "entity",
                            "content": f"å®ä½“: {entity_name} (ç±»å‹: {entity_type})",
                            "title": entity_name,
                            "order": i + 1,
                            "page_number": entity.get("page_number", 1),
                            "bbox": {},
                            "metadata": {
                                "doc_id": actual_doc_id,
                                "section_id": section_id,
                                "entity_type": entity_type,
                                "entity_id": entity.get('entity_id', ''),
                                "confidence": entity.get('confidence', 0.0),
                                "source": "neo4j_entity"
                            }
                        }
                        expanded_elements.append(element)
                
                # ç­–ç•¥2: å¦‚æœæ‰¾ä¸åˆ°Documentï¼Œå°è¯•ç›´æ¥æŸ¥æ‰¾ç›¸å…³å®ä½“
                if not expanded_elements:
                    # ä»sectionæ ‡é¢˜ä¸­æå–å…³é”®è¯ï¼ŒæŸ¥æ‰¾ç›¸å…³å®ä½“
                    section_title = top_section.get("title", "")
                    if section_title:
                        # æå–å¯èƒ½çš„å®ä½“åç§°
                        keywords = [word for word in section_title.split() if len(word) > 2]
                        
                        for keyword in keywords[:3]:  # é™åˆ¶å…³é”®è¯æ•°é‡
                            cypher_query = """
                            MATCH (e:Entity)
                            WHERE e.name CONTAINS $keyword OR e.canonical CONTAINS $keyword
                            RETURN e
                            LIMIT 5
                            """
                            result = session.run(cypher_query, keyword=keyword)
                            
                            for i, record in enumerate(result):
                                entity = dict(record["e"])
                                entity_name = entity.get('name', '') or entity.get('canonical', '') or f"ç›¸å…³å®ä½“_{i+1}"
                                entity_type = entity.get('entity_type', '') or entity.get('type', '') or "æœªçŸ¥ç±»å‹"
                                
                                element = {
                                    "element_id": f"{section_id}_related_{keyword}_{i}",
                                    "content_type": "related_entity",
                                    "content": f"ç›¸å…³å®ä½“: {entity_name} (åŒ¹é…å…³é”®è¯: {keyword})",
                                    "title": entity_name,
                                    "order": len(expanded_elements) + i + 1,
                                    "page_number": entity.get("page_number", 1),
                                    "bbox": {},
                                    "metadata": {
                                        "doc_id": "related",
                                        "section_id": section_id,
                                        "entity_type": entity_type,
                                        "entity_id": entity.get('entity_id', ''),
                                        "match_keyword": keyword,
                                        "source": "neo4j_related"
                                    }
                                }
                                expanded_elements.append(element)
                
                # ç­–ç•¥3: æ··åˆå®é™…æ•°æ®å’Œç»“æ„åŒ–å†…å®¹
                if expanded_elements:
                    # å¦‚æœæ‰¾åˆ°å®é™…å®ä½“ï¼Œæ·»åŠ ç»“æ„åŒ–çš„ç« èŠ‚å†…å®¹
                    section_title = top_section.get("title", "")
                    if section_title:
                        # æ·»åŠ æ ‡é¢˜å…ƒç´ 
                        title_element = {
                            "element_id": f"{section_id}_title",
                            "content_type": "title",
                            "content": section_title,
                            "title": section_title,
                            "order": 0,
                            "page_number": 1,
                            "bbox": {},
                            "metadata": {
                                "doc_id": actual_doc_id,
                                "section_id": section_id,
                                "source": "mixed_content"
                            }
                        }
                        expanded_elements.insert(0, title_element)
                        
                        # é‡æ–°è°ƒæ•´order
                        for i, element in enumerate(expanded_elements[1:], 1):
                            element["order"] = i
                
                logger.info(f"ä»å®é™…å›¾æ•°æ®åº“ç»“æ„æŸ¥è¯¢åˆ°{len(expanded_elements)}ä¸ªç›¸å…³å…ƒç´ ")
                return expanded_elements
                
        except Exception as e:
            logger.error(f"å®é™…å›¾æ•°æ®åº“ç»“æ„æŸ¥è¯¢å¤±è´¥: {str(e)}")
            return []
    
    def _extract_doc_id_from_section(self, section_id: str, top_section: Dict) -> any:
        """ä»section_idæå–çœŸå®çš„doc_id"""
        try:
            # ç­–ç•¥1: ä»top_sectionè·å–ï¼Œä½†éœ€è¦éªŒè¯æ ¼å¼
            doc_id_from_section = top_section.get("doc_id", "")
            
            # ç­–ç•¥2: ä»section_idè§£æ
            # section_idæ ¼å¼: 20250818_170435_05dc2896_doc#2025-08-18#7_0009
            if "#" in section_id:
                parts = section_id.split("#")
                if len(parts) >= 3:
                    # ä»æœ€åä¸€éƒ¨åˆ†æå–æ•°å­— (å¦‚: 7_0009 -> 7)
                    last_part = parts[-1]
                    if "_" in last_part:
                        doc_id_str = last_part.split("_")[0]
                        try:
                            doc_id = int(doc_id_str)
                            logger.info(f"ä»section_idè§£æåˆ°doc_id: {doc_id}")
                            return doc_id
                        except ValueError:
                            pass
            
            # ç­–ç•¥3: æ£€æŸ¥æ˜¯å¦æ˜¯æµ‹è¯•æ•°æ®ï¼Œè½¬æ¢ä¸ºå®é™…ID
            if doc_id_from_section == "test_doc_001" or not doc_id_from_section:
                # æŸ¥è¯¢æ•°æ®åº“ä¸­å®é™…å­˜åœ¨çš„ç¬¬ä¸€ä¸ªDocument ID
                with self.neo4j_client.session() as session:
                    result = session.run("MATCH (d:Document) RETURN d.id as doc_id LIMIT 1")
                    for record in result:
                        actual_id = record["doc_id"]
                        logger.info(f"ä½¿ç”¨æ•°æ®åº“ä¸­çš„å®é™…doc_id: {actual_id}")
                        return actual_id
            
            # ç­–ç•¥4: å°è¯•ç›´æ¥ä½¿ç”¨åŸå§‹doc_id
            if doc_id_from_section:
                logger.info(f"ä½¿ç”¨åŸå§‹doc_id: {doc_id_from_section}")
                return doc_id_from_section
            
            logger.warning("æ— æ³•æå–æœ‰æ•ˆçš„doc_id")
            return None
            
        except Exception as e:
            logger.error(f"æå–doc_idå¤±è´¥: {str(e)}")
            return None
    
    def _mock_section_expansion(self, top_section: Dict) -> List[Dict]:
        """æ¨¡æ‹Ÿsectionæ‰©å±•å†…å®¹"""
        section_id = top_section.get("section_id", "")
        doc_id = top_section.get("doc_id", "")
        
        return [
            {
                "element_id": f"{section_id}_title",
                "content_type": "title",
                "content": top_section.get("title", ""),
                "title": top_section.get("title", ""),
                "order": 1,
                "page_number": 1,
                "bbox": {},
                "metadata": {
                    "doc_id": doc_id, 
                    "section_id": section_id,
                    "source": "mock_data"
                }
            },
            {
                "element_id": f"{section_id}_paragraph_001",
                "content_type": "paragraph",
                "content": "è¿™æ˜¯è¯¥ç« èŠ‚çš„ç¬¬ä¸€æ®µå†…å®¹ï¼Œè¯¦ç»†æè¿°äº†ç›¸å…³çš„æŠ€æœ¯è¦ç‚¹å’Œæ“ä½œè§„èŒƒ...",
                "title": "",
                "order": 2,
                "page_number": 1,
                "bbox": {"x": 100, "y": 200, "width": 400, "height": 50},
                "metadata": {
                    "doc_id": doc_id, 
                    "section_id": section_id,
                    "source": "mock_data"
                }
            },
            {
                "element_id": f"{section_id}_table_001",
                "content_type": "table",
                "content": "å‚æ•°åç§° | æ ‡å‡†å€¼ | æ£€æµ‹æ–¹æ³•\nHCPå«é‡ | <100ng/mg | ELISA\npHå€¼ | 7.0Â±0.2 | pHè®¡",
                "title": "å…³é”®å‚æ•°è¡¨",
                "order": 3,
                "page_number": 1,
                "bbox": {"x": 100, "y": 300, "width": 400, "height": 100},
                "metadata": {
                    "doc_id": doc_id, 
                    "section_id": section_id,
                    "source": "mock_data"
                }
            },
            {
                "element_id": f"{section_id}_image_001",
                "content_type": "image",
                "content": "å›¾1ï¼šHCPæ£€æµ‹æµç¨‹ç¤ºæ„å›¾",
                "title": "æ£€æµ‹æµç¨‹å›¾",
                "order": 4,
                "page_number": 2,
                "bbox": {"x": 100, "y": 100, "width": 400, "height": 300},
                "metadata": {
                    "doc_id": doc_id, 
                    "section_id": section_id,
                    "source": "mock_data",
                    "image_path": "/images/hcp_process.jpg"
                }
            }
        ]
    
    def _enrich_multimodal_details(self, expanded_content: List[Dict]) -> List[Dict]:
        """â‘§ å›¾è¡¨ç»†èŠ‚ï¼ˆMySQLï¼‰"""
        try:
            enriched_content = []
            
            for element in expanded_content:
                element_copy = element.copy()
                content_type = element.get("content_type", "")
                
                if content_type == "table":
                    # è¡¥å……è¡¨æ ¼ç»†èŠ‚
                    element_copy["table_details"] = {
                        "rows": 3,
                        "columns": 3,
                        "headers": ["å‚æ•°åç§°", "æ ‡å‡†å€¼", "æ£€æµ‹æ–¹æ³•"],
                        "data": [
                            ["HCPå«é‡", "<100ng/mg", "ELISA"],
                            ["pHå€¼", "7.0Â±0.2", "pHè®¡"],
                            ["çº¯åº¦", ">95%", "SDS-PAGE"]
                        ],
                        "html": """<table class="data-table">
                            <tr><th>å‚æ•°åç§°</th><th>æ ‡å‡†å€¼</th><th>æ£€æµ‹æ–¹æ³•</th></tr>
                            <tr><td>HCPå«é‡</td><td>&lt;100ng/mg</td><td>ELISA</td></tr>
                            <tr><td>pHå€¼</td><td>7.0Â±0.2</td><td>pHè®¡</td></tr>
                            <tr><td>çº¯åº¦</td><td>&gt;95%</td><td>SDS-PAGE</td></tr>
                        </table>"""
                    }
                elif content_type == "image":
                    # è¡¥å……å›¾ç‰‡ç»†èŠ‚
                    element_copy["image_details"] = {
                        "image_path": "/upload/images/hcp_process_diagram.jpg",
                        "caption": "HCPæ£€æµ‹æ ‡å‡†æ“ä½œæµç¨‹å›¾",
                        "alt_text": "æµç¨‹å›¾æ˜¾ç¤ºäº†ä»æ ·å“å‡†å¤‡åˆ°ç»“æœåˆ†æçš„å®Œæ•´HCPæ£€æµ‹æ­¥éª¤",
                        "width": 800,
                        "height": 600,
                        "format": "jpg",
                        "size_kb": 245
                    }
                
                enriched_content.append(element_copy)
            
            return enriched_content
            
        except Exception as e:
            logger.error(f"å›¾è¡¨ç»†èŠ‚è¡¥å……å¤±è´¥: {str(e)}")
            return expanded_content
    
    def _stream_render_answer(self, query: str, top_section: Dict, enriched_content: List[Dict], 
                            understanding_result: Dict) -> Generator[Dict, None, None]:
        """â‘¨ ç»„è£…/æ¸²æŸ“ï¼ˆå¯æµå¼ï¼‰"""
        try:
            # é¦–å±è¾“å‡ºï¼šæ‰¾åˆ°ç« èŠ‚ä¿¡æ¯
            section_title = self._get_section_title(enriched_content)
            yield {
                "type": "answer_chunk",
                "content": f"æ‰¾åˆ°ç›¸å…³ç« èŠ‚ï¼š**{section_title}**\n\n"
            }
            
            # æŒ‰orderæ’åºå†…å®¹
            sorted_content = sorted(enriched_content, key=lambda x: x.get("order", 999))
            
            # æµå¼è¾“å‡ºå†…å®¹å…ƒç´ 
            paragraph_count = 0
            for element in sorted_content:
                content_type = element.get("content_type", "text")
                
                if content_type in ["title", "paragraph"]:
                    # æ ‡é¢˜å’Œæ®µè½ï¼šç«‹å³æµå¼è¾“å‡º
                    content = self._apply_evidence_highlighting(element, top_section.get("evidence_highlights", []))
                    
                    yield {
                        "type": "answer_chunk",
                        "content": content + "\n\n"
                    }
                    
                    paragraph_count += 1
                    if paragraph_count == 2:
                        sleep(0.1)  # å‰ä¸¤ä¸ªæ®µè½è¾“å‡ºåç¨å¾®æš‚åœ
                        
                elif content_type == "table":
                    # è¡¨æ ¼ï¼šæ¨é€è¡¨æ ¼äº‹ä»¶
                    yield {
                        "type": "multimodal_content",
                        "content_type": "table",
                        "data": self._format_table_for_stream(element)
                    }
                    
                elif content_type == "image":
                    # å›¾ç‰‡ï¼šæ¨é€å›¾ç‰‡äº‹ä»¶
                    yield {
                        "type": "multimodal_content",
                        "content_type": "image",
                        "data": self._format_image_for_stream(element)
                    }
            
            # ç”Ÿæˆå¼•ç”¨ä¿¡æ¯
            references = self._build_references_from_content(enriched_content, top_section.get("evidence_highlights", []))
            if references:
                yield {
                    "type": "answer_chunk",
                    "content": f"\n**å‚è€ƒæ¥æºï¼š**\n{references}\n"
                }
            
            # ç”Ÿæˆæœ€ç»ˆå®Œæ•´ç­”æ¡ˆ
            final_answer = {
                "query": query,
                "intent_type": understanding_result.get("intent_type", ""),
                "selected_section": {
                    "section_id": top_section.get("section_id"),
                    "score": top_section.get("final_score", 0),
                    "title": section_title
                },
                "evidence_highlights": top_section.get("evidence_highlights", []),
                "total_elements": len(enriched_content),
                "multimodal_elements": {
                    "tables": len([e for e in enriched_content if e.get("content_type") == "table"]),
                    "images": len([e for e in enriched_content if e.get("content_type") == "image"]),
                    "paragraphs": len([e for e in enriched_content if e.get("content_type") == "paragraph"])
                },
                "generation_time": datetime.now().isoformat()
            }
            
            yield {
                "type": "final_answer",
                "content": final_answer,
                "metadata": {
                    "generation_method": "order_based_rendering",
                    "has_multimodal": any(e.get("content_type") in ["table", "image"] for e in enriched_content)
                }
            }
            
        except Exception as e:
            logger.error(f"æµå¼æ¸²æŸ“å¤±è´¥: {str(e)}")
            yield {"type": "error", "message": f"ç­”æ¡ˆç”Ÿæˆå¤±è´¥: {str(e)}"}
    
    def _get_section_title(self, content: List[Dict]) -> str:
        """ä»å†…å®¹ä¸­æå–sectionæ ‡é¢˜"""
        for element in content:
            if element.get("content_type") == "title":
                return element.get("content", "æœªçŸ¥ç« èŠ‚")
        
        if content:
            return content[0].get("title", "æœªçŸ¥ç« èŠ‚")
        
        return "æœªçŸ¥ç« èŠ‚"
    
    def _apply_evidence_highlighting(self, element: Dict, evidence_highlights: List[Dict]) -> str:
        """å¯¹è¯æ®è¿›è¡Œé«˜äº®æ ‡è®°"""
        content = element.get("content", "")
        element_id = element.get("element_id", "")
        
        # æ£€æŸ¥å½“å‰å…ƒç´ æ˜¯å¦åœ¨é«˜äº®è¯æ®ä¸­
        is_highlighted = any(ev.get("element_id") == element_id for ev in evidence_highlights)
        
        if is_highlighted and content:
            return f"<mark style='background-color: #fff3cd; padding: 2px 4px;'>{content}</mark>"
        
        return content
    
    def _format_table_for_stream(self, table_element: Dict) -> Dict:
        """æ ¼å¼åŒ–è¡¨æ ¼ç”¨äºæµå¼è¾“å‡º"""
        table_details = table_element.get("table_details", {})
        
        return {
            "element_id": table_element.get("element_id", ""),
            "title": table_element.get("title", "æ•°æ®è¡¨"),
            "content": table_element.get("content", ""),
            "html": table_details.get("html", ""),
            "rows": table_details.get("rows", 0),
            "columns": table_details.get("columns", 0),
            "headers": table_details.get("headers", []),
            "data": table_details.get("data", []),
            "page_number": table_element.get("page_number", 1),
            "bbox": table_element.get("bbox", {}),
            "url": f"/api/file/view/{table_element.get('metadata', {}).get('doc_id')}?page={table_element.get('page_number')}&highlight=table"
        }
    
    def _format_image_for_stream(self, image_element: Dict) -> Dict:
        """æ ¼å¼åŒ–å›¾ç‰‡ç”¨äºæµå¼è¾“å‡º"""
        image_details = image_element.get("image_details", {})
        
        return {
            "element_id": image_element.get("element_id", ""),
            "title": image_element.get("title", "å›¾ç‰‡"),
            "content": image_element.get("content", ""),
            "caption": image_details.get("caption", ""),
            "alt_text": image_details.get("alt_text", ""),
            "image_path": image_details.get("image_path", ""),
            "width": image_details.get("width", 0),
            "height": image_details.get("height", 0),
            "format": image_details.get("format", ""),
            "page_number": image_element.get("page_number", 1),
            "bbox": image_element.get("bbox", {}),
            "url": f"/api/file/view/{image_element.get('metadata', {}).get('doc_id')}?page={image_element.get('page_number')}&highlight=image"
        }
    
    def _build_references_from_content(self, content: List[Dict], evidence_highlights: List[Dict]) -> str:
        """ä»å†…å®¹æ„å»ºå‚è€ƒæ¥æº"""
        references = []
        doc_info = {}
        
        # æ”¶é›†æ–‡æ¡£ä¿¡æ¯
        for element in content:
            metadata = element.get("metadata", {})
            doc_id = metadata.get("doc_id", "")
            section_id = metadata.get("section_id", "")
            
            if doc_id and doc_id not in doc_info:
                doc_info[doc_id] = {
                    "section_id": section_id,
                    "title": element.get("title", ""),
                    "page_numbers": set()
                }
            
            if doc_id and element.get("page_number"):
                doc_info[doc_id]["page_numbers"].add(element.get("page_number"))
        
        # ç”Ÿæˆå¼•ç”¨æ ¼å¼
        for i, (doc_id, info) in enumerate(doc_info.items(), 1):
            pages = sorted(list(info["page_numbers"]))
            page_text = f"ç¬¬{', '.join(map(str, pages))}é¡µ" if pages else ""
            
            ref = f"[{i}] {info['title']} ({page_text})"
            references.append(ref)
        
        return "\n".join(references)
