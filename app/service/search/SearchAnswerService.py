"""
æœç´¢ç­”æ¡ˆç”ŸæˆæœåŠ¡
å®ç°ä¸Šä¸‹æ–‡æ‹¼è£…ä¸å›ç­”ç”ŸæˆåŠŸèƒ½ï¼ŒåŒ…æ‹¬ï¼š
1. è¯æ®æ‹¼è£…ï¼ˆContext Builderï¼‰
2. ä¿ç•™åŸæ ¼å¼è¾“å‡º
3. ç”Ÿæˆç­”æ¡ˆï¼ˆæ”¯æŒæµå¼ç”Ÿæˆï¼‰
"""

import json
import logging
import re
import yaml
from typing import Dict, List, Optional, Generator, Any
from datetime import datetime
import requests
from time import sleep

# é…ç½®æ—¥å¿—
logger = logging.getLogger(__name__)


class SearchAnswerService:
    """æœç´¢ç­”æ¡ˆç”ŸæˆæœåŠ¡ç±»"""
    
    def __init__(self):
        """åˆå§‹åŒ–æœç´¢ç­”æ¡ˆç”ŸæˆæœåŠ¡"""
        self._load_config()
        self._init_llm_client()
        self._init_templates()
        
    def _load_config(self):
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        try:
            with open('config/model.yaml', 'r', encoding='utf-8') as f:
                self.model_config = yaml.safe_load(f)
            with open('config/prompt.yaml', 'r', encoding='utf-8') as f:
                self.prompt_config = yaml.safe_load(f)
            logger.info("é…ç½®æ–‡ä»¶åŠ è½½æˆåŠŸ")
        except Exception as e:
            logger.error(f"åŠ è½½é…ç½®æ–‡ä»¶å¤±è´¥: {str(e)}")
            self.model_config = {}
            self.prompt_config = {}
    
    def _init_llm_client(self):
        """åˆå§‹åŒ–LLMå®¢æˆ·ç«¯"""
        try:
            deepseek_config = self.model_config.get('deepseek', {})
            self.api_url = deepseek_config.get('api_url', 'https://api.deepseek.com')
            self.api_key = deepseek_config.get('api_key', '')
            self.model_name = deepseek_config.get('model_name', 'deepseek-chat')
            self.max_tokens = deepseek_config.get('max_tokens', 4096)
            self.temperature = deepseek_config.get('temperature', 0.7)
            
            if not self.api_key:
                logger.warning("DeepSeek APIå¯†é’¥æœªé…ç½®ï¼Œå°†ä½¿ç”¨æ¨¡æ‹Ÿå›ç­”")
                self.llm_client = None
            else:
                self.llm_client = self._create_llm_client()
                logger.info("LLMå®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ")
                
        except Exception as e:
            logger.error(f"LLMå®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {str(e)}")
            self.llm_client = None
    
    def _create_llm_client(self):
        """åˆ›å»ºLLMå®¢æˆ·ç«¯"""
        # è¿™é‡Œå¯ä»¥æ ¹æ®éœ€è¦ä½¿ç”¨ä¸åŒçš„LLMåº“
        # ç›®å‰ä½¿ç”¨ç®€å•çš„HTTPè¯·æ±‚
        return {
            'api_url': self.api_url,
            'api_key': self.api_key,
            'headers': {
                'Authorization': f'Bearer {self.api_key}',
                'Content-Type': 'application/json'
            }
        }
    
    def _init_templates(self):
        """åˆå§‹åŒ–ç­”æ¡ˆæ¨¡æ¿"""
        self.answer_templates = {
            "definition": """åŸºäºä»¥ä¸‹è¯æ®ï¼Œæˆ‘ä¸ºæ‚¨è§£é‡Š"{query}"çš„å®šä¹‰ï¼š

{conclusion}

**è¯¦ç»†è¯´æ˜ï¼š**
{evidence_points}

**å‚è€ƒæ¥æºï¼š**
{references}

{original_links}""",
            
            "factual": """æ ¹æ®æ£€ç´¢åˆ°çš„ä¿¡æ¯ï¼Œå…³äº"{query}"çš„å›ç­”å¦‚ä¸‹ï¼š

{conclusion}

**å…·ä½“ä¿¡æ¯ï¼š**
{evidence_points}

**å‚è€ƒæ¥æºï¼š**
{references}

{original_links}""",
            
            "comparison": """å…³äº"{query}"çš„å¯¹æ¯”åˆ†æå¦‚ä¸‹ï¼š

{conclusion}

**å¯¹æ¯”è¦ç‚¹ï¼š**
{evidence_points}

**å‚è€ƒæ¥æºï¼š**
{references}

{original_links}""",
            
            "process": """å…³äº"{query}"çš„æµç¨‹æ­¥éª¤å¦‚ä¸‹ï¼š

{conclusion}

**è¯¦ç»†æ­¥éª¤ï¼š**
{evidence_points}

**å‚è€ƒæ¥æºï¼š**
{references}

{original_links}""",
            
            "numerical": """å…³äº"{query}"çš„æ•°å€¼å‚æ•°ä¿¡æ¯ï¼š

{conclusion}

**å…·ä½“æ•°å€¼ï¼š**
{evidence_points}

**å‚è€ƒæ¥æºï¼š**
{references}

{original_links}""",
            
            "default": """æ ¹æ®æ‚¨çš„æŸ¥è¯¢"{query}"ï¼Œæ£€ç´¢åˆ°ä»¥ä¸‹ç›¸å…³ä¿¡æ¯ï¼š

{conclusion}

**ç›¸å…³è¦ç‚¹ï¼š**
{evidence_points}

**å‚è€ƒæ¥æºï¼š**
{references}

{original_links}"""
        }
    
    def generate_answer_stream(self, query: str, retrieval_result: Dict, 
                             understanding_result: Dict) -> Generator[Dict, None, None]:
        """
        æµå¼ç”Ÿæˆç­”æ¡ˆ
        ä¸¥æ ¼æŒ‰ç…§è¦æ±‚è¾“å‡ºå¢é‡å†…å®¹å’Œå¤šæ¨¡æ€äº‹ä»¶
        
        Args:
            query: ç”¨æˆ·æŸ¥è¯¢
            retrieval_result: æ£€ç´¢ç»“æœ
            understanding_result: æŸ¥è¯¢ç†è§£ç»“æœ
            
        Yields:
            Dict: æµå¼ç­”æ¡ˆç‰‡æ®µæˆ–å¤šæ¨¡æ€äº‹ä»¶
        """
        try:
            logger.info(f"å¼€å§‹æµå¼ç”Ÿæˆç­”æ¡ˆ: {query}")
            
            # æ„å»ºä¸Šä¸‹æ–‡
            context = self._build_context(retrieval_result, understanding_result)
            
            # æµå¼ç”Ÿæˆæ–‡æœ¬ç­”æ¡ˆ
            if self.llm_client:
                # ä½¿ç”¨LLMæµå¼ç”Ÿæˆ
                for chunk in self._stream_llm_generation(query, context, understanding_result):
                    yield chunk
            else:
                # ä½¿ç”¨æ¨¡æ¿æµå¼ç”Ÿæˆ
                for chunk in self._stream_template_generation(query, context, understanding_result):
                    yield chunk
            
            # å¤„ç†å¤šæ¨¡æ€å†…å®¹
            for multimodal_chunk in self._stream_multimodal_content(context):
                yield multimodal_chunk
            
            # ç”Ÿæˆæœ€ç»ˆå®Œæ•´ç­”æ¡ˆï¼ˆåŒ…å«å¼•ç”¨ç­‰ï¼‰
            final_answer = self._format_final_answer(query, context, understanding_result)
            
            yield {
                "type": "final_answer",
                "content": final_answer,
                "context": context,
                "metadata": {
                    "total_sources": len(context.get("sources", [])),
                    "evidence_count": len(context.get("evidence_list", [])),
                    "generation_method": "llm" if self.llm_client else "template"
                }
            }
            
        except Exception as e:
            logger.error(f"æµå¼ç”Ÿæˆç­”æ¡ˆå¤±è´¥: {str(e)}")
            yield {
                "type": "error",
                "content": f"ç­”æ¡ˆç”Ÿæˆå¤±è´¥: {str(e)}"
            }
    
    def generate_answer_complete(self, query: str, retrieval_result: Dict, 
                               understanding_result: Dict) -> Dict:
        """
        å®Œæ•´ç”Ÿæˆç­”æ¡ˆï¼ˆéæµå¼ï¼‰
        
        Args:
            query: ç”¨æˆ·æŸ¥è¯¢
            retrieval_result: æ£€ç´¢ç»“æœ
            understanding_result: æŸ¥è¯¢ç†è§£ç»“æœ
            
        Returns:
            Dict: å®Œæ•´ç­”æ¡ˆ
        """
        try:
            logger.info(f"å¼€å§‹å®Œæ•´ç”Ÿæˆç­”æ¡ˆ: {query}")
            
            # è¯æ®æ‹¼è£…
            context = self._build_context(retrieval_result, understanding_result)
            
            # ç”Ÿæˆç­”æ¡ˆ
            if self.llm_client:
                answer_content = self._generate_llm_answer(query, context, understanding_result)
            else:
                answer_content = self._generate_template_answer(query, context, understanding_result)
            
            # æ ¼å¼åŒ–æœ€ç»ˆç­”æ¡ˆ
            final_answer = self._format_final_answer(query, context, understanding_result, answer_content)
            
            return {
                "answer": final_answer,
                "context": context,
                "metadata": {
                    "total_sources": len(context.get("sources", [])),
                    "evidence_count": len(context.get("evidence_list", [])),
                    "generation_method": "llm" if self.llm_client else "template",
                    "timestamp": datetime.now().isoformat()
                }
            }
            
        except Exception as e:
            logger.error(f"å®Œæ•´ç”Ÿæˆç­”æ¡ˆå¤±è´¥: {str(e)}")
            return {
                "answer": f"æŠ±æ­‰ï¼Œç”Ÿæˆç­”æ¡ˆæ—¶å‘ç”Ÿé”™è¯¯ï¼š{str(e)}",
                "context": {},
                "metadata": {"error": str(e)}
            }
    
    def _build_context(self, retrieval_result: Dict, understanding_result: Dict) -> Dict:
        """
        æ„å»ºä¸Šä¸‹æ–‡
        
        Args:
            retrieval_result: æ£€ç´¢ç»“æœ
            understanding_result: æŸ¥è¯¢ç†è§£ç»“æœ
            
        Returns:
            Dict: ä¸Šä¸‹æ–‡ä¿¡æ¯
        """
        try:
            candidates = retrieval_result.get("candidates", [])
            
            # æŒ‰æ–‡æ¡£åˆå¹¶ç›¸é‚»å—
            merged_chunks = self._merge_adjacent_chunks(candidates)
            
            # è¡¨æ ¼å¼ºåŒ–
            enhanced_chunks = self._enhance_table_context(merged_chunks)
            
            # å›¾ç‰‡/å›¾è¡¨å¤„ç†
            image_enhanced_chunks = self._enhance_image_context(enhanced_chunks)
            
            # æ„å»ºè¯æ®åˆ—è¡¨
            evidence_list = self._build_evidence_list(image_enhanced_chunks)
            
            # æå–å…³é”®ä¿¡æ¯
            key_facts = self._extract_key_facts(evidence_list, understanding_result)
            
            # æ„å»ºå¼•ç”¨æ˜ å°„
            references = self._build_references(evidence_list)
            
            # æ„å»ºåŸæ–‡é“¾æ¥
            original_links = self._build_original_links(evidence_list)
            
            return {
                "evidence_list": evidence_list,
                "key_facts": key_facts,
                "references": references,
                "original_links": original_links,
                "sources": retrieval_result.get("sources", []),
                "total_chunks": len(candidates),
                "merged_chunks": len(merged_chunks)
            }
            
        except Exception as e:
            logger.error(f"æ„å»ºä¸Šä¸‹æ–‡å¤±è´¥: {str(e)}")
            return {
                "evidence_list": [],
                "key_facts": [],
                "references": [],
                "original_links": [],
                "sources": [],
                "error": str(e)
            }
    
    def _merge_adjacent_chunks(self, candidates: List[Dict]) -> List[Dict]:
        """åˆå¹¶ç›¸é‚»å—"""
        if not candidates:
            return []
        
        # æŒ‰æ–‡æ¡£å’Œé¡µé¢åˆ†ç»„
        doc_groups = {}
        for candidate in candidates:
            doc_id = candidate.get("doc_id", "")
            page_no = candidate.get("page_no", 1)
            key = f"{doc_id}_{page_no}"
            
            if key not in doc_groups:
                doc_groups[key] = []
            doc_groups[key].append(candidate)
        
        merged_chunks = []
        
        for key, chunks in doc_groups.items():
            if len(chunks) == 1:
                merged_chunks.extend(chunks)
                continue
            
            # æŒ‰bboxä½ç½®æ’åºï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
            chunks.sort(key=lambda x: x.get("bbox", [0])[0] if x.get("bbox") else 0)
            
            # æ£€æŸ¥æ˜¯å¦å¯ä»¥åˆå¹¶
            current_chunk = chunks[0]
            
            for next_chunk in chunks[1:]:
                # æ£€æŸ¥æ˜¯å¦ç›¸é‚»ï¼ˆç®€åŒ–åˆ¤æ–­ï¼‰
                if self._are_chunks_adjacent(current_chunk, next_chunk):
                    # åˆå¹¶å†…å®¹
                    current_chunk["content"] += " " + next_chunk.get("content", "")
                    current_chunk["bbox"] = self._merge_bbox(
                        current_chunk.get("bbox", []),
                        next_chunk.get("bbox", [])
                    )
                    # å–æœ€é«˜åˆ†æ•°
                    current_chunk["final_score"] = max(
                        current_chunk.get("final_score", 0),
                        next_chunk.get("final_score", 0)
                    )
                else:
                    # ä¸ç›¸é‚»ï¼Œæ·»åŠ å½“å‰å—å¹¶å¼€å§‹æ–°çš„å—
                    merged_chunks.append(current_chunk)
                    current_chunk = next_chunk
            
            # æ·»åŠ æœ€åä¸€ä¸ªå—
            merged_chunks.append(current_chunk)
        
        return merged_chunks
    
    def _are_chunks_adjacent(self, chunk1: Dict, chunk2: Dict) -> bool:
        """åˆ¤æ–­ä¸¤ä¸ªå—æ˜¯å¦ç›¸é‚»"""
        bbox1 = chunk1.get("bbox", [])
        bbox2 = chunk2.get("bbox", [])
        
        if not bbox1 or not bbox2 or len(bbox1) < 4 or len(bbox2) < 4:
            return False
        
        # ç®€å•åˆ¤æ–­ï¼šå‚ç›´è·ç¦»å°äºé˜ˆå€¼
        vertical_distance = abs(bbox1[1] - bbox2[3])  # chunk1çš„topä¸chunk2çš„bottom
        return vertical_distance < 50  # 50åƒç´ é˜ˆå€¼
    
    def _merge_bbox(self, bbox1: List, bbox2: List) -> List:
        """åˆå¹¶ä¸¤ä¸ªbbox"""
        if not bbox1:
            return bbox2
        if not bbox2:
            return bbox1
        
        if len(bbox1) >= 4 and len(bbox2) >= 4:
            return [
                min(bbox1[0], bbox2[0]),  # left
                min(bbox1[1], bbox2[1]),  # top
                max(bbox1[2], bbox2[2]),  # right
                max(bbox1[3], bbox2[3])   # bottom
            ]
        
        return bbox1
    
    def _enhance_table_context(self, chunks: List[Dict]) -> List[Dict]:
        """è¡¨æ ¼å¼ºåŒ–"""
        enhanced_chunks = []
        
        for chunk in chunks:
            content_type = chunk.get("metadata", {}).get("content_type", "text")
            
            if content_type == "table":
                # å¯¹è¡¨æ ¼å†…å®¹è¿›è¡Œå¼ºåŒ–
                enhanced_chunk = chunk.copy()
                content = chunk.get("content", "")
                
                # æ·»åŠ è¡¨æ ¼è¯´æ˜
                enhanced_chunk["content"] = f"[è¡¨æ ¼æ•°æ®] {content}"
                
                # å¦‚æœæ˜¯è¡¨æ ¼ï¼Œå°è¯•è§£æç»“æ„
                table_data = self._parse_table_structure(content)
                if table_data:
                    enhanced_chunk["table_structure"] = table_data
                
                enhanced_chunks.append(enhanced_chunk)
            else:
                enhanced_chunks.append(chunk)
        
        return enhanced_chunks
    
    def _parse_table_structure(self, content: str) -> Optional[Dict]:
        """è§£æè¡¨æ ¼ç»“æ„ï¼ˆç®€åŒ–ç‰ˆï¼‰"""
        try:
            # ç®€å•çš„è¡¨æ ¼è§£æ
            lines = content.split('\n')
            table_data = {
                "rows": len(lines),
                "estimated_columns": 0,
                "headers": [],
                "preview": content[:200] + "..." if len(content) > 200 else content
            }
            
            # ä¼°è®¡åˆ—æ•°ï¼ˆåŸºäºåˆ†éš”ç¬¦ï¼‰
            if lines:
                separators = ['\t', '|', ',']
                for sep in separators:
                    col_count = lines[0].count(sep) + 1
                    if col_count > table_data["estimated_columns"]:
                        table_data["estimated_columns"] = col_count
            
            return table_data
            
        except Exception as e:
            logger.error(f"è§£æè¡¨æ ¼ç»“æ„å¤±è´¥: {str(e)}")
            return None
    
    def _enhance_image_context(self, chunks: List[Dict]) -> List[Dict]:
        """å›¾ç‰‡/å›¾è¡¨å¼ºåŒ–"""
        enhanced_chunks = []
        
        for chunk in chunks:
            if "å›¾" in chunk.get("content", "") or "å›¾è¡¨" in chunk.get("content", ""):
                enhanced_chunk = chunk.copy()
                
                # æ·»åŠ å›¾ç‰‡è¯´æ˜
                enhanced_chunk["has_image"] = True
                enhanced_chunk["image_info"] = {
                    "page_no": chunk.get("page_no", 1),
                    "bbox": chunk.get("bbox", []),
                    "doc_id": chunk.get("doc_id", ""),
                    "caption": self._extract_image_caption(chunk.get("content", ""))
                }
                
                enhanced_chunks.append(enhanced_chunk)
            else:
                enhanced_chunks.append(chunk)
        
        return enhanced_chunks
    
    def _extract_image_caption(self, content: str) -> str:
        """æå–å›¾ç‰‡è¯´æ˜"""
        # ç®€å•çš„å›¾ç‰‡è¯´æ˜æå–
        if "å›¾" in content:
            lines = content.split('\n')
            for line in lines:
                if "å›¾" in line and len(line) < 100:
                    return line.strip()
        
        return ""
    
    def _build_evidence_list(self, chunks: List[Dict]) -> List[Dict]:
        """æ„å»ºè¯æ®åˆ—è¡¨"""
        evidence_list = []
        
        for i, chunk in enumerate(chunks[:10]):  # æœ€å¤šå–10ä¸ªè¯æ®
            evidence = {
                "id": i + 1,
                "content": chunk.get("content", ""),
                "title": chunk.get("title", ""),
                "source": chunk.get("source", ""),
                "doc_id": chunk.get("doc_id", ""),
                "page_no": chunk.get("page_no", 1),
                "bbox": chunk.get("bbox", []),
                "score": chunk.get("final_score", 0),
                "file_type": chunk.get("file_type", ""),
                "has_table": chunk.get("metadata", {}).get("content_type") == "table",
                "has_image": chunk.get("has_image", False),
                "confidence": self._calculate_evidence_confidence(chunk)
            }
            
            evidence_list.append(evidence)
        
        return evidence_list
    
    def _calculate_evidence_confidence(self, chunk: Dict) -> float:
        """è®¡ç®—è¯æ®ç½®ä¿¡åº¦"""
        confidence = chunk.get("final_score", 0)
        
        # æ ¹æ®æ¥æºè°ƒæ•´ç½®ä¿¡åº¦
        source = chunk.get("source", "")
        if source == "graph":
            confidence += 0.1  # å›¾è°±ç»“æœæ›´å¯ä¿¡
        elif source == "bm25" and chunk.get("highlight"):
            confidence += 0.05  # æœ‰é«˜äº®çš„BM25ç»“æœæ›´å¯ä¿¡
        
        # æ ¹æ®å†…å®¹è´¨é‡è°ƒæ•´
        content = chunk.get("content", "")
        if len(content) > 100:
            confidence += 0.05  # å†…å®¹ä¸°å¯Œ
        
        return min(confidence, 1.0)
    
    def _extract_key_facts(self, evidence_list: List[Dict], understanding_result: Dict) -> List[str]:
        """æå–å…³é”®äº‹å®"""
        key_facts = []
        query_type = understanding_result.get("query_type", "factual")
        
        for evidence in evidence_list[:5]:  # ä»å‰5ä¸ªè¯æ®æå–
            content = evidence.get("content", "")
            
            # åŸºäºæŸ¥è¯¢ç±»å‹æå–ä¸åŒç±»å‹çš„äº‹å®
            if query_type == "numerical":
                facts = self._extract_numerical_facts(content)
            elif query_type == "definition":
                facts = self._extract_definition_facts(content)
            elif query_type == "process":
                facts = self._extract_process_facts(content)
            else:
                facts = self._extract_general_facts(content)
            
            key_facts.extend(facts)
        
        # å»é‡å¹¶é™åˆ¶æ•°é‡
        unique_facts = list(set(key_facts))
        return unique_facts[:8]
    
    def _extract_numerical_facts(self, content: str) -> List[str]:
        """æå–æ•°å€¼äº‹å®"""
        facts = []
        
        # æŸ¥æ‰¾æ•°å€¼æ¨¡å¼
        patterns = [
            r'\d+\.?\d*\s*[%ï¼…]',  # ç™¾åˆ†æ¯”
            r'\d+\.?\d*\s*[mg|ml|g|kg|â„ƒ|Â°C]',  # å•ä½æ•°å€¼
            r'pH\s*[å€¼]?\s*[:ï¼š]?\s*\d+\.?\d*',  # pHå€¼
            r'èŒƒå›´\s*[:ï¼š]?\s*\d+\.?\d*\s*[-~]\s*\d+\.?\d*',  # èŒƒå›´
            r'\d+\.?\d*\s*[-~è‡³åˆ°]\s*\d+\.?\d*',  # èŒƒå›´
        ]
        
        for pattern in patterns:
            matches = re.findall(pattern, content)
            for match in matches:
                facts.append(match.strip())
        
        return facts[:3]
    
    def _extract_definition_facts(self, content: str) -> List[str]:
        """æå–å®šä¹‰äº‹å®"""
        facts = []
        
        # æŸ¥æ‰¾å®šä¹‰æ¨¡å¼
        sentences = content.split('ã€‚')
        for sentence in sentences:
            if any(keyword in sentence for keyword in ['æ˜¯', 'æŒ‡', 'å®šä¹‰ä¸º', 'ç§°ä¸º', 'å«åš']):
                if len(sentence.strip()) < 200:  # é¿å…è¿‡é•¿çš„å¥å­
                    facts.append(sentence.strip() + 'ã€‚')
        
        return facts[:3]
    
    def _extract_process_facts(self, content: str) -> List[str]:
        """æå–æµç¨‹äº‹å®"""
        facts = []
        
        # æŸ¥æ‰¾æ­¥éª¤æ¨¡å¼
        patterns = [
            r'ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+æ­¥[ï¼š:]?[^ã€‚]*ã€‚',
            r'\d+[ã€\.][\s]*[^ã€‚]*ã€‚',
            r'æ­¥éª¤\d+[ï¼š:]?[^ã€‚]*ã€‚',
            r'[é¦–å…ˆ|ç„¶å|æ¥ä¸‹æ¥|æœ€å][^ã€‚]*ã€‚'
        ]
        
        for pattern in patterns:
            matches = re.findall(pattern, content)
            for match in matches:
                if len(match.strip()) < 150:
                    facts.append(match.strip())
        
        return facts[:4]
    
    def _extract_general_facts(self, content: str) -> List[str]:
        """æå–ä¸€èˆ¬äº‹å®"""
        facts = []
        
        # ç®€å•çš„å¥å­åˆ†å‰²
        sentences = content.split('ã€‚')
        for sentence in sentences:
            sentence = sentence.strip()
            if 20 < len(sentence) < 100:  # é•¿åº¦åˆé€‚çš„å¥å­
                facts.append(sentence + 'ã€‚')
        
        return facts[:2]
    
    def _build_references(self, evidence_list: List[Dict]) -> List[str]:
        """æ„å»ºå¼•ç”¨åˆ—è¡¨"""
        references = []
        
        for evidence in evidence_list:
            ref = f"[{evidence['id']}] {evidence.get('title', 'æœªçŸ¥æ ‡é¢˜')}"
            
            if evidence.get('page_no'):
                ref += f" (ç¬¬{evidence['page_no']}é¡µ)"
            
            if evidence.get('file_type'):
                ref += f" [{evidence['file_type'].upper()}]"
            
            references.append(ref)
        
        return references
    
    def _build_original_links(self, evidence_list: List[Dict]) -> List[Dict]:
        """æ„å»ºåŸæ–‡é“¾æ¥"""
        links = []
        
        for evidence in evidence_list:
            link = {
                "id": evidence["id"],
                "text": "æŸ¥çœ‹åŸæ–‡",
                "doc_id": evidence.get("doc_id", ""),
                "page_no": evidence.get("page_no", 1),
                "bbox": evidence.get("bbox", []),
                "file_type": evidence.get("file_type", "")
            }
            
            # ç”ŸæˆæŸ¥çœ‹é“¾æ¥ï¼ˆæ ¹æ®æ–‡ä»¶ç±»å‹ï¼‰
            if evidence.get("file_type") == "pdf" and evidence.get("bbox"):
                link["url"] = f"/api/file/view/{evidence['doc_id']}?page={evidence['page_no']}&bbox={','.join(map(str, evidence['bbox']))}"
            else:
                link["url"] = f"/api/file/view/{evidence['doc_id']}?page={evidence['page_no']}"
            
            links.append(link)
        
        return links
    
    def _stream_llm_generation(self, query: str, context: Dict, 
                             understanding_result: Dict) -> Generator[Dict, None, None]:
        """æµå¼LLMç”Ÿæˆ"""
        try:
            prompt = self._build_llm_prompt(query, context, understanding_result)
            
            # æ„å»ºè¯·æ±‚
            payload = {
                "model": self.model_name,
                "messages": [
                    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„çŸ¥è¯†é—®ç­”åŠ©æ‰‹ï¼Œæ ¹æ®æä¾›çš„è¯æ®å‡†ç¡®å›ç­”ç”¨æˆ·é—®é¢˜ã€‚"},
                    {"role": "user", "content": prompt}
                ],
                "max_tokens": self.max_tokens,
                "temperature": self.temperature,
                "stream": True
            }
            
            # å‘é€æµå¼è¯·æ±‚
            response = requests.post(
                f"{self.api_url}/v1/chat/completions",
                headers=self.llm_client["headers"],
                json=payload,
                stream=True
            )
            
            if response.status_code == 200:
                for line in response.iter_lines():
                    if line:
                        line_str = line.decode('utf-8')
                        if line_str.startswith('data: '):
                            data_str = line_str[6:]
                            if data_str == '[DONE]':
                                break
                            
                            try:
                                data = json.loads(data_str)
                                if 'choices' in data and data['choices']:
                                    delta = data['choices'][0].get('delta', {})
                                    if 'content' in delta:
                                        yield {
                                            "type": "answer_chunk",
                                            "content": delta['content']
                                        }
                            except json.JSONDecodeError:
                                continue
            else:
                # è¯·æ±‚å¤±è´¥ï¼Œé™çº§åˆ°æ¨¡æ¿ç”Ÿæˆ
                for chunk in self._stream_template_generation(query, context, understanding_result):
                    yield chunk
                    
        except Exception as e:
            logger.error(f"LLMæµå¼ç”Ÿæˆå¤±è´¥: {str(e)}")
            # é™çº§åˆ°æ¨¡æ¿ç”Ÿæˆ
            for chunk in self._stream_template_generation(query, context, understanding_result):
                yield chunk
    
    def _stream_template_generation(self, query: str, context: Dict, 
                                  understanding_result: Dict) -> Generator[Dict, None, None]:
        """æµå¼æ¨¡æ¿ç”Ÿæˆ - æŒ‰å­—ç¬¦å¢é‡è¾“å‡º"""
        try:
            # ç”Ÿæˆå®Œæ•´ç­”æ¡ˆ
            answer = self._generate_template_answer(query, context, understanding_result)
            
            # æŒ‰å­—ç¬¦æµå¼è¾“å‡ºï¼Œæ¯æ¬¡å‘é€1-3ä¸ªå­—ç¬¦
            chars_per_chunk = 2
            for i in range(0, len(answer), chars_per_chunk):
                chunk = answer[i:i + chars_per_chunk]
                yield {
                    "type": "answer_chunk",
                    "content": chunk
                }
                sleep(0.05)  # æ¨¡æ‹ŸçœŸå®çš„ç”Ÿæˆé€Ÿåº¦
                    
        except Exception as e:
            logger.error(f"æ¨¡æ¿æµå¼ç”Ÿæˆå¤±è´¥: {str(e)}")
            yield {
                "type": "answer_chunk",
                "content": f"æŠ±æ­‰ï¼Œç”Ÿæˆç­”æ¡ˆæ—¶å‘ç”Ÿé”™è¯¯ï¼š{str(e)}"
            }
    
    def _stream_multimodal_content(self, context: Dict) -> Generator[Dict, None, None]:
        """
        æµå¼å¤„ç†å¤šæ¨¡æ€å†…å®¹
        ä¸¥æ ¼æŒ‰ç…§è¦æ±‚åˆ†åˆ«æ¨é€å›¾ç‰‡ã€è¡¨æ ¼ã€å›¾è¡¨äº‹ä»¶
        
        Args:
            context: ä¸Šä¸‹æ–‡ä¿¡æ¯
            
        Yields:
            Dict: å¤šæ¨¡æ€å†…å®¹äº‹ä»¶
        """
        try:
            evidence_list = context.get("evidence_list", [])
            
            for evidence in evidence_list:
                # å¤„ç†å›¾ç‰‡å†…å®¹
                if evidence.get("has_image") and evidence.get("image_info"):
                    yield {
                        "type": "multimodal_content",
                        "content_type": "image",
                        "data": {
                            "element_id": evidence.get("id"),
                            "doc_id": evidence.get("doc_id"),
                            "page_no": evidence.get("page_no"),
                            "bbox": evidence.get("bbox", []),
                            "caption": evidence.get("image_info", {}).get("caption", ""),
                            "url": f"/api/file/view/{evidence.get('doc_id')}?page={evidence.get('page_no')}&highlight=image",
                            "description": evidence.get("content", "")[:100] + "..."
                        }
                    }
                    sleep(0.1)  # æ¨¡æ‹Ÿå¤„ç†æ—¶é—´
                
                # å¤„ç†è¡¨æ ¼å†…å®¹  
                if evidence.get("has_table") and evidence.get("table_structure"):
                    table_data = evidence.get("table_structure", {})
                    yield {
                        "type": "multimodal_content",
                        "content_type": "table",
                        "data": {
                            "element_id": evidence.get("id"),
                            "doc_id": evidence.get("doc_id"),
                            "page_no": evidence.get("page_no"),
                            "bbox": evidence.get("bbox", []),
                            "title": evidence.get("title", f"è¡¨æ ¼ {evidence.get('id')}"),
                            "summary": table_data.get("preview", ""),
                            "rows": table_data.get("rows", 0),
                            "columns": table_data.get("estimated_columns", 0),
                            "url": f"/api/file/view/{evidence.get('doc_id')}?page={evidence.get('page_no')}&highlight=table"
                        }
                    }
                    sleep(0.1)  # æ¨¡æ‹Ÿå¤„ç†æ—¶é—´
                
                # å¤„ç†å›¾è¡¨å†…å®¹
                content = evidence.get("content", "")
                if any(keyword in content for keyword in ["å›¾è¡¨", "å›¾å½¢", "Chart", "chart", "å›¾"]):
                    yield {
                        "type": "multimodal_content", 
                        "content_type": "chart",
                        "data": {
                            "element_id": evidence.get("id"),
                            "doc_id": evidence.get("doc_id"),
                            "page_no": evidence.get("page_no"),
                            "bbox": evidence.get("bbox", []),
                            "description": content[:150] + "..." if len(content) > 150 else content,
                            "url": f"/api/file/view/{evidence.get('doc_id')}?page={evidence.get('page_no')}&highlight=chart"
                        }
                    }
                    sleep(0.1)  # æ¨¡æ‹Ÿå¤„ç†æ—¶é—´
                    
        except Exception as e:
            logger.error(f"æµå¼å¤„ç†å¤šæ¨¡æ€å†…å®¹å¤±è´¥: {str(e)}")
            yield {
                "type": "error",
                "content": f"å¤šæ¨¡æ€å†…å®¹å¤„ç†å¤±è´¥: {str(e)}"
            }
    
    def _build_llm_prompt(self, query: str, context: Dict, understanding_result: Dict) -> str:
        """æ„å»ºLLMæç¤ºè¯"""
        evidence_list = context.get("evidence_list", [])
        key_facts = context.get("key_facts", [])
        
        # æ„å»ºè¯æ®æ–‡æœ¬
        evidence_text = "\n\n".join([
            f"è¯æ®{i+1}ï¼š{evidence['content']}"
            for i, evidence in enumerate(evidence_list[:5])
        ])
        
        # æ„å»ºå…³é”®äº‹å®
        facts_text = "\n".join([f"- {fact}" for fact in key_facts])
        
        prompt = f"""åŸºäºä»¥ä¸‹æ£€ç´¢åˆ°çš„è¯æ®ï¼Œè¯·å‡†ç¡®å›ç­”ç”¨æˆ·é—®é¢˜ã€‚

ç”¨æˆ·é—®é¢˜ï¼š{query}

ç›¸å…³è¯æ®ï¼š
{evidence_text}

å…³é”®äº‹å®ï¼š
{facts_text}

è¦æ±‚ï¼š
1. æ ¹æ®è¯æ®å†…å®¹å›ç­”ï¼Œä¸è¦ç¼–é€ ä¿¡æ¯
2. å¦‚æœè¯æ®ä¸è¶³ï¼Œè¯·æ˜ç¡®è¯´æ˜
3. ä¿æŒå›ç­”çš„ç»“æ„åŒ–å’Œæ¡ç†æ€§
4. é€‚å½“å¼•ç”¨è¯æ®ç¼–å·
5. å›ç­”è¦ä¸“ä¸šã€å‡†ç¡®ã€æœ‰æ¡ç†

è¯·å›ç­”ï¼š"""
        
        return prompt
    
    def _generate_llm_answer(self, query: str, context: Dict, understanding_result: Dict) -> str:
        """ç”ŸæˆLLMç­”æ¡ˆï¼ˆéæµå¼ï¼‰"""
        try:
            prompt = self._build_llm_prompt(query, context, understanding_result)
            
            payload = {
                "model": self.model_name,
                "messages": [
                    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„çŸ¥è¯†é—®ç­”åŠ©æ‰‹ï¼Œæ ¹æ®æä¾›çš„è¯æ®å‡†ç¡®å›ç­”ç”¨æˆ·é—®é¢˜ã€‚"},
                    {"role": "user", "content": prompt}
                ],
                "max_tokens": self.max_tokens,
                "temperature": self.temperature
            }
            
            response = requests.post(
                f"{self.api_url}/v1/chat/completions",
                headers=self.llm_client["headers"],
                json=payload
            )
            
            if response.status_code == 200:
                result = response.json()
                if 'choices' in result and result['choices']:
                    return result['choices'][0]['message']['content']
            
            # å¦‚æœLLMè°ƒç”¨å¤±è´¥ï¼Œé™çº§åˆ°æ¨¡æ¿ç”Ÿæˆ
            return self._generate_template_answer(query, context, understanding_result)
            
        except Exception as e:
            logger.error(f"LLMç­”æ¡ˆç”Ÿæˆå¤±è´¥: {str(e)}")
            return self._generate_template_answer(query, context, understanding_result)
    
    def _generate_template_answer(self, query: str, context: Dict, understanding_result: Dict) -> str:
        """ä½¿ç”¨æ¨¡æ¿ç”Ÿæˆç­”æ¡ˆ"""
        try:
            query_type = understanding_result.get("query_type", "factual")
            evidence_list = context.get("evidence_list", [])
            key_facts = context.get("key_facts", [])
            
            # ç”Ÿæˆç»“è®º
            conclusion = self._generate_conclusion(query, evidence_list, key_facts, query_type)
            
            # ç”Ÿæˆè¯æ®è¦ç‚¹
            evidence_points = self._generate_evidence_points(evidence_list, key_facts)
            
            # ç”Ÿæˆå¼•ç”¨
            references = context.get("references", [])
            
            # ç”ŸæˆåŸæ–‡é“¾æ¥
            original_links = self._format_original_links(context.get("original_links", []))
            
            # é€‰æ‹©æ¨¡æ¿
            template = self.answer_templates.get(query_type, self.answer_templates["default"])
            
            # å¡«å……æ¨¡æ¿
            answer = template.format(
                query=query,
                conclusion=conclusion,
                evidence_points=evidence_points,
                references="\n".join(references),
                original_links=original_links
            )
            
            return answer
            
        except Exception as e:
            logger.error(f"æ¨¡æ¿ç­”æ¡ˆç”Ÿæˆå¤±è´¥: {str(e)}")
            return f"æŠ±æ­‰ï¼ŒåŸºäºæ£€ç´¢åˆ°çš„ä¿¡æ¯æ— æ³•ç”Ÿæˆå®Œæ•´ç­”æ¡ˆã€‚é”™è¯¯ï¼š{str(e)}"
    
    def _generate_conclusion(self, query: str, evidence_list: List[Dict], 
                           key_facts: List[str], query_type: str) -> str:
        """ç”Ÿæˆç»“è®º"""
        if not evidence_list:
            return f"æŠ±æ­‰ï¼Œæ²¡æœ‰æ‰¾åˆ°å…³äºã€Œ{query}ã€çš„ç›¸å…³ä¿¡æ¯ã€‚"
        
        # åŸºäºç¬¬ä¸€ä¸ªæœ€é«˜åˆ†è¯æ®ç”Ÿæˆç»“è®º
        top_evidence = evidence_list[0]
        content = top_evidence.get("content", "")
        
        if query_type == "definition":
            return f"æ ¹æ®æ£€ç´¢åˆ°çš„èµ„æ–™ï¼Œ{self._extract_definition_from_content(content, query)}"
        elif query_type == "numerical":
            return f"å…³äº{query}çš„æ•°å€¼ä¿¡æ¯å¦‚ä¸‹ï¼š{self._extract_numbers_from_content(content)}"
        elif query_type == "process":
            return f"{query}çš„æµç¨‹åŒ…æ‹¬ä»¥ä¸‹æ­¥éª¤ï¼š"
        else:
            # æå–å‰100å­—ç¬¦ä½œä¸ºç®€è¦ç»“è®º
            conclusion = content[:100].strip()
            if len(content) > 100:
                conclusion += "..."
            return conclusion
    
    def _extract_definition_from_content(self, content: str, query: str) -> str:
        """ä»å†…å®¹ä¸­æå–å®šä¹‰"""
        sentences = content.split('ã€‚')
        for sentence in sentences:
            if query in sentence and any(keyword in sentence for keyword in ['æ˜¯', 'æŒ‡', 'å®šä¹‰ä¸º']):
                return sentence.strip() + 'ã€‚'
        
        return f"{query}çš„ç›¸å…³å®šä¹‰ä¿¡æ¯å¦‚ä¸‹ï¼š" + content[:150] + ("..." if len(content) > 150 else "")
    
    def _extract_numbers_from_content(self, content: str) -> str:
        """ä»å†…å®¹ä¸­æå–æ•°å­—ä¿¡æ¯"""
        # æŸ¥æ‰¾æ•°å­—æ¨¡å¼
        number_patterns = [
            r'\d+\.?\d*\s*[%ï¼…mg/mlgkgâ„ƒÂ°C]',
            r'pH\s*[å€¼]?\s*[:ï¼š]?\s*\d+\.?\d*',
            r'èŒƒå›´\s*[:ï¼š]?\s*\d+\.?\d*\s*[-~]\s*\d+\.?\d*'
        ]
        
        numbers = []
        for pattern in number_patterns:
            matches = re.findall(pattern, content)
            numbers.extend(matches)
        
        if numbers:
            return "ã€".join(numbers[:5])
        else:
            return content[:100] + ("..." if len(content) > 100 else "")
    
    def _generate_evidence_points(self, evidence_list: List[Dict], key_facts: List[str]) -> str:
        """ç”Ÿæˆè¯æ®è¦ç‚¹"""
        points = []
        
        # ä½¿ç”¨å…³é”®äº‹å®
        for i, fact in enumerate(key_facts[:6]):
            points.append(f"{i+1}. {fact}")
        
        # å¦‚æœå…³é”®äº‹å®ä¸å¤Ÿï¼Œä»è¯æ®ä¸­è¡¥å……
        if len(points) < 3:
            for i, evidence in enumerate(evidence_list[:3]):
                if i >= len(key_facts):
                    content = evidence.get("content", "")
                    point = content[:80].strip()
                    if len(content) > 80:
                        point += "..."
                    points.append(f"{len(points)+1}. {point}")
        
        return "\n".join(points)
    
    def _format_original_links(self, links: List[Dict]) -> str:
        """æ ¼å¼åŒ–åŸæ–‡é“¾æ¥"""
        if not links:
            return ""
        
        formatted_links = []
        for link in links[:5]:  # æœ€å¤šæ˜¾ç¤º5ä¸ªé“¾æ¥
            formatted_links.append(
                f"ğŸ“„ [{link['text']}]({link['url']}) (ç¬¬{link['page_no']}é¡µ)"
            )
        
        return "\n**æŸ¥çœ‹åŸæ–‡ï¼š**\n" + "\n".join(formatted_links)
    
    def _format_final_answer(self, query: str, context: Dict, 
                           understanding_result: Dict, answer_content: str = None) -> Dict:
        """æ ¼å¼åŒ–æœ€ç»ˆç­”æ¡ˆ"""
        evidence_list = context.get("evidence_list", [])
        
        if answer_content is None:
            answer_content = self._generate_template_answer(query, context, understanding_result)
        
        return {
            "query": query,
            "answer": answer_content,
            "evidence_count": len(evidence_list),
            "confidence": self._calculate_answer_confidence(evidence_list),
            "query_type": understanding_result.get("query_type", "factual"),
            "sources": context.get("sources", []),
            "has_tables": any(e.get("has_table", False) for e in evidence_list),
            "has_images": any(e.get("has_image", False) for e in evidence_list),
            "original_links": context.get("original_links", []),
            "generation_time": datetime.now().isoformat()
        }
    
    def _calculate_answer_confidence(self, evidence_list: List[Dict]) -> float:
        """è®¡ç®—ç­”æ¡ˆç½®ä¿¡åº¦"""
        if not evidence_list:
            return 0.0
        
        # åŸºäºè¯æ®æ•°é‡å’Œè´¨é‡è®¡ç®—ç½®ä¿¡åº¦
        confidence = 0.0
        
        # è¯æ®æ•°é‡è´¡çŒ®
        evidence_count = len(evidence_list)
        confidence += min(evidence_count * 0.1, 0.4)
        
        # è¯æ®è´¨é‡è´¡çŒ®ï¼ˆå¹³å‡ç½®ä¿¡åº¦ï¼‰
        avg_evidence_confidence = sum(e.get("confidence", 0) for e in evidence_list) / len(evidence_list)
        confidence += avg_evidence_confidence * 0.6
        
        return min(confidence, 1.0)
