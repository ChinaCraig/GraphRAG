"""
PDFå‘é‡åŒ–æœåŠ¡
è´Ÿè´£å°†PDFæ–‡æ¡£ä¸­çš„æ ‡é¢˜å’Œæ­£æ–‡å†…å®¹è¿›è¡Œå‘é‡åŒ–å¤„ç†ï¼Œæ”¯æŒGraphRAGæŸ¥è¯¢
"""

import logging
import yaml
import os
from typing import Optional, Dict, Any, List
from datetime import datetime
from sentence_transformers import SentenceTransformer

from utils.MilvusManager import MilvusManager


class PdfVectorService:
    """PDFå‘é‡åŒ–æœåŠ¡ç±»"""
    
    def __init__(self, config_path: str = 'config/config.yaml'):
        """
        åˆå§‹åŒ–PDFå‘é‡åŒ–æœåŠ¡
        
        Args:
            config_path: é…ç½®æ–‡ä»¶è·¯å¾„
        """
        self.config_path = config_path
        self.logger = logging.getLogger(__name__)
        
        # åŠ è½½é…ç½®
        self._load_configs()
        
        # åˆå§‹åŒ–åµŒå…¥æ¨¡å‹
        self._init_embedding_model()
        
        # åˆå§‹åŒ–æ•°æ®åº“ç®¡ç†å™¨
        self.milvus_manager = MilvusManager()
    
    def _load_configs(self) -> None:
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        try:
            with open(self.config_path, 'r', encoding='utf-8') as file:
                self.config = yaml.safe_load(file)
            
            with open('config/model.yaml', 'r', encoding='utf-8') as file:
                self.model_config = yaml.safe_load(file)
            
            self.logger.info("PDFå‘é‡åŒ–æœåŠ¡é…ç½®åŠ è½½æˆåŠŸ")
            
        except Exception as e:
            self.logger.error(f"åŠ è½½PDFå‘é‡åŒ–æœåŠ¡é…ç½®å¤±è´¥: {str(e)}")
            raise
    
    def _init_embedding_model(self) -> None:
        """åˆå§‹åŒ–åµŒå…¥æ¨¡å‹"""
        try:
            model_name = self.model_config['embedding']['model_name']
            cache_dir = self.model_config['embedding']['cache_dir']
            
            # è®¾ç½®HuggingFaceç¼“å­˜ç›®å½•ç¯å¢ƒå˜é‡
            os.environ['HF_HOME'] = os.path.abspath(cache_dir)
            os.environ['TRANSFORMERS_CACHE'] = os.path.abspath(cache_dir)
            os.environ['SENTENCE_TRANSFORMERS_HOME'] = os.path.abspath(cache_dir)
            
            self.embedding_model = SentenceTransformer(
                model_name,
                cache_folder=cache_dir
            )
            
            self.dimension = self.model_config['embedding']['dimension']
            self.batch_size = self.model_config['embedding']['batch_size']
            self.normalize = self.model_config['embedding']['normalize']
            
            self.logger.info(f"åµŒå…¥æ¨¡å‹åˆå§‹åŒ–æˆåŠŸ: {model_name}")
            
        except Exception as e:
            self.logger.error(f"åˆå§‹åŒ–åµŒå…¥æ¨¡å‹å¤±è´¥: {str(e)}")
            raise
    
    def process_pdf_json_to_vectors(self, json_data: Dict[str, Any], document_id: int) -> Dict[str, Any]:
        """
        å°†PDFæå–çš„JSONæ•°æ®å¤„ç†ä¸ºå‘é‡æ•°æ®
        titles ç”¨ full_textï¼›fragments ç”¨ blocks[*].text/row_text/caption
        
        Args:
            json_data: JSONæ•°æ®ï¼ˆåŒ…å«sectionsï¼‰
            document_id: æ–‡æ¡£ID
            
        Returns:
            Dict[str, Any]: å¤„ç†ç»“æœ
        """
        try:
            # è§£æå¹¶æ„å»ºå†…å®¹å•å…ƒ
            content_units = self._parse_sections_to_content_units(json_data)
            
            if not content_units:
                return {
                    'success': False,
                    'message': 'æœªæ‰¾åˆ°å¯å‘é‡åŒ–çš„å†…å®¹',
                    'vectorized_count': 0
                }
            
            # å‘é‡åŒ–å†…å®¹å•å…ƒ
            vector_data = []
            for idx, unit in enumerate(content_units):
                if not isinstance(unit, dict):
                    self.logger.error(f"å†…å®¹å•å…ƒ {idx} ä¸æ˜¯å­—å…¸ç±»å‹: {type(unit)}")
                    continue
                
                vector = self._get_text_embedding(unit['content'])
                if vector:
                    vector_id = f"{document_id}_{idx}"
                    
                    # ğŸ”§ æå–content_typeåˆ°ç‹¬ç«‹å­—æ®µ
                    content_type = unit['content_type']
                    
                    vector_data.append({
                        'id': vector_id,
                        'vector': vector,
                        'document_id': document_id,
                        'element_id': unit.get('element_id', ''),
                        'chunk_index': idx,
                        'content': unit['content'],
                        # ğŸ”§ æ–°å¢ï¼šç‹¬ç«‹çš„content_typeå­—æ®µ
                        'content_type': content_type,
                        'metadata': {
                            'content_type': content_type,  # ä¿æŒå‘åå…¼å®¹
                            'title': unit.get('title', ''),
                            'page_number': unit.get('page_number', 1),
                            'element_ids': unit.get('element_ids', []),
                            'section_id': unit.get('section_id', ''),
                            'block_type': unit.get('block_type', ''),
                            'process_time': datetime.now().isoformat()
                        }
                    })
            
            if not vector_data:
                return {
                    'success': False,
                    'message': 'å‘é‡åŒ–å¤±è´¥ï¼Œæœªèƒ½ç”Ÿæˆæœ‰æ•ˆå‘é‡',
                    'vectorized_count': 0
                }
            
            # å­˜å‚¨å‘é‡åˆ°Milvus
            self.logger.info(f"å¼€å§‹å­˜å‚¨ {len(vector_data)} æ¡å‘é‡æ•°æ®åˆ°Milvus")
            success = self.milvus_manager.insert_vectors(vector_data)
            
            if success:
                self.logger.info("Milvuså‘é‡å­˜å‚¨æˆåŠŸ")
                
                self.logger.info(f"PDFå‘é‡åŒ–å®Œæˆï¼Œæ–‡æ¡£ID: {document_id}, å‘é‡æ•°é‡: {len(vector_data)}")
                
                return {
                    'success': True,
                    'message': 'PDFå‘é‡åŒ–æˆåŠŸ',
                    'vectorized_count': len(vector_data),
                    'document_id': document_id
                }
            else:
                self.logger.error("Milvuså‘é‡å­˜å‚¨å¤±è´¥")
                return {
                    'success': False,
                    'message': 'Milvuså‘é‡å­˜å‚¨å¤±è´¥',
                    'vectorized_count': 0
                }
            
        except Exception as e:
            self.logger.error(f"PDFå‘é‡åŒ–å¤„ç†å¤±è´¥: {str(e)}")
            return {
                'success': False,
                'message': f'PDFå‘é‡åŒ–å¤„ç†å¤±è´¥: {str(e)}',
                'vectorized_count': 0
            }
    
    def _parse_sections_to_content_units(self, json_data: Dict[str, Any]) -> List[Dict[str, Any]]:
        """
        åŸºäºæ–°çš„sectionæ ¼å¼è§£æJSONæ•°æ®ä¸ºå†…å®¹å•å…ƒ
        titles ç”¨ full_textï¼›fragments ç”¨ blocks[*].text/row_text/caption
        
        Args:
            json_data: æ–°æ ¼å¼çš„JSONæ•°æ®ï¼ˆåŒ…å«sectionsï¼‰
            
        Returns:
            List[Dict[str, Any]]: å†…å®¹å•å…ƒåˆ—è¡¨
        """
        try:
            sections = json_data.get('sections', [])
            content_units = []
            
            for section in sections:
                section_id = section.get('section_id', '')
                title = section.get('title', '')
                full_text = section.get('full_text', '')
                page_start = section.get('page_start', 1)
                blocks = section.get('blocks', [])
                
                # 1. åˆ›å»ºtitleçº§åˆ«çš„å†…å®¹å•å…ƒï¼ˆğŸ”§ ä¿®å¤ï¼šåªä½¿ç”¨æ ‡é¢˜æ–‡æœ¬ï¼‰
                if title.strip():
                    title_unit = {
                        'content': title,  # ğŸ”§ ä¿®å¤ï¼šåªå­˜å‚¨æ ‡é¢˜æ–‡æœ¬ï¼Œä¸æ˜¯full_text
                        'content_type': 'title',
                        'title': title,
                        'page_number': page_start,
                        'element_id': section_id,
                        'element_ids': section.get('elem_ids', []),
                        'section_id': section_id
                    }
                    content_units.append(title_unit)
                
                # ğŸ”§ æ–°å¢ï¼šåˆ›å»ºsectionçº§åˆ«çš„å®Œæ•´å†…å®¹å•å…ƒï¼ˆç”¨äºè·å–å®Œæ•´ä¸Šä¸‹æ–‡ï¼‰
                if full_text.strip() and full_text != title:
                    section_unit = {
                        'content': full_text,
                        'content_type': 'section',  # æ–°çš„ç±»å‹ï¼šå®Œæ•´section
                        'title': title,
                        'page_number': page_start,
                        'element_id': section_id + '_full',
                        'element_ids': section.get('elem_ids', []),
                        'section_id': section_id
                    }
                    content_units.append(section_unit)
                
                # 2. åˆ›å»ºfragmentçº§åˆ«çš„å†…å®¹å•å…ƒï¼ˆå¤„ç†blocksï¼‰
                for block in blocks:
                    block_type = block.get('type', '').lower()
                    elem_id = block.get('elem_id', '')
                    page = block.get('page', page_start)
                    
                    # ğŸ”§ ä¿®å¤ï¼šè·³è¿‡æ ‡é¢˜ç±»å‹çš„blockï¼Œé¿å…é‡å¤å­˜å‚¨
                    # æ ‡é¢˜å·²ç»åœ¨ä¸Šé¢ä½œä¸ºtitleç±»å‹å¤„ç†è¿‡äº†
                    if block_type == 'title':
                        continue
                    
                    # æ ¹æ®blockç±»å‹æå–æ–‡æœ¬å†…å®¹
                    fragment_text = self._extract_block_text(block, block_type)
                    
                    if fragment_text.strip():
                        fragment_unit = {
                            'content': fragment_text,
                            'content_type': 'fragment',
                            'title': title,  # ç»§æ‰¿sectionçš„title
                            'page_number': page,
                            'element_id': elem_id,
                            'element_ids': [elem_id],
                            'section_id': section_id,
                            'block_type': block_type
                        }
                        content_units.append(fragment_unit)
            
            self.logger.info(f"è§£æsectionså®Œæˆï¼Œç”Ÿæˆå†…å®¹å•å…ƒ: {len(content_units)}")
            return content_units
            
        except Exception as e:
            self.logger.error(f"è§£æsectionså¤±è´¥: {str(e)}")
            return []
    
    def _extract_block_text(self, block: Dict[str, Any], block_type: str) -> str:
        """
        æ ¹æ®blockç±»å‹æå–æ–‡æœ¬å†…å®¹
        
        Args:
            block: blockæ•°æ®
            block_type: blockç±»å‹
            
        Returns:
            str: æå–çš„æ–‡æœ¬å†…å®¹
        """
        try:
            if block_type == 'table':
                # å¯¹äºtableç±»å‹ï¼Œä½¿ç”¨rowsä¸­çš„row_text
                rows = block.get('rows', [])
                if rows:
                    row_texts = [row.get('row_text', '') for row in rows if row.get('row_text', '').strip()]
                    return ' '.join(row_texts)
                else:
                    # å¦‚æœæ²¡æœ‰rowsï¼Œå›é€€åˆ°text
                    return block.get('text', '')
            
            elif block_type == 'figure':
                # å¯¹äºfigureç±»å‹ï¼Œä½¿ç”¨caption
                caption = block.get('caption', '')
                if caption.strip():
                    return caption
                else:
                    # å¦‚æœæ²¡æœ‰captionï¼Œå›é€€åˆ°text
                    return block.get('text', '')
            
            else:
                # å¯¹äºå…¶ä»–ç±»å‹ï¼ˆparagraphç­‰ï¼‰ï¼Œä½¿ç”¨text
                return block.get('text', '')
                
        except Exception as e:
            self.logger.warning(f"æå–blockæ–‡æœ¬å¤±è´¥: {str(e)}")
            return block.get('text', '')
    
    def _get_text_embedding(self, text: str) -> Optional[List[float]]:
        """
        è·å–æ–‡æœ¬å‘é‡
        
        Args:
            text: è¾“å…¥æ–‡æœ¬
            
        Returns:
            Optional[List[float]]: æ–‡æœ¬å‘é‡ï¼Œå¤±è´¥è¿”å›None
        """
        try:
            # æ–‡æœ¬é¢„å¤„ç†
            processed_text = self._preprocess_text(text)
            
            if not processed_text:
                return None
            
            # ç”Ÿæˆå‘é‡
            embedding = self.embedding_model.encode(
                processed_text,
                normalize_embeddings=self.normalize
            )
            
            return embedding.tolist()
            
        except Exception as e:
            self.logger.error(f"è·å–æ–‡æœ¬å‘é‡å¤±è´¥: {str(e)}")
            return None
    
    def _preprocess_text(self, text: str) -> str:
        """
        æ–‡æœ¬é¢„å¤„ç†
        
        Args:
            text: åŸå§‹æ–‡æœ¬
            
        Returns:
            str: é¢„å¤„ç†åçš„æ–‡æœ¬
        """
        try:
            if not text:
                return ""
            
            preprocessing_config = self.model_config['embedding']['preprocessing']
            
            # æ¸…ç†æ–‡æœ¬
            if preprocessing_config.get('clean_text', True):
                # ç§»é™¤å¤šä½™çš„ç©ºç™½å­—ç¬¦
                text = ' '.join(text.split())
                
                # ç§»é™¤ç‰¹æ®Šå­—ç¬¦ï¼ˆå¦‚æœé…ç½®è¦æ±‚ï¼‰
                if preprocessing_config.get('remove_special_chars', False):
                    import re
                    text = re.sub(r'[^\w\s\u4e00-\u9fff]', ' ', text)
            
            # è½¬æ¢ä¸ºå°å†™ï¼ˆå¦‚æœé…ç½®è¦æ±‚ï¼‰
            if preprocessing_config.get('lowercase', False):
                text = text.lower()
            
            # é™åˆ¶æœ€å¤§é•¿åº¦
            max_length = self.model_config['embedding']['max_length']
            if len(text) > max_length:
                text = text[:max_length]
            
            return text.strip()
            
        except Exception as e:
            self.logger.error(f"æ–‡æœ¬é¢„å¤„ç†å¤±è´¥: {str(e)}")
            return text
    



