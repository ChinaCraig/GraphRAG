#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
BERTä¸­æ–‡tokenizerä¸‹è½½è„šæœ¬ - çŸ¥è¯†å›¾è°±NERæ¨¡å—ä¸“ç”¨
æ¨¡å‹: bert-base-chinese
ç‰ˆæœ¬: transformers-4.36.x compatible
æè¿°: ä¸­æ–‡BERTæ¨¡å‹çš„tokenizerï¼Œç”¨äºçŸ¥è¯†å›¾è°±ç»Ÿè®¡å¼NER
å¤§å°: ~400MB
"""

import os
import sys
import time
from pathlib import Path

# æ¨¡å‹ä¿¡æ¯
MODEL_NAME = "bert-base-chinese"
MODEL_DESC = "ä¸­æ–‡BERTæ¨¡å‹tokenizerï¼Œç”¨äºçŸ¥è¯†å›¾è°±NER"
MODEL_SIZE = "~400MB"
FRAMEWORK = "transformers"
VERSION = "4.36.x"

print("=" * 60)
print("ğŸ“¦ GraphRAG çŸ¥è¯†å›¾è°±æ¨¡å‹ä¸‹è½½å™¨")
print("=" * 60)

# æ£€æŸ¥ç½‘ç»œé…ç½®
hf_endpoint = os.environ.get('HF_ENDPOINT')
if hf_endpoint:
    print(f"ğŸª ä½¿ç”¨ç”¨æˆ·é…ç½®çš„é•œåƒæº: {hf_endpoint}")
else:
    # è®¾ç½®é»˜è®¤é•œåƒæºï¼ˆé˜¿é‡Œäº‘ï¼‰
    os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'
    print("ğŸª è‡ªåŠ¨ä½¿ç”¨é˜¿é‡Œäº‘é•œåƒæºï¼ˆæé«˜ä¸‹è½½æˆåŠŸç‡ï¼‰")
    print("   å¦‚éœ€ä½¿ç”¨å…¶ä»–æºï¼Œè¯·è®¾ç½® HF_ENDPOINT ç¯å¢ƒå˜é‡")

print(f"ğŸ“„ æ¨¡å‹: {MODEL_NAME}")
print(f"ğŸ“ æè¿°: {MODEL_DESC}")
print(f"ğŸ“¦ å¤§å°: {MODEL_SIZE}")
print(f"ğŸ”§ æ¡†æ¶: {FRAMEWORK} {VERSION}")
print("")

# è®¾ç½®æ¨¡å‹ç¼“å­˜ç›®å½•ï¼ˆä¸é¡¹ç›®é…ç½®ä¸€è‡´ï¼‰
cache_dir = os.path.abspath("./models")
os.makedirs(cache_dir, exist_ok=True)

# è®¾ç½®ç¯å¢ƒå˜é‡
os.environ['HF_HOME'] = cache_dir
os.environ['TRANSFORMERS_CACHE'] = cache_dir

print(f"ğŸ“ æ¨¡å‹å­˜å‚¨ç›®å½•: {cache_dir}")
print("")

# æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
model_path = os.path.join(cache_dir, "models--bert-base-chinese")
if os.path.exists(model_path):
    print(f"âœ“ {MODEL_NAME} å·²å­˜åœ¨")
    print("â­ï¸  è·³è¿‡ä¸‹è½½")
    
    # åˆ›å»ºé…ç½®è¯´æ˜
    config_info = f"""
# BERTä¸­æ–‡tokenizeré…ç½®è¯´æ˜
# 
# æ¨¡å‹ä½ç½®: {cache_dir}/models--bert-base-chinese
# æ¨¡å‹å¤§å°: {MODEL_SIZE}
# 
# åœ¨çŸ¥è¯†å›¾è°±é…ç½®ä¸­ä½¿ç”¨:
# knowledge_graph:
#   ner:
#     model_name: "bert-base-chinese"
#     cache_dir: "{cache_dir}"
# 
# ä¸»è¦ç”¨é€”:
# - ç»Ÿè®¡å¼NERçš„tokenizer
# - ä¸­æ–‡æ–‡æœ¬tokenåŒ–
# - offset_mappingæ”¯æŒ
"""
    print(config_info)
    sys.exit(0)

print("ğŸš€ å¼€å§‹ä¸‹è½½...")
print("   â³ è¿™å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿï¼Œè¯·è€å¿ƒç­‰å¾…...")
print("")

# ä¸‹è½½æ¨¡å‹
try:
    from transformers import AutoTokenizer
    print("ğŸ“¥ ä¸‹è½½tokenizer...")
    
    # åˆ†æ­¥ä¸‹è½½ï¼Œæ˜¾ç¤ºè¿›åº¦
    tokenizer = AutoTokenizer.from_pretrained(
        MODEL_NAME,
        cache_dir=cache_dir,
        force_download=False,  # ä¸å¼ºåˆ¶é‡æ–°ä¸‹è½½
        resume_download=True   # æ”¯æŒæ–­ç‚¹ç»­ä¼ 
    )
    
    print("âœ… tokenizerä¸‹è½½å®Œæˆ")
    
    # éªŒè¯æ¨¡å‹
    print("ğŸ” éªŒè¯æ¨¡å‹...")
    test_text = "CHOç»†èƒç”Ÿäº§è›‹ç™½è´¨"
    inputs = tokenizer(test_text, return_tensors="pt", return_offsets_mapping=True)
    
    print(f"âœ… æ¨¡å‹éªŒè¯æˆåŠŸ")
    print(f"   æµ‹è¯•æ–‡æœ¬: {test_text}")
    print(f"   Tokenæ•°é‡: {len(inputs['input_ids'][0])}")
    print(f"   Offsetæ˜ å°„: {'æ”¯æŒ' if 'offset_mapping' in inputs else 'ä¸æ”¯æŒ'}")
    
    # æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
    model_size = sum(f.stat().st_size for f in Path(cache_dir).rglob('*') if f.is_file())
    print(f"ğŸ“Š å®é™…å¤§å°: {model_size / 1024 / 1024:.1f}MB")
    
    print("")
    print("ğŸ‰ ä¸‹è½½å®Œæˆï¼")
    print("")
    
    # ä½¿ç”¨è¯´æ˜
    usage_info = f"""
ğŸ“‹ ä½¿ç”¨è¯´æ˜:

1. é…ç½®æ–‡ä»¶å·²æ›´æ–° (config/model.yaml):
   knowledge_graph:
     ner:
       model_name: "bert-base-chinese"
       cache_dir: "{cache_dir}"

2. åœ¨ä»£ç ä¸­ä½¿ç”¨:
   from transformers import AutoTokenizer
   tokenizer = AutoTokenizer.from_pretrained(
       "bert-base-chinese", 
       cache_dir="{cache_dir}"
   )

3. çŸ¥è¯†å›¾è°±æœåŠ¡ä¼šè‡ªåŠ¨ä½¿ç”¨æ­¤æ¨¡å‹è¿›è¡Œç»Ÿè®¡å¼NER

âš ï¸  æ³¨æ„äº‹é¡¹:
- æ¨¡å‹è·¯å¾„: {cache_dir}
- æ”¯æŒä¸­æ–‡æ–‡æœ¬å¤„ç†
- æä¾›offset_mappingåŠŸèƒ½
- é™çº§æœºåˆ¶ï¼šæ¨¡å‹åŠ è½½å¤±è´¥æ—¶è‡ªåŠ¨ä½¿ç”¨è§„åˆ™æ–¹æ³•
"""
    print(usage_info)
    
except ImportError as e:
    print(f"âŒ ç¼ºå°‘ä¾èµ–: {e}")
    print("ğŸ’¡ è¯·å…ˆå®‰è£…: pip install transformers torch")
    sys.exit(1)
    
except Exception as e:
    print(f"âŒ ä¸‹è½½å¤±è´¥: {str(e)}")
    print("ğŸ’¡ å¯èƒ½çš„è§£å†³æ–¹æ¡ˆ:")
    print("   1. æ£€æŸ¥ç½‘ç»œè¿æ¥")
    print("   2. è®¾ç½®ä»£ç†: export HTTP_PROXY=http://127.0.0.1:7890")
    print("   3. ä½¿ç”¨å…¶ä»–é•œåƒ: export HF_ENDPOINT=https://hf-mirror.com")
    print("   4. é‡æ–°è¿è¡Œè„šæœ¬")
    sys.exit(1)

